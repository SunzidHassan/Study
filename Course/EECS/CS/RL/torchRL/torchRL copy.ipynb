{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch RL Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to TorchRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=9298) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensordict-nightly\n",
      "  Downloading tensordict_nightly-2024.9.30-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "INFO: pip is looking at multiple versions of tensordict-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensordict_nightly-2024.9.29-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.28-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.27-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.26-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.25-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.24-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.23-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "INFO: pip is still looking at multiple versions of tensordict-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensordict_nightly-2024.9.22-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.21-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.20-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.19-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.18-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading tensordict_nightly-2024.9.16-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.15-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.14-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.13-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.12-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.11-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.10-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.9-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.8-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.7-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.6-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.5-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.4-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.3-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.2-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.9.1-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.31-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.30-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.29-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.28-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.27-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.26-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.25-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.24-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.23-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.22-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.21-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.20-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.19-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.18-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.17-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.16-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.15-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.14-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.13-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.12-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.11-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.10-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.9-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.8-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.7-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.6-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.5-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.4-cp312-cp312-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
      "  Downloading tensordict_nightly-2024.8.3-cp312-cp312-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "  Downloading tensordict_nightly-2024.8.2-cp312-cp312-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "  Downloading tensordict_nightly-2024.8.1-cp312-cp312-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "  Downloading tensordict_nightly-2024.7.3-cp312-cp312-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: torch>=2.4.0.dev in /home/sunzid/anaconda3/lib/python3.12/site-packages (from tensordict-nightly) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy in /home/sunzid/anaconda3/lib/python3.12/site-packages (from tensordict-nightly) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in /home/sunzid/anaconda3/lib/python3.12/site-packages (from tensordict-nightly) (2.2.1)\n",
      "Requirement already satisfied: orjson in /home/sunzid/anaconda3/lib/python3.12/site-packages (from tensordict-nightly) (3.10.7)\n",
      "Requirement already satisfied: filelock in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->tensordict-nightly) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.4.0.dev->tensordict-nightly) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from sympy->torch>=2.4.0.dev->tensordict-nightly) (1.3.0)\n",
      "Downloading tensordict_nightly-2024.7.3-cp312-cp312-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensordict-nightly\n",
      "Successfully installed tensordict-nightly-2024.7.3\n",
      "Collecting torchrl-nightly\n",
      "  Downloading torchrl_nightly-2024.9.30-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "INFO: pip is looking at multiple versions of torchrl-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchrl_nightly-2024.9.29-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.28-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.27-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.26-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.25-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.24-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.23-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "INFO: pip is still looking at multiple versions of torchrl-nightly to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchrl_nightly-2024.9.22-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.21-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.20-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.19-cp312-cp312-manylinux1_x86_64.whl.metadata (39 kB)\n",
      "  Downloading torchrl_nightly-2024.9.16-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchrl_nightly-2024.9.15-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.14-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.13-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.12-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.11-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.10-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.9-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.8-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.7-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.6-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.5-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.4-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.3-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.2-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.9.1-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.31-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.30-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.29-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.28-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.27-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.26-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.25-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.24-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.23-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.22-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.21-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.20-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.19-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.18-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.17-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.16-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.15-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.14-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.13-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.12-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.11-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.10-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.9-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.8-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.7-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.6-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.5-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.4-cp312-cp312-manylinux1_x86_64.whl.metadata (34 kB)\n",
      "  Downloading torchrl_nightly-2024.8.3-cp312-cp312-manylinux1_x86_64.whl.metadata (33 kB)\n",
      "  Downloading torchrl_nightly-2024.8.2-cp312-cp312-manylinux1_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: torch>=2.4.0.dev in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torchrl-nightly) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torchrl-nightly) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torchrl-nightly) (23.2)\n",
      "Requirement already satisfied: cloudpickle in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torchrl-nightly) (2.2.1)\n",
      "Requirement already satisfied: tensordict-nightly in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torchrl-nightly) (2024.7.3)\n",
      "Requirement already satisfied: filelock in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from torch>=2.4.0.dev->torchrl-nightly) (3.0.0)\n",
      "Requirement already satisfied: orjson in /home/sunzid/anaconda3/lib/python3.12/site-packages (from tensordict-nightly->torchrl-nightly) (3.10.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.4.0.dev->torchrl-nightly) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from sympy->torch>=2.4.0.dev->torchrl-nightly) (1.3.0)\n",
      "Downloading torchrl_nightly-2024.8.2-cp312-cp312-manylinux1_x86_64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torchrl-nightly\n",
      "Successfully installed torchrl-nightly-2024.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensordict-nightly\n",
    "!pip install torchrl-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from gymnasium) (4.11.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "torch.zeros(batch_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "tensordict = TensorDict(\n",
    "    source={\n",
    "        \"key 1\": torch.zeros(batch_size, 3),\n",
    "        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n",
    "    },\n",
    "    batch_size=[batch_size]\n",
    ")\n",
    "\n",
    "print(tensordict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False) \n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tensordict[2],'\\n')\n",
    "print(tensordict[\"key 1\"] is tensordict.get(\"key 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking multiple TensorDicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensordict1 = TensorDict(\n",
    "    source={\n",
    "        \"key 1\": torch.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments, TED and transform\n",
    "The standard RL training loop: train a model/policy to accomplish a task in an environment (e.g., a simulator).\n",
    "\n",
    ":mod:`~torchrl.envs` environment wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `env.reset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset = env.reset()\n",
    "print(reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6809,  0.7323, -0.0661])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset[\"observation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a random action in the td."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset_with_action = env.rand_action(reset)\n",
    "print(reset_with_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6137])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_with_action[\"action\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `env.step()`\n",
    "Passing the td with initial env variable values and a random action to env.step().\n",
    "\n",
    "This returns TED - TorchRL Episode Data format - ubiquitous way of representing data in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step_mdp`: to bring the \"next\" entry at root to perform the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import step_mdp\n",
    "\n",
    "data = step_mdp(stepped_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rollout()`\n",
    "Combination of three steps:\n",
    "- computing an action\n",
    "- taking a step\n",
    "- moving in the MDP\n",
    "\n",
    "Without a policy, it'll execute random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "\n",
      "tensor([[-0.2421],\n",
      "        [ 0.6406],\n",
      "        [ 0.6442],\n",
      "        [ 0.2375],\n",
      "        [ 0.6837],\n",
      "        [-0.7404],\n",
      "        [-1.5189],\n",
      "        [-0.7908],\n",
      "        [ 0.2366],\n",
      "        [ 1.8354]])\n",
      "\n",
      "tensor([0.2375])\n"
     ]
    }
   ],
   "source": [
    "# spatial indexing\n",
    "print(rollout[3])\n",
    "print()\n",
    "\n",
    "# indexing by key\n",
    "print(rollout[\"action\"])\n",
    "print()\n",
    "\n",
    "# indexing by key and step\n",
    "print(rollout['action'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TransformedEnv`\n",
    "Complete list of transforms [here](https://pytorch.org/rl/stable/reference/envs.html#id2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import StepCounter, TransformedEnv\n",
    "\n",
    "transformed_env = TransformedEnv(env, StepCounter(max_steps=10))\n",
    "rollout = transformed_env.rollout(max_steps=100)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  tensor([[ 1.2521],\n",
      "        [ 0.3543],\n",
      "        [-1.4463],\n",
      "        [ 1.6247],\n",
      "        [-0.0473],\n",
      "        [-1.5524],\n",
      "        [-0.2393],\n",
      "        [ 0.3432],\n",
      "        [-0.4568],\n",
      "        [ 0.1557]]) \n",
      "\n",
      "done:  tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]]) \n",
      "\n",
      "observation:  tensor([[-0.8678,  0.4969, -0.0552],\n",
      "        [-0.8801,  0.4748,  0.5053],\n",
      "        [-0.9009,  0.4341,  0.9146],\n",
      "        [-0.9219,  0.3875,  1.0232],\n",
      "        [-0.9492,  0.3146,  1.5575],\n",
      "        [-0.9735,  0.2287,  1.7863],\n",
      "        [-0.9896,  0.1440,  1.7250],\n",
      "        [-0.9985,  0.0546,  1.7970],\n",
      "        [-0.9992, -0.0399,  1.8894],\n",
      "        [-0.9916, -0.1291,  1.7910]]) \n",
      "\n",
      "observation:  tensor([[[-0.8678,  0.4969, -0.0552],\n",
      "         [-0.8801,  0.4748,  0.5053]],\n",
      "\n",
      "        [[-0.8801,  0.4748,  0.5053],\n",
      "         [-0.9009,  0.4341,  0.9146]],\n",
      "\n",
      "        [[-0.9009,  0.4341,  0.9146],\n",
      "         [-0.9219,  0.3875,  1.0232]],\n",
      "\n",
      "        [[-0.9219,  0.3875,  1.0232],\n",
      "         [-0.9492,  0.3146,  1.5575]],\n",
      "\n",
      "        [[-0.9492,  0.3146,  1.5575],\n",
      "         [-0.9735,  0.2287,  1.7863]],\n",
      "\n",
      "        [[-0.9735,  0.2287,  1.7863],\n",
      "         [-0.9896,  0.1440,  1.7250]],\n",
      "\n",
      "        [[-0.9896,  0.1440,  1.7250],\n",
      "         [-0.9985,  0.0546,  1.7970]],\n",
      "\n",
      "        [[-0.9985,  0.0546,  1.7970],\n",
      "         [-0.9992, -0.0399,  1.8894]],\n",
      "\n",
      "        [[-0.9992, -0.0399,  1.8894],\n",
      "         [-0.9916, -0.1291,  1.7910]],\n",
      "\n",
      "        [[-0.9916, -0.1291,  1.7910],\n",
      "         [-0.9769, -0.2136,  1.7176]]]) \n",
      "\n",
      "step:  tensor([[[ 0],\n",
      "         [ 1]],\n",
      "\n",
      "        [[ 1],\n",
      "         [ 2]],\n",
      "\n",
      "        [[ 2],\n",
      "         [ 3]],\n",
      "\n",
      "        [[ 3],\n",
      "         [ 4]],\n",
      "\n",
      "        [[ 4],\n",
      "         [ 5]],\n",
      "\n",
      "        [[ 5],\n",
      "         [ 6]],\n",
      "\n",
      "        [[ 6],\n",
      "         [ 7]],\n",
      "\n",
      "        [[ 7],\n",
      "         [ 8]],\n",
      "\n",
      "        [[ 8],\n",
      "         [ 9]],\n",
      "\n",
      "        [[ 9],\n",
      "         [10]]]) \n",
      "\n",
      "terminated:  tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]]) \n",
      "\n",
      "truncated:  tensor([[[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [False]],\n",
      "\n",
      "        [[False],\n",
      "         [ True]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('action: ', rollout[\"action\"],'\\n')\n",
    "print('done: ', rollout['done'],'\\n')\n",
    "print('observation: ', rollout['observation'],'\\n')\n",
    "print('observation: ', torch.stack([rollout[\"observation\"], rollout[\"next\",\"observation\"]],1),'\\n')\n",
    "# print('step: ', rollout['step_count'],'\\n')\n",
    "print('step: ', torch.stack([rollout[\"step_count\"], rollout[\"next\",\"step_count\"]],1),'\\n')\n",
    "print('terminated: ', rollout['terminated'],'\\n')\n",
    "# print('truncated: ', rollout['truncated'],'\\n')\n",
    "print('truncated: ', torch.stack([rollout[\"truncated\"], rollout[\"next\",\"truncated\"]],1),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TensorDictModules`: Policy construction\n",
    "Similar to how environments interact with instances of TensorDict, modules used to represent policies and vlaue functions also do the same. \n",
    "Code idea:\n",
    "- encapsulate a `Module` within a class\n",
    "- that know which entries need to be read and passed to the module\n",
    "- and then records the results with the assigned entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n",
    "# LazyLinear automatically fetches observation space\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialized wrapper\n",
    "`Actor` provides default values for the `in_keys` and `out_keys`, making integration with common environments straightforward.\n",
    "\n",
    "[List of specialized modules](https://pytorch.org/rl/stable/reference/modules.html#tdmodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import Actor\n",
    "\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "\n",
    "# specialized actor policy\n",
    "policy = Actor(module)\n",
    "\n",
    "## non-specialized policy\n",
    "# policy = TensorDictModule(\n",
    "#     module,\n",
    "#     in_keys=[\"observation\"],\n",
    "#     out_keys=[\"action\"],\n",
    "# )\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networks\n",
    "Regular modules that can be used without recurring to tensordict features. Two most common are `MLP` and `ConvNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import MLP\n",
    "\n",
    "## TensorDictModule\n",
    "# module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "\n",
    "# MLP module\n",
    "module = MLP(out_features=env.action_spec.shape[-1],\n",
    "             num_cells=[32, 64],\n",
    "             activation_class=torch.nn.Tanh,)\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic Policies\n",
    "TorchRL facilitates stocastic policy by grouping operations like:\n",
    "- building distribution from parameters\n",
    "- sampling from that distribution\n",
    "- retrieving log-probability\n",
    "\n",
    "In the next example, we'll build an actor that relies on a regular normal distribution using three components:\n",
    "- an `MLP` backbone reading observation of size [3], and outputting a single tensor of size [2].\n",
    "- a `NormalParamExtractor` module that will split the output into two chunks - a mean and a std dev of size [1]\n",
    "- a `ProbabilisticActor` that will read those parameters as `in_keys`, create a distribution with them, and populate the tensordict with samples and log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.12/site-packages/tensordict/nn/probabilistic.py:497: UserWarning: deterministic_sample wasn't found when queried on <class 'torch.distributions.normal.Normal'>. SafeProbabilisticModule is falling back on mean instead. For better code quality and efficiency, make sure to either provide a distribution with a deterministic_sample attribute or to change the InteractionMode to the desired value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "# out is 2 instead of 1\n",
    "backbone = MLP(in_features=3, out_features=2)\n",
    "# split the out of size [2] into a mean and std dev of size [1]\n",
    "extractor = NormalParamExtractor()\n",
    "module = torch.nn.Sequential(backbone, extractor)\n",
    "\n",
    "# instead of observation to action, we have observation > mean/std > action\n",
    "td_module = TensorDictModule(module,\n",
    "                             in_keys=[\"observation\"],\n",
    "                             out_keys=[\"loc\", \"scale\"])\n",
    "\n",
    "policy = ProbabilisticActor(td_module,\n",
    "                            in_keys=[\"loc\",\"scale\"],\n",
    "                            out_keys=[\"action\"],\n",
    "                            distribution_class=Normal,\n",
    "                            return_log_prob=True,)\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling of action can chose expected value instead of using random samples with `set_exploration_type()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # takes the mean as action\n",
    "    rolloutMean = env.rollout(max_steps=10, policy=policy)\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # takes the mean as action\n",
    "    rolloutDist = env.rollout(max_steps=10, policy=policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration\n",
    "Deterministic policies don't explore inherently. Torchrl has modules for exploration.\n",
    "\n",
    "Epsilon-greedy exploration module parameters:\n",
    "- epsilon: (1 is every action random, 0 is no exploration)\n",
    "- anneal: decrease epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.modules import EGreedyModule\n",
    "\n",
    "# # MLP module\n",
    "# backbone = MLP(in_features=3, out_features=2,\n",
    "#              num_cells=[32, 64])\n",
    "# policy = Actor(module)\n",
    "\n",
    "policy = Actor(MLP(3, 1, num_cells=[32, 64]))\n",
    "\n",
    "exploration_module = EGreedyModule(\n",
    "    spec=   env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
    ")\n",
    "\n",
    "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # turns off exploration\n",
    "    rollout1 = env.rollout(max_steps=10, policy=exploration_policy)\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # turns on exploration\n",
    "    rollout2 = env.rollout(max_steps=10, policy=exploration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4319],\n",
      "         [ 1.6355]],\n",
      "\n",
      "        [[ 0.4239],\n",
      "         [ 0.1764]],\n",
      "\n",
      "        [[ 0.3959],\n",
      "         [-1.3561]],\n",
      "\n",
      "        [[ 0.3594],\n",
      "         [ 1.5796]],\n",
      "\n",
      "        [[ 0.3245],\n",
      "         [ 0.0813]],\n",
      "\n",
      "        [[ 0.2960],\n",
      "         [ 0.1487]],\n",
      "\n",
      "        [[ 0.2750],\n",
      "         [ 0.1530]],\n",
      "\n",
      "        [[ 0.2605],\n",
      "         [ 0.1623]],\n",
      "\n",
      "        [[ 0.2513],\n",
      "         [ 0.1778]],\n",
      "\n",
      "        [[ 0.2460],\n",
      "         [ 0.2005]]], grad_fn=<StackBackward0>) \n",
      "\n",
      "tensor([[[ -3.6267],\n",
      "         [ -5.7703]],\n",
      "\n",
      "        [[ -3.7987],\n",
      "         [ -6.3049]],\n",
      "\n",
      "        [[ -4.1803],\n",
      "         [ -6.9850]],\n",
      "\n",
      "        [[ -4.7753],\n",
      "         [ -7.6511]],\n",
      "\n",
      "        [[ -5.5833],\n",
      "         [ -8.6991]],\n",
      "\n",
      "        [[ -6.5935],\n",
      "         [ -9.7239]],\n",
      "\n",
      "        [[ -7.7793],\n",
      "         [-10.7903]],\n",
      "\n",
      "        [[ -9.0957],\n",
      "         [-10.0133]],\n",
      "\n",
      "        [[-10.4822],\n",
      "         [ -9.0440]],\n",
      "\n",
      "        [[-11.4969],\n",
      "         [ -8.1109]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([rollout1[\"action\"], rollout2[\"action\"]], 1), '\\n')\n",
    "print(torch.stack([rollout1[\"next\", \"reward\"], rollout2[\"next\", \"reward\"]], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Value actors\n",
    "Policy can be a composite module. **Q-Value actors** require an estimate of action value, and will greedily pick up the action with the highest value. DQN is for continuous state-space where a neural network encode the `Q(s,a)` value map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotDiscreteTensorSpec(\n",
      "    shape=torch.Size([2]),\n",
      "    space=DiscreteBox(n=2),\n",
      "    device=cpu,\n",
      "    dtype=torch.int64,\n",
      "    domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(\"CartPole-v1\")\n",
    "print(env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a value network that produces one value per action when it reads a state from the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import QValueModule\n",
    "\n",
    "num_actions = 2\n",
    "value_net = TensorDictModule(\n",
    "    MLP(out_features=num_actions, num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_value\"],\n",
    ")\n",
    "\n",
    "policy = TensorDictSequential(value_net, # writes \"action_value\"\n",
    "                              QValueModule(spec=env.action_spec), # reads \"action_value\" by default\n",
    "                              )\n",
    "\n",
    "rollout = env.rollout(max_steps=3, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output has \"action_values\" and \"chosen_action_values\". Since it relies on `argmax` (only exploitation), we will use `EGreedyModule` for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1172],\n",
      "         [-0.1195]],\n",
      "\n",
      "        [[-0.1217],\n",
      "         [-0.1240]],\n",
      "\n",
      "        [[-0.1268],\n",
      "         [-0.1290]]], grad_fn=<StackBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_explore = TensorDictSequential(policy,\n",
    "                                      EGreedyModule(env.action_spec))\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)\n",
    "\n",
    "print(torch.stack([rollout[\"chosen_action_value\"], rollout_explore[\"chosen_action_value\"]], 1), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and optimization\n",
    "\n",
    "Typical training loop:\n",
    "```\n",
    ">>> for i in range(n_collections):\n",
    "...     data = get_next_batch(env, policy)\n",
    "...     for j in range(n_optim):\n",
    "...         loss = loss_fn(data)\n",
    "...         loss.backward()\n",
    "...         optim.step()\n",
    "```\n",
    "#### RL objective functions\n",
    "Off-policy DDPG: requires a deterministic map from the observation space to the action space, and a value network that predicts the value of a state-action pair. The DDPG loss attempts to find the policy parameters that output actions that maximize the value for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n",
    "\n",
    "from torchrl.modules import Actor, MLP, ValueOperator\n",
    "from torchrl.objectives import DDPGLoss\n",
    "\n",
    "n_obs = env.observation_spec[\"observation\"].shape[-1]\n",
    "n_act = env.action_spec.shape[-1]\n",
    "\n",
    "actor = Actor(\n",
    "    MLP(in_features=n_obs,\n",
    "         out_features=n_act,\n",
    "           num_cells=[32, 32]))\n",
    "\n",
    "value_net = ValueOperator(\n",
    "    MLP(in_features=n_obs + n_act,\n",
    "         out_features=1,\n",
    "           num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\", \"action\"],\n",
    ")\n",
    "\n",
    "ddpg_loss = DDPGLoss(actor_network=actor, value_network=value_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        td_error: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False) \n",
      "\n",
      "53.0244026184082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.12/site-packages/torchrl/objectives/common.py:29: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=100, policy=actor)\n",
    "loss_vals = ddpg_loss(rollout)\n",
    "print(loss_vals, '\\n')\n",
    "\n",
    "total_loss = 0\n",
    "for key, val in loss_vals.items():\n",
    "    if key.startswith(\"loss_\"):\n",
    "        total_loss += val\n",
    "    \n",
    "print(total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a LossModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optim=Adam(ddpg_loss.parameters())\n",
    "total_loss.backward()\n",
    "\n",
    "# # with following items\n",
    "# optim.step()\n",
    "# optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target parameters\n",
    "Target parameters represent a delayed or smoothed version of the parameters over time. The value are updated by user's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.objectives import SoftUpdate\n",
    "updater = SoftUpdate(ddpg_loss, eps=0.99)\n",
    "\n",
    "# updating\n",
    "updater.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection and storage\n",
    "#### Data collectors\n",
    "The primary data collector is the `SyncDataCollector` -\n",
    "- executing policy within the environment\n",
    "- resetting the environment when necessary\n",
    "- providing batches of a predefined size\n",
    "\n",
    "Collectors don't reset between consecutive batches, thus two batches can share a trajectory.\n",
    "\n",
    "Collector arguments\n",
    "- batch size (`frames_per_batch`)\n",
    "- length of iterator\n",
    "- the policy\n",
    "- the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import GymEnv\n",
    "from torchrl.envs.utils import RandomPolicy\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\")\n",
    "env.set_seed(0)\n",
    "\n",
    "policy = RandomPolicy(env.action_spec)\n",
    "collector = SyncDataCollector(env, policy, frames_per_batch=200, total_frames=-1) #-1 indicates never ending collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([200, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([200]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([200]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(data[\"collector\", \"traj_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay buffers\n",
    "To temporarily store data after collection.\n",
    "```\n",
    ">>> for data in collector:\n",
    "...     storage.store(data)\n",
    "...     for i in range(n_optim):\n",
    "...         sample = storage.sample()\n",
    "...         loss_val = loss_fn(sample)\n",
    "...         loss_val.backward()\n",
    "...         optim.step() # etc\n",
    "```\n",
    "`ReplayBuffer`: TorchRL data storage parent class. They are composable:\n",
    "- edit storage type\n",
    "- sampling technique\n",
    "- writing heuristic\n",
    "- transforms\n",
    "\n",
    "General requirement: type of storage.\n",
    "Recommendation: a `TensorStorage` subclass, like `LazyMemmapStorage`. It can be populated with `add()` (single element) or `extend()` (multiple element) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import LazyMemmapStorage, ReplayBuffer\n",
    "\n",
    "buffer = ReplayBuffer(storage=LazyMemmapStorage(max_size=1000))\n",
    "\n",
    "indices = buffer.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer has the same number of elements as from the collector\n",
    "assert len(buffer) == collector.frames_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([30, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([30]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([30]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "sample = buffer.sample(batch_size=30)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchRL's logging\n",
    "Logging is crucial for reporting results for the outside world, and for performance check. TorchRL has several loggers that interface with custom backends such as wandb, tensorboard, CSV logger. They require at least an experiment name, and directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.record import CSVLogger\n",
    "logger = CSVLogger(exp_name='my_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the logger is instantiated, logging method `log_scalar()` can be called in several places across the training example to log values such as reward, loss value, time elapsed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_scalar(\"my_scalar\", 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording videos\n",
    "In `GymEnv`, `from_pixels=True` makes env `step` function write a `pixels` entry containing images of observation, and `pixels_only=False` will indicate you want the observations to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False)\n",
    "\n",
    "print(env.rollout(max_steps=3))\n",
    "\n",
    "from torchrl.envs import TransformedEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recorder and logger can be used to save video from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.record import VideoRecorder\n",
    "\n",
    "recorder = VideoRecorder(logger, tag=\"my_video\")\n",
    "record_env = TransformedEnv(env, recorder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = record_env.rollout(max_steps=3)\n",
    "# Uncomment this line to save the video on disk:\n",
    "# recorder.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "795726461"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import time\n",
    "\n",
    "from torchrl.envs import GymEnv, StepCounter, TransformedEnv\n",
    "\n",
    "env = TransformedEnv(GymEnv(\"CartPole-v1\"), StepCounter())\n",
    "env.set_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "\n",
    "value_mlp = MLP(out_features=env.action_spec.shape[-1], num_cells=[64, 64])\n",
    "value_net = Mod(value_mlp, in_keys=[\"observation\"], out_keys=[\"action_value\"])\n",
    "\n",
    "policy = Seq(value_net, QValueModule(spec=env.action_spec))\n",
    "\n",
    "exploration_module = EGreedyModule(\n",
    "    env.action_spec, annealing_num_steps=100_000, eps_init=0.5)\n",
    "\n",
    "policy_explore = Seq(policy, exploration_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "\n",
    "init_rand_steps = 5000\n",
    "frames_per_batch = 100\n",
    "optim_steps = 10\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=-1,\n",
    "    init_random_frames=init_rand_steps,\n",
    ")\n",
    "rb = ReplayBuffer(storage=LazyTensorStorage(100_000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss module and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "\n",
    "loss = DQNLoss(value_network=policy, action_space=env.action_spec, delay_value=True)\n",
    "optim = Adam(loss.parameters(), lr=0.02)\n",
    "updater = SoftUpdate(loss, eps=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl._utils import logger as torchrl_logger\n",
    "from torchrl.record import CSVLogger, VideoRecorder\n",
    "\n",
    "path = \"./training_loop\"\n",
    "logger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\n",
    "video_recorder = VideoRecorder(logger, tag=\"video\")\n",
    "record_env = TransformedEnv(\n",
    "    GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:59:52,531 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,544 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,556 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,571 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,584 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,597 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,611 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,622 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,635 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,649 [torchrl][INFO] Max num steps: 100, rb length 5200\n",
      "2024-10-14 10:59:52,758 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,770 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,785 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,799 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,816 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,830 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,843 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,858 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,871 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:52,889 [torchrl][INFO] Max num steps: 100, rb length 5300\n",
      "2024-10-14 10:59:53,002 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,017 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,029 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,043 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,058 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,070 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,084 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,097 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,109 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,123 [torchrl][INFO] Max num steps: 100, rb length 5400\n",
      "2024-10-14 10:59:53,240 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,254 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,266 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,280 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,293 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,306 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,321 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,334 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,351 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,365 [torchrl][INFO] Max num steps: 100, rb length 5500\n",
      "2024-10-14 10:59:53,475 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,491 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,507 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,523 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,539 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,557 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,573 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,591 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,607 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,623 [torchrl][INFO] Max num steps: 100, rb length 5600\n",
      "2024-10-14 10:59:53,736 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,756 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,779 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,796 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,812 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,832 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,849 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,865 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,881 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:53,897 [torchrl][INFO] Max num steps: 100, rb length 5700\n",
      "2024-10-14 10:59:54,160 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,173 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,188 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,205 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,224 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,239 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,258 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,273 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,288 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,303 [torchrl][INFO] Max num steps: 100, rb length 5800\n",
      "2024-10-14 10:59:54,436 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,451 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,464 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,480 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,497 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,515 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,530 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,547 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,562 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,575 [torchrl][INFO] Max num steps: 100, rb length 5900\n",
      "2024-10-14 10:59:54,690 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,707 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,722 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,738 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,753 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,770 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,789 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,804 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,825 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:54,842 [torchrl][INFO] Max num steps: 100, rb length 6000\n",
      "2024-10-14 10:59:55,268 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,286 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,300 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,315 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,329 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,343 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,358 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,375 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,393 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,408 [torchrl][INFO] Max num steps: 103, rb length 6200\n",
      "2024-10-14 10:59:55,518 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,534 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,549 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,565 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,583 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,596 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,611 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,627 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,643 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,659 [torchrl][INFO] Max num steps: 174, rb length 6300\n",
      "2024-10-14 10:59:55,783 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,796 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,810 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,821 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,835 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,851 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,870 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,884 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,896 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,907 [torchrl][INFO] Max num steps: 174, rb length 6400\n",
      "2024-10-14 10:59:55,983 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:55,996 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,010 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,031 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,050 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,066 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,084 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,099 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,113 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,132 [torchrl][INFO] Max num steps: 174, rb length 6500\n",
      "2024-10-14 10:59:56,274 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,294 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,314 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,330 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,345 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,361 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,378 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,391 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,407 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,430 [torchrl][INFO] Max num steps: 174, rb length 6600\n",
      "2024-10-14 10:59:56,551 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,564 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,577 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,593 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,605 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,618 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,631 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,642 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,659 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,673 [torchrl][INFO] Max num steps: 174, rb length 6700\n",
      "2024-10-14 10:59:56,746 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,755 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,767 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,778 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,792 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,803 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,814 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,824 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,834 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,846 [torchrl][INFO] Max num steps: 174, rb length 6800\n",
      "2024-10-14 10:59:56,901 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,909 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,918 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,927 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,936 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,946 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,956 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,967 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,977 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:56,987 [torchrl][INFO] Max num steps: 174, rb length 6900\n",
      "2024-10-14 10:59:57,047 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,056 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,067 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,078 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,087 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,098 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,108 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,119 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,130 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,140 [torchrl][INFO] Max num steps: 174, rb length 7000\n",
      "2024-10-14 10:59:57,314 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,323 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,333 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,343 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,354 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,363 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,375 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,386 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,397 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,409 [torchrl][INFO] Max num steps: 174, rb length 7200\n",
      "2024-10-14 10:59:57,469 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,479 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,491 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,501 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,512 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,524 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,534 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,546 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,557 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,568 [torchrl][INFO] Max num steps: 194, rb length 7300\n",
      "2024-10-14 10:59:57,635 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,644 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,653 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,663 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,673 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,686 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,695 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,707 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,717 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,728 [torchrl][INFO] Max num steps: 194, rb length 7400\n",
      "2024-10-14 10:59:57,783 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,791 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,801 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,812 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,822 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,833 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,846 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,857 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,870 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,881 [torchrl][INFO] Max num steps: 194, rb length 7500\n",
      "2024-10-14 10:59:57,942 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:57,951 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:57,962 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:57,972 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:57,982 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:57,994 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:58,005 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:58,015 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:58,026 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:58,036 [torchrl][INFO] Max num steps: 194, rb length 7600\n",
      "2024-10-14 10:59:58,097 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,106 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,115 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,132 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,144 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,156 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,166 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,178 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,191 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,202 [torchrl][INFO] Max num steps: 194, rb length 7700\n",
      "2024-10-14 10:59:58,260 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,269 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,277 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,286 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,298 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,307 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,318 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,329 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,339 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,349 [torchrl][INFO] Max num steps: 194, rb length 7800\n",
      "2024-10-14 10:59:58,404 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,415 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,426 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,437 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,451 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,465 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,481 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,496 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,508 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,527 [torchrl][INFO] Max num steps: 194, rb length 7900\n",
      "2024-10-14 10:59:58,640 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,654 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,668 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,680 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,694 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,709 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,722 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,736 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,748 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,759 [torchrl][INFO] Max num steps: 210, rb length 8000\n",
      "2024-10-14 10:59:58,760 [torchrl][INFO] solved after 30000 steps, 1270 episodes and in 9.330539226531982s.\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "total_episodes = 0\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(collector):\n",
    "    # Write data in replay buffer\n",
    "    rb.extend(data)\n",
    "    max_length = rb[:][\"next\", \"step_count\"].max()\n",
    "    if len(rb) > init_rand_steps:\n",
    "        # Optim loop (we do several optim steps\n",
    "        # per batch collected for efficiency)\n",
    "        for _ in range(optim_steps):\n",
    "            sample = rb.sample(128)\n",
    "            loss_vals = loss(sample)\n",
    "            loss_vals[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            # Update exploration factor\n",
    "            exploration_module.step(data.numel())\n",
    "            # Update target params\n",
    "            updater.step()\n",
    "            if i % 10:\n",
    "                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n",
    "            total_count += data.numel()\n",
    "            total_episodes += data[\"next\", \"done\"].sum()\n",
    "    if max_length > 200:\n",
    "        break\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "torchrl_logger.info(\n",
    "    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_env.rollout(max_steps=1000, policy=policy)\n",
    "video_recorder.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum: writing own environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 things to take care of when designing a new environment class:\n",
    "* `EnvBase._reset()`, resetting the simulator at a (potentially random) initial state;\n",
    "* `EnvBase._step()` which codes for the state transition dynamic;\n",
    "* `EnvBase._set_seed()` which implements seeding mechanism;\n",
    "* the environment specs\n",
    "\n",
    "For the pendulum task, we need\n",
    "- a motion equation (following action)\n",
    "- a reward equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding the effect of an action: `_step()`\n",
    "The _step() method should do the following:\n",
    "\n",
    "- Read the input keys (such as \"action\") and execute the simulation based on these;\n",
    "- Retrieve observations, done state and reward;\n",
    "- Write the set of observation values along with the reward and done state at the corresponding entries in a new TensorDict.\n",
    "- Merge the output TensorDict (as \"next\" key) in the input TensorDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n",
    "\n",
    "    g_force = tensordict[\"params\", \"g\"]\n",
    "    mass = tensordict[\"params\", \"m\"]\n",
    "    length = tensordict[\"params\", \"l\"]\n",
    "    dt = tensordict[\"params\", \"dt\"]\n",
    "    u = tensordict[\"action\"].squeeze(-1)\n",
    "    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "    new_thdot = (\n",
    "        thdot\n",
    "        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n",
    "    )\n",
    "    new_thdot = new_thdot.clamp(\n",
    "        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n",
    "    )\n",
    "    new_th = th + new_thdot * dt\n",
    "    reward = -costs.view(*tensordict.shape, 1)\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": new_th,\n",
    "            \"thdot\": new_thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting the simulator: `_reset()`\n",
    "\n",
    "- _reset() writes observation entries and a done (default=False) state.\n",
    "- expects (not mandatory) a tensordict as input (beneficial in multi-agent settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "    high_th = torch.tensor(DEFAULT_X, device=self.device)\n",
    "    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n",
    "    low_th = -high_th\n",
    "    low_thdot = -high_thdot\n",
    "\n",
    "    # for non batch-locked environments, the input ``tensordict`` shape dictates the number\n",
    "    # of simulators run simultaneously. In other contexts, the initial\n",
    "    # random state's shape will depend upon the environment batch-size instead.\n",
    "    th = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_th - low_th)\n",
    "        + low_th\n",
    "    )\n",
    "    thdot = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_thdot - low_thdot)\n",
    "        + low_thdot\n",
    "    )\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": th,\n",
    "            \"thdot\": thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment metadata: `env.*_spec`\n",
    "\n",
    "The specs define the input and output domain of the environment. They can also be used to instantiate lazily defined neural networks and test scripts. There are four specs that we must code in our environment:\n",
    "\n",
    "- `EnvBase.observation_spec`: This will be a `CompositeSpec` instance where each key is an observation (a CompositeSpec can be viewed as a dictionary of specs).\n",
    "- `EnvBase.action_spec`: It can be any type of spec, it corresponds to the \"action\" entry in the input tensordict;\n",
    "- `EnvBase.reward_spec`: provides information about the reward space;\n",
    "- `EnvBase.done_spec`: provides information about the space of the done flag.\n",
    "\n",
    "TorchRL specs are organized in two general containers:\n",
    "- input_spec which contains the specs of the information that the step function reads (divided between action_spec containing the action and state_spec containing all the rest),\n",
    "- output_spec which encodes the specs that the step outputs (observation_spec, reward_spec and done_spec).\n",
    "\n",
    "In general, you should not interact directly with output_spec and input_spec but only with their content: observation_spec, reward_spec, done_spec, action_spec and state_spec. TorchRL offers multiple TensorSpec subclasses to encode the environment’s input and output characteristics.\n",
    "\n",
    "##### Specs shape\n",
    "The environment specs leading dimensions must match the environment batch-size. This is done to enforce that every component of an environment (including its transforms) have an accurate representation of the expected input and output shapes. This is something that should be accurately coded in stateful settings. For non batch-locked environments, such as the one in our example (see below), this is irrelevant as the environment batch size will most likely be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    # Under the hood, this will populate self.output_spec[\"observation\"]\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        th=BoundedTensorSpec(\n",
    "            low=-torch.pi,\n",
    "            high=torch.pi,\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        thdot=BoundedTensorSpec(\n",
    "            low=-td_params[\"params\", \"max_speed\"],\n",
    "            high=td_params[\"params\", \"max_speed\"],\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        # we need to add the ``params`` to the observation specs, as we want\n",
    "        # to pass it at each step during a rollout\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "    # since the environment is stateless, we expect the previous output as input.\n",
    "    # For this, ``EnvBase`` expects some state_spec to be available\n",
    "    self.state_spec = self.observation_spec.clone()\n",
    "    # action-spec will be automatically wrapped in input_spec when\n",
    "    # `self.action_spec = spec` will be called supported\n",
    "    self.action_spec = BoundedTensorSpec(\n",
    "        low=-td_params[\"params\", \"max_torque\"],\n",
    "        high=td_params[\"params\", \"max_torque\"],\n",
    "        shape=(1,),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducible experiments: seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `EnvBase` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"max_speed\": 8,\n",
    "                    \"max_torque\": 2.0,\n",
    "                    \"dt\": 0.05,\n",
    "                    \"g\": g,\n",
    "                    \"m\": 1.0,\n",
    "                    \"l\": 1.0,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the environment as non-batch_locked by turning the homonymous attribute to False. This means that we will not enforce the input tensordict to have a batch-size that matches the one of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing environment\n",
    "TorchRL provides a simple function `check_env_specs()` to check that a (transformed) environment has an input/output structure that matches the one dictated by its specs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PendulumEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at our specs to have a visual representation of the environment signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a couple of commands too to check that the output structure matches what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the env.rand_step() to generate an action randomly from the action_spec domain. A tensordict containing the hyperparameters and the current state must be passed since our environment is stateless. In stateful contexts, env.rand_step() works perfectly too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming an environment\n",
    "\n",
    "Writing environment transforms for stateless simulators is slightly more complicated than for stateful ones: transforming an output entry that needs to be read at the following iteration requires to apply the inverse transform before calling meth.step() at the next step. This is an ideal scenario to showcase all the features of TorchRL’s transforms!\n",
    "\n",
    "For instance, in the following transformed environment we unsqueeze the entries [\"th\", \"thdot\"] to be able to stack them along the last dimension. We also pass them as in_keys_inv to squeeze them back to their original shape once they are passed as input in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    # ``Unsqueeze`` the observations that we will concatenate\n",
    "    UnsqueezeTransform(\n",
    "        unsqueeze_dim=-1,\n",
    "        in_keys=[\"th\", \"thdot\"],\n",
    "        in_keys_inv=[\"th\", \"thdot\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing custom transforms\n",
    "TorchRL’s transforms may not cover all the operations one wants to execute after an environment has been executed. Writing a transform does not require much effort. As for the environment design, there are two steps in writing a transform:\n",
    "\n",
    "- Getting the dynamics right (forward and inverse);\n",
    "- Adapting the environment specs.\n",
    "\n",
    "A transform can be used in two settings: on its own, it can be used as a `Module`. It can also be used appended to a `TransformedEnv`. The structure of the class allows to customize the behavior in the different contexts.\n",
    "\n",
    "A `Transform` skeleton can be summarized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(nn.Module):\n",
    "    def forward(self, tensordict):\n",
    "        ...\n",
    "    def _apply_transform(self, tensordict):\n",
    "        ...\n",
    "    def _step(self, tensordict):\n",
    "        ...\n",
    "    def _call(self, tensordict):\n",
    "        ...\n",
    "    def inv(self, tensordict):\n",
    "        ...\n",
    "    def _inv_apply_transform(self, tensordict):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three entry points (`forward()`, `_step()` and `inv()`) which all receive `tensordict.TensorDict` instances. The first two will eventually go through the keys indicated by in_keys and call `_apply_transform()` to each of these. The results will be written in the entries pointed by `Transform.out_keys` if provided (if not the `in_keys` will be updated with the transformed values). If inverse transforms need to be executed, a similar data flow will be executed but with the `Transform.inv()` and `Transform._inv_apply_transform()` methods and across the `in_keys_inv` and `out_keys_inv` list of keys. The following figure summarized this flow for environments and replay buffers.\n",
    "\n",
    "In some cases, a transform will not work on a subset of keys in a unitary manner, but will execute some operation on the parent environment or work with the entire input tensordict. In those cases, the `_call()` and `forward()` methods should be re-written, and the `_apply_transform()` method can be skipped.\n",
    "\n",
    "Let us code new transforms that will compute the sine and cosine values of the position angle, as these values are more useful to us to learn a policy than the raw angle value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.sin()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "class CosTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.cos()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "t_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\n",
    "t_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\n",
    "env.append_transform(t_sin)\n",
    "env.append_transform(t_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenates the observations onto an “observation” entry. `del_keys=False` ensures that we keep these values for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transform = CatTensors(\n",
    "    in_keys=[\"sin\", \"cos\", \"thdot\"], dim=-1, out_key=\"observation\", del_keys=False\n",
    ")\n",
    "env.append_transform(cat_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, let us check that our environment specs match what is received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing a rollout\n",
    "Executing a rollout is a succession of simple steps:\n",
    "- reset the environment\n",
    "- while some condition is not met:\n",
    "    - compute an action given a policy\n",
    "    - execute a step given this action\n",
    "    - collect the data\n",
    "    - make a MDP step\n",
    "- gather the data and return\n",
    "\n",
    "These operations have been conveniently wrapped in the `rollout()` method, from which we provide a simplified version here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rollout(steps=100):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching computations\n",
    "\n",
    "The last unexplored end of our tutorial is the ability that we have to batch computations in TorchRL. Because our environment does not make any assumptions regarding the input data shape, we can seamlessly execute it over batches of data. Even better: for non-batch-locked environments such as our Pendulum, we can change the batch size on the fly without recreating the environment. To do this, we just generate parameters with the desired shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(\"reset (batch size of 10)\", td)\n",
    "td = env.rand_step(td)\n",
    "print(\"rand step (batch size of 10)\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a rollout with a batch of data requires us to reset the environment out of the rollout function, since we need to define the batch_size dynamically and this is not supported by `rollout()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(\n",
    "    3,\n",
    "    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n",
    "    tensordict=env.reset(env.gen_params(batch_size=[batch_size])),\n",
    ")\n",
    "print(\"rollout of len 3 (batch size of 10):\", rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a simple policy\n",
    "In this example, we will train a simple policy using the reward as a differentiable objective, such as a negative loss. We will take advantage of the fact that our dynamic system is fully differentiable to backpropagate through the trajectory return and adjust the weights of our policy to maximize this value directly. Of course, in many settings many of the assumptions we make do not hold, such as differentiable system and full access to the underlying mechanics.%0A%0AStill, this is a very simple example that showcases how a training loop can be coded with a custom environment in TorchRL.%0A%0ALet us first write the policy network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "We will successively:\n",
    "\n",
    "- generate a trajectory\n",
    "- sum the rewards\n",
    "- backpropagate through the graph defined by these operations\n",
    "- clip the gradient norm and make an optimization step\n",
    "- repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to 0 which demonstrates that the pendulum is upward and still as desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we have learned how to code a stateless environment from scratch. We touched the subjects of:\n",
    "\n",
    "The four essential components that need to be taken care of when coding an environment (step, reset, seeding and building specs). We saw how these methods and classes interact with the TensorDict class;\n",
    "\n",
    "How to test that an environment is properly coded using check_env_specs();\n",
    "\n",
    "How to append transforms in the context of stateless environments and how to write custom transformations;\n",
    "\n",
    "How to train a policy on a fully differentiable simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch old RL tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Tutorial on CartPole\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find more\n",
    "information about the environment and other more challenging\n",
    "environments at [Gymnasium\\'s\n",
    "website](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "![CartPole](https://pytorch.org/tutorials/_static/img/cartpole.gif)\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more than\n",
    "2.4 units away from center. This means better performing scenarios will\n",
    "run for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.). We\n",
    "take these 4 inputs without any scaling and pass them through a small\n",
    "fully-connected network with 2 outputs, one for each action. The network\n",
    "is trained to predict the expected value for each action, given the\n",
    "input state. The action with the highest expected value is then chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "We\\'ll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we\\'re going to need two classes:\n",
    "\n",
    "-   `Transition` - a named tuple representing a single transition in our\n",
    "    environment. It essentially maps (state, action) pairs to their\n",
    "    (next\\_state, reward) result, with the state being the screen\n",
    "    difference image as described later on.\n",
    "-   `ReplayMemory` - a cyclic buffer of bounded size that holds the\n",
    "    transitions observed recently. It also implements a `.sample()`\n",
    "    method for selecting a random batch of transitions for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # deque is list like data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let\\'s define our model. But first, let\\'s quickly recap what a DQN\n",
    "is.\n",
    "\n",
    "### DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where $R_{t_0}$\n",
    "is also known as the *return*. The discount, $\\gamma$, should be a\n",
    "constant between $0$ and $1$ that ensures the sum converges. A lower\n",
    "$\\gamma$ makes rewards from the uncertain far future less important for\n",
    "our agent than the ones in the near future that it can be fairly\n",
    "confident about. It also encourages agents to collect reward closer in\n",
    "time than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell us\n",
    "what our return would be, if we were to take an action in a given state,\n",
    "then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "However, we don\\'t know everything about the world, so we don\\'t have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For our training update rule, we\\'ll use a fact that every $Q$ function\n",
    "for some policy obeys the Bellman equation:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))$$\n",
    "\n",
    "To minimize this error, we will use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate this\n",
    "over a batch of transitions, $B$, sampled from the replay memory:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "#### Q-network\n",
    "\n",
    "Our model will be a feed forward neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and $Q(s, \\mathrm{right})$\n",
    "(where $s$ is the input to the network). In effect, the network is\n",
    "trying to predict the *expected return* of taking each action given the\n",
    "current input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Hyperparameters and utilities\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-   `select_action` - will select an action according to an epsilon\n",
    "    greedy policy. Simply put, we\\'ll sometimes use our model for\n",
    "    choosing the action, and sometimes we\\'ll just sample one uniformly.\n",
    "    The probability of choosing a random action will start at\n",
    "    `EPS_START` and will decay exponentially towards `EPS_END`.\n",
    "    `EPS_DECAY` controls the rate of the decay.\n",
    "-   `plot_durations` - a helper for plotting the duration of episodes,\n",
    "    along with an average over the last 100 episodes (the measure used\n",
    "    in the official evaluations). The plot will be underneath the cell\n",
    "    containing the main training loop, and will update after every\n",
    "    episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an `optimize_model` function that performs a single\n",
    "step of the optimization. It first samples a batch, concatenates all the\n",
    "tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our loss. By\n",
    "definition we set $V(s) = 0$ if $s$ is a terminal state. We also use a\n",
    "target network to compute $V(s_{t+1})$ for added stability. The target\n",
    "network is updated at every step with a [soft\n",
    "update](https://arxiv.org/pdf/1509.02971.pdf) controlled by the\n",
    "hyperparameter `TAU`, which was previously defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and obtain the initial `state` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always 1),\n",
    "and optimize our model once. When the episode ends (our model fails), we\n",
    "restart the loop.\n",
    "\n",
    "Below, [num\\_episodes]{.title-ref} is set to 600 if a GPU is available,\n",
    "otherwise 50 episodes are scheduled so training does not take too long.\n",
    "However, 50 episodes is insufficient for to observe good performance on\n",
    "CartPole. You should see the model constantly achieve 500 steps within\n",
    "600 training episodes. Training RL agents can be a noisy process, so\n",
    "restarting training can produce better results if convergence is not\n",
    "observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the diagram that illustrates the overall resulting data flow.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/reinforcement_learning_diagram.jpg)\n",
    "\n",
    "Actions are chosen either randomly or based on a policy, getting the\n",
    "next step sample from the gym environment. We record the results in the\n",
    "replay memory and also run optimization step on every iteration.\n",
    "Optimization picks a random batch from the replay memory to do training\n",
    "of the new policy. The \\\"older\\\" target\\_net is also used in\n",
    "optimization to compute the expected Q values. A soft update of its\n",
    "weights are performed at every step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = env.reset() # 4 real values to express env (pos, vel ,etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "# Replay memory is composed of transitions, generated by play, used for training. It has push, sample, len methods.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # deque is list like data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "# DQN algorithm\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    # forward is used with one x to determine next action,\n",
    "    # or during optimization with a batch of inputs.\n",
    "    # returns tensor([[left0exp, right0exp], [left1exp, right1exp]...])\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x) # the value prediction is passed as it is\n",
    "    \n",
    "    # Training\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99 # discount factor\n",
    "EPS_START = 0.9 # high exploration at first\n",
    "EPS_END = 0.05 # non-zero exploration later\n",
    "EPD_DECAY = 1000 # 1000 steps for decaying\n",
    "TAU = 0.005 # update rate of target network\n",
    "LR = 1e-4 # learning rate of optimizer\n",
    "\n",
    "# number of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) # same initial parameters\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memoery = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# action selection based on epsilon greedy\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    esp_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > esp_threshold:\n",
    "        with torch.no_grad():\n",
    "            # max(0) is largest of the two values, max(1) is the index\n",
    "            #\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "## Training\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "## Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(10):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated: next_state = None\n",
    "        else: next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t+1)\n",
    "            break\n",
    "\n",
    "# Optimization\n",
    "\n",
    "batch = Transition(*zip(*memory.sample(5)))\n",
    "\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0863],\n",
       "        [0.0938],\n",
       "        [0.0910],\n",
       "        [0.0911],\n",
       "        [0.0877]], device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9998, 1.0111, 1.0079, 1.0035, 1.0022], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_state_values = torch.zeros(5, device=device)\n",
    "with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "expected_state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "next_state_values = torch.zeros(5, device=device)\n",
    "with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "loss\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning (PPO) with TorchRL Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use PyTorch and\n",
    ":py`torchrl`{.interpreted-text role=\"mod\"} to train a parametric policy\n",
    "network to solve the Inverted Pendulum task from the\n",
    "[OpenAI-Gym/Farama-Gymnasium control\n",
    "library](https://github.com/Farama-Foundation/Gymnasium).\n",
    "\n",
    "![Inverted\n",
    "pendulum](https://pytorch.org/tutorials/_static/img/invpendulum.gif)\n",
    "\n",
    "Key learnings:\n",
    "\n",
    "-   How to create an environment in TorchRL, transform its outputs, and\n",
    "    collect data from this environment;\n",
    "-   How to make your classes talk to each other using\n",
    "    `~tensordict.TensorDict`{.interpreted-text role=\"class\"};\n",
    "-   The basics of building your training loop with TorchRL:\n",
    "    -   How to compute the advantage signal for policy gradient methods;\n",
    "    -   How to create a stochastic policy using a probabilistic neural\n",
    "        network;\n",
    "    -   How to create a dynamic replay buffer and sample from it without\n",
    "        repetition.\n",
    "\n",
    "We will cover six crucial components of TorchRL:\n",
    "\n",
    "-   [environments](https://pytorch.org/rl/reference/envs.html)\n",
    "-   [transforms](https://pytorch.org/rl/reference/envs.html#transforms)\n",
    "-   [models (policy and value\n",
    "    function)](https://pytorch.org/rl/reference/modules.html)\n",
    "-   [loss modules](https://pytorch.org/rl/reference/objectives.html)\n",
    "-   [data collectors](https://pytorch.org/rl/reference/collectors.html)\n",
    "-   [replay\n",
    "    buffers](https://pytorch.org/rl/reference/data.html#replay-buffers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this in Google Colab, make sure you install the\n",
    "following dependencies:\n",
    "\n",
    "``` {.sourceCode .bash}\n",
    "!pip3 install torchrl\n",
    "!pip3 install gym[mujoco]\n",
    "!pip3 install tqdm\n",
    "```\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where\n",
    "a batch of data is being collected and directly consumed to train the\n",
    "policy to maximise the expected return given some proximality\n",
    "constraints. You can think of it as a sophisticated version of\n",
    "[REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf),\n",
    "the foundational policy-optimization algorithm. For more information,\n",
    "see the [Proximal Policy Optimization\n",
    "Algorithms](https://arxiv.org/abs/1707.06347) paper.\n",
    "\n",
    "PPO is usually regarded as a fast and efficient method for online,\n",
    "on-policy reinforcement algorithm. TorchRL provides a loss-module that\n",
    "does all the work for you, so that you can rely on this implementation\n",
    "and focus on solving your problem rather than re-inventing the wheel\n",
    "every time you want to train a policy.\n",
    "\n",
    "For completeness, here is a brief overview of what the loss computes,\n",
    "even though this is taken care of by our\n",
    "`~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"}\n",
    "module---the algorithm works as follows: 1. we will sample a batch of\n",
    "data by playing the policy in the environment for a given number of\n",
    "steps. 2. Then, we will perform a given number of optimization steps\n",
    "with random sub-samples of this batch using a clipped version of the\n",
    "REINFORCE loss. 3. The clipping will put a pessimistic bound on our\n",
    "loss: lower return estimates will be favored compared to higher ones.\n",
    "The precise formula of the loss is:\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n",
    "\\right),$$\n",
    "\n",
    "There are two components in that loss: in the first part of the minimum\n",
    "operator, we simply compute an importance-weighted version of the\n",
    "REINFORCE loss (for example, a REINFORCE loss that we have corrected for\n",
    "the fact that the current policy configuration lags the one that was\n",
    "used for the data collection). The second part of that minimum operator\n",
    "is a similar loss where we have clipped the ratios when they exceeded or\n",
    "were below a given pair of thresholds.\n",
    "\n",
    "This loss ensures that whether the advantage is positive or negative,\n",
    "policy updates that would produce significant shifts from the previous\n",
    "configuration are being discouraged.\n",
    "\n",
    "This tutorial is structured as follows:\n",
    "\n",
    "1.  First, we will define a set of hyperparameters we will be using for\n",
    "    training.\n",
    "2.  Next, we will focus on creating our environment, or simulator, using\n",
    "    TorchRL\\'s wrappers and transforms.\n",
    "3.  Next, we will design the policy network and the value model, which\n",
    "    is indispensable to the loss function. These modules will be used to\n",
    "    configure our loss module.\n",
    "4.  Next, we will create the replay buffer and data loader.\n",
    "5.  Finally, we will run our training loop and analyze the results.\n",
    "\n",
    "Throughout this tutorial, we\\'ll be using the\n",
    "`tensordict`{.interpreted-text role=\"mod\"} library.\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} is the lingua\n",
    "franca of TorchRL: it helps us abstract what a module reads and writes\n",
    "and care less about the specific data description and more about the\n",
    "algorithm itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters\n",
    "\n",
    "We set the hyperparameters for our algorithm. Depending on the resources\n",
    "available, one may choose to execute the policy on GPU or on another\n",
    "device. The `frame_skip` will control how for how many frames is a\n",
    "single action being executed. The rest of the arguments that count\n",
    "frames must be corrected for this value (since one environment step will\n",
    "actually return `frame_skip` frames).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection parameters\n",
    "\n",
    "When collecting data, we will be able to choose how big each batch will\n",
    "be by defining a `frames_per_batch` parameter. We will also define how\n",
    "many frames (such as the number of interactions with the simulator) we\n",
    "will allow ourselves to use. In general, the goal of an RL algorithm is\n",
    "to learn to solve the task as fast as it can in terms of environment\n",
    "interactions: the lower the `total_frames` the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO parameters\n",
    "\n",
    "At each data collection (or batch collection) we will run the\n",
    "optimization over a certain number of *epochs*, each time consuming the\n",
    "entire data we just acquired in a nested training loop. Here, the\n",
    "`sub_batch_size` is different from the `frames_per_batch` here above:\n",
    "recall that we are working with a \\\"batch of data\\\" coming from our\n",
    "collector, which size is defined by `frames_per_batch`, and that we will\n",
    "further split in smaller sub-batches during the inner training loop. The\n",
    "size of these sub-batches is controlled by `sub_batch_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an environment\n",
    "\n",
    "In RL, an *environment* is usually the way we refer to a simulator or a\n",
    "control system. Various libraries provide simulation environments for\n",
    "reinforcement learning, including Gymnasium (previously OpenAI Gym),\n",
    "DeepMind control suite, and many others. As a general library,\n",
    "TorchRL\\'s goal is to provide an interchangeable interface to a large\n",
    "panel of RL simulators, allowing you to easily swap one environment with\n",
    "another. For example, creating a wrapped gym environment can be achieved\n",
    "with few characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to notice in this code: first, we created the\n",
    "environment by calling the `GymEnv` wrapper. If extra keyword arguments\n",
    "are passed, they will be transmitted to the `gym.make` method, hence\n",
    "covering the most common environment construction commands.\n",
    "Alternatively, one could also directly create a gym environment using\n",
    "`gym.make(env_name, **kwargs)` and wrap it in a [GymWrapper]{.title-ref}\n",
    "class.\n",
    "\n",
    "Also the `device` argument: for gym, this only controls the device where\n",
    "input action and observed states will be stored, but the execution will\n",
    "always be done on CPU. The reason for this is simply that gym does not\n",
    "support on-device execution, unless specified otherwise. For other\n",
    "libraries, we have control over the execution device and, as much as we\n",
    "can, we try to stay consistent in terms of storing and execution\n",
    "backends.\n",
    "\n",
    "### Transforms\n",
    "\n",
    "We will append some transforms to our environments to prepare the data\n",
    "for the policy. In Gym, this is usually achieved via wrappers. TorchRL\n",
    "takes a different approach, more similar to other pytorch domain\n",
    "libraries, through the use of transforms. To add transforms to an\n",
    "environment, one should simply wrap it in a\n",
    "`~torchrl.envs.transforms.TransformedEnv`{.interpreted-text\n",
    "role=\"class\"} instance and append the sequence of transforms to it. The\n",
    "transformed environment will inherit the device and meta-data of the\n",
    "wrapped environment, and transform these depending on the sequence of\n",
    "transforms it contains.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "The first to encode is a normalization transform. As a rule of thumbs,\n",
    "it is preferable to have data that loosely match a unit Gaussian\n",
    "distribution: to obtain this, we will run a certain number of random\n",
    "steps in the environment and compute the summary statistics of these\n",
    "observations.\n",
    "\n",
    "We\\'ll append two other transforms: the\n",
    "`~torchrl.envs.transforms.DoubleToFloat`{.interpreted-text role=\"class\"}\n",
    "transform will convert double entries to single-precision numbers, ready\n",
    "to be read by the policy. The\n",
    "`~torchrl.envs.transforms.StepCounter`{.interpreted-text role=\"class\"}\n",
    "transform will be used to count the steps before the environment is\n",
    "terminated. We will use this measure as a supplementary measure of\n",
    "performance.\n",
    "\n",
    "As we will see later, many of the TorchRL\\'s classes rely on\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} to communicate.\n",
    "You could think of it as a python dictionary with some extra tensor\n",
    "features. In practice, this means that many modules we will be working\n",
    "with need to be told what key to read (`in_keys`) and what key to write\n",
    "(`out_keys`) in the `tensordict` they will receive. Usually, if\n",
    "`out_keys` is omitted, it is assumed that the `in_keys` entries will be\n",
    "updated in-place. For our transforms, the only entry we are interested\n",
    "in is referred to as `\"observation\"` and our transform layers will be\n",
    "told to modify this entry and this entry only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, we have created a normalization layer but we\n",
    "did not set its normalization parameters. To do this,\n",
    "`~torchrl.envs.transforms.ObservationNorm`{.interpreted-text\n",
    "role=\"class\"} can automatically gather the summary statistics of our\n",
    "environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `~torchrl.envs.transforms.ObservationNorm`{.interpreted-text\n",
    "role=\"class\"} transform has now been populated with a location and a\n",
    "scale that will be used to normalize the data.\n",
    "\n",
    "Let us do a little sanity check for the shape of our summary stats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant shape: torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An environment is not only defined by its simulator and transforms, but\n",
    "also by a series of metadata that describe what can be expected during\n",
    "its execution. For efficiency purposes, TorchRL is quite stringent when\n",
    "it comes to environment specs, but you can easily check that your\n",
    "environment specs are adequate. In our example, the\n",
    "`~torchrl.envs.libs.gym.GymWrapper`{.interpreted-text role=\"class\"} and\n",
    "`~torchrl.envs.libs.gym.GymEnv`{.interpreted-text role=\"class\"} that\n",
    "inherits from it already take care of setting the proper specs for your\n",
    "environment so you should not have to care about this.\n",
    "\n",
    "Nevertheless, let\\'s see a concrete example using our transformed\n",
    "environment by looking at its specs. There are three specs to look at:\n",
    "`observation_spec` which defines what is to be expected when executing\n",
    "an action in the environment, `reward_spec` which indicates the reward\n",
    "domain and finally the `input_spec` (which contains the `action_spec`)\n",
    "and which represents everything an environment requires to execute a\n",
    "single step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    observation: UnboundedContinuousTensorSpec(\n",
      "        shape=torch.Size([11]),\n",
      "        space=None,\n",
      "        device=cuda:0,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    step_count: BoundedTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.int64,\n",
      "        domain=continuous), device=cuda:0, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n",
      "input_spec: CompositeSpec(\n",
      "    full_state_spec: CompositeSpec(\n",
      "        step_count: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.int64,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([])),\n",
      "    full_action_spec: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([])), device=cuda:0, shape=torch.Size([]))\n",
      "action_spec (as defined by input_spec): BoundedTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "    device=cuda:0,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `check_env_specs`{.interpreted-text role=\"func\"} function runs a\n",
    "small rollout and compares its output against the environment specs. If\n",
    "no error is raised, we can be confident that the specs are properly\n",
    "defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 20:28:55,992 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let\\'s see what a simple random rollout looks like. You can\n",
    "call [env.rollout(n\\_steps)]{.title-ref} and get an overview of what the\n",
    "environment inputs and outputs look like. Actions will automatically be\n",
    "drawn from the action spec domain, so you don\\'t need to care about\n",
    "designing a random sampler.\n",
    "\n",
    "Typically, at each step, an RL environment receives an action as input,\n",
    "and outputs an observation, a reward and a done state. The observation\n",
    "may be composite, meaning that it could be composed of more than one\n",
    "tensor. This is not a problem for TorchRL, since the whole set of\n",
    "observations is automatically packed in the output\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"}. After\n",
    "executing a rollout (for example, a sequence of environment steps and\n",
    "random action generations) over a given number of steps, we will\n",
    "retrieve a `~tensordict.TensorDict`{.interpreted-text role=\"class\"}\n",
    "instance with a shape that matches this trajectory length:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Shape of the rollout TensorDict: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rollout data has a shape of `torch.Size([3])`, which matches the\n",
    "number of steps we ran it for. The `\"next\"` entry points to the data\n",
    "coming after the current step. In most cases, the `\"next\"` data at time\n",
    "[t]{.title-ref} matches the data at `t+1`, but this may not be the case\n",
    "if we are using some specific transformations (for example, multi-step).\n",
    "\n",
    "### Policy\n",
    "\n",
    "PPO utilizes a stochastic policy to handle exploration. This means that\n",
    "our neural network will have to output the parameters of a distribution,\n",
    "rather than a single value corresponding to the action taken.\n",
    "\n",
    "As the data is continuous, we use a Tanh-Normal distribution to respect\n",
    "the action space boundaries. TorchRL provides such distribution, and the\n",
    "only thing we need to care about is to build a neural network that\n",
    "outputs the right number of parameters for the policy to work with (a\n",
    "location, or mean, and a scale):\n",
    "\n",
    "$$f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})$$\n",
    "\n",
    "The only extra-difficulty that is brought up here is to split our output\n",
    "in two equal parts and map the second to a strictly positive space.\n",
    "\n",
    "We design the policy in three steps:\n",
    "\n",
    "1.  Define a neural network `D_obs` -\\> `2 * D_action`. Indeed, our\n",
    "    `loc` (mu) and `scale` (sigma) both have dimension `D_action`.\n",
    "2.  Append a\n",
    "    `~tensordict.nn.distributions.NormalParamExtractor`{.interpreted-text\n",
    "    role=\"class\"} to extract a location and a scale (for example, splits\n",
    "    the input in two equal parts and applies a positive transformation\n",
    "    to the scale parameter).\n",
    "3.  Create a probabilistic\n",
    "    `~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}\n",
    "    that can generate this distribution and sample from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable the policy to \\\"talk\\\" with the environment through the\n",
    "`tensordict` data carrier, we wrap the `nn.Module` in a\n",
    "`~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}. This\n",
    "class will simply ready the `in_keys` it is provided with and write the\n",
    "outputs in-place at the registered `out_keys`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build a distribution out of the location and scale of our\n",
    "normal distribution. To do so, we instruct the\n",
    "`~torchrl.modules.tensordict_module.ProbabilisticActor`{.interpreted-text\n",
    "role=\"class\"} class to build a\n",
    "`~torchrl.modules.TanhNormal`{.interpreted-text role=\"class\"} out of the\n",
    "location and scale parameters. We also provide the minimum and maximum\n",
    "values of this distribution, which we gather from the environment specs.\n",
    "\n",
    "The name of the `in_keys` (and hence the name of the `out_keys` from the\n",
    "`~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"} above)\n",
    "cannot be set to any value one may like, as the\n",
    "`~torchrl.modules.TanhNormal`{.interpreted-text role=\"class\"}\n",
    "distribution constructor will expect the `loc` and `scale` keyword\n",
    "arguments. That being said,\n",
    "`~torchrl.modules.tensordict_module.ProbabilisticActor`{.interpreted-text\n",
    "role=\"class\"} also accepts `Dict[str, str]` typed `in_keys` where the\n",
    "key-value pair indicates what `in_key` string should be used for every\n",
    "keyword argument that is to be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value network\n",
    "\n",
    "The value network is a crucial component of the PPO algorithm, even\n",
    "though it won\\'t be used at inference time. This module will read the\n",
    "observations and return an estimation of the discounted return for the\n",
    "following trajectory. This allows us to amortize learning by relying on\n",
    "the some utility estimation that is learned on-the-fly during training.\n",
    "Our value network share the same structure as the policy, but for\n",
    "simplicity we assign it its own set of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let\\'s try our policy and value modules. As we said earlier, the usage\n",
    "of `~tensordict.nn.TensorDictModule`{.interpreted-text role=\"class\"}\n",
    "makes it possible to directly read the output of the environment to run\n",
    "these modules, as they know what information to read and where to write\n",
    "it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector\n",
    "\n",
    "TorchRL provides a set of [DataCollector\n",
    "classes](https://pytorch.org/rl/reference/collectors.html). Briefly,\n",
    "these classes execute three operations: reset an environment, compute an\n",
    "action given the latest observation, execute a step in the environment,\n",
    "and repeat the last two steps until the environment signals a stop (or\n",
    "reaches a done state).\n",
    "\n",
    "They allow you to control how many frames to collect at each iteration\n",
    "(through the `frames_per_batch` parameter), when to reset the\n",
    "environment (through the `max_frames_per_traj` argument), on which\n",
    "`device` the policy should be executed, etc. They are also designed to\n",
    "work efficiently with batched and multiprocessed environments.\n",
    "\n",
    "The simplest data collector is the\n",
    "`~torchrl.collectors.collectors.SyncDataCollector`{.interpreted-text\n",
    "role=\"class\"}: it is an iterator that you can use to get batches of data\n",
    "of a given length, and that will stop once a total number of frames\n",
    "(`total_frames`) have been collected. Other data collectors\n",
    "(`~torchrl.collectors.collectors.MultiSyncDataCollector`{.interpreted-text\n",
    "role=\"class\"} and\n",
    "`~torchrl.collectors.collectors.MultiaSyncDataCollector`{.interpreted-text\n",
    "role=\"class\"}) will execute the same operations in synchronous and\n",
    "asynchronous manner over a set of multiprocessed workers.\n",
    "\n",
    "As for the policy and environment before, the data collector will return\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} instances with\n",
    "a total number of elements that will match `frames_per_batch`. Using\n",
    "`~tensordict.TensorDict`{.interpreted-text role=\"class\"} to pass data to\n",
    "the training loop allows you to write data loading pipelines that are\n",
    "100% oblivious to the actual specificities of the rollout content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "\n",
    "Replay buffers are a common building piece of off-policy RL algorithms.\n",
    "In on-policy contexts, a replay buffer is refilled every time a batch of\n",
    "data is collected, and its data is repeatedly consumed for a certain\n",
    "number of epochs.\n",
    "\n",
    "TorchRL\\'s replay buffers are built using a common container\n",
    "`~torchrl.data.ReplayBuffer`{.interpreted-text role=\"class\"} which takes\n",
    "as argument the components of the buffer: a storage, a writer, a sampler\n",
    "and possibly some transforms. Only the storage (which indicates the\n",
    "replay buffer capacity) is mandatory. We also specify a sampler without\n",
    "repetition to avoid sampling multiple times the same item in one epoch.\n",
    "Using a replay buffer for PPO is not mandatory and we could simply\n",
    "sample the sub-batches from the collected batch, but using these classes\n",
    "make it easy for us to build the inner training loop in a reproducible\n",
    "way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "The PPO loss can be directly imported from TorchRL for convenience using\n",
    "the `~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"}\n",
    "class. This is the easiest way of utilizing PPO: it hides away the\n",
    "mathematical operations of PPO and the control flow that goes with it.\n",
    "\n",
    "PPO requires some \\\"advantage estimation\\\" to be computed. In short, an\n",
    "advantage is a value that reflects an expectancy over the return value\n",
    "while dealing with the bias / variance tradeoff. To compute the\n",
    "advantage, one just needs to (1) build the advantage module, which\n",
    "utilizes our value operator, and (2) pass each batch of data through it\n",
    "before each epoch. The GAE module will update the input `tensordict`\n",
    "with new `\"advantage\"` and `\"value_target\"` entries. The\n",
    "`\"value_target\"` is a gradient-free tensor that represents the empirical\n",
    "value that the value network should represent with the input\n",
    "observation. Both of these will be used by\n",
    "`~torchrl.objectives.ClipPPOLoss`{.interpreted-text role=\"class\"} to\n",
    "return the policy and value losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "We now have all the pieces needed to code our training loop. The steps\n",
    "include:\n",
    "\n",
    "-   Collect data\n",
    "    -   Compute advantage\n",
    "        -   Loop over the collected to compute loss values\n",
    "        -   Back propagate\n",
    "        -   Optimize\n",
    "        -   Repeat\n",
    "    -   Repeat\n",
    "-   Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval cumulative reward:  91.9957 (init:  91.9957), eval step-count: 9, average reward=9.2303(init=9.0693), step count (max): 32, lr policy:  0.0003:  14%|█▍        | 7000/50000 [00:33<03:24, 210.36it/s]\n",
      "eval cumulative reward:  1344.8938 (init:  250.7185), eval step-count: 143, average reward= 9.3147 (init= 9.2526), step count (max): 111, lr policy:  0.0000:  84%|████████▍ | 42000/50000 [02:32<00:31, 253.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval cumulative reward:  1344.8938 (init:  250.7185), eval step-count: 143, average reward= 9.3147 (init= 9.2526), step count (max): 111, lr policy:  0.0000:  84%|████████▍ | 42000/50000 [02:44<00:31, 253.02it/s]"
     ]
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Before the 1M step cap is reached, the algorithm should have reached a\n",
    "max step count of 1000 steps, which is the maximum number of steps\n",
    "before the trajectory is truncated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and next steps\n",
    "\n",
    "In this tutorial, we have learned:\n",
    "\n",
    "1.  How to create and customize an environment with\n",
    "    :py`torchrl`{.interpreted-text role=\"mod\"};\n",
    "2.  How to write a model and a loss function;\n",
    "3.  How to set up a typical training loop.\n",
    "\n",
    "If you want to experiment with this tutorial a bit more, you can apply\n",
    "the following modifications:\n",
    "\n",
    "-   From an efficiency perspective, we could run several simulations in\n",
    "    parallel to speed up data collection. Check\n",
    "    `~torchrl.envs.ParallelEnv`{.interpreted-text role=\"class\"} for\n",
    "    further information.\n",
    "-   From a logging perspective, one could add a\n",
    "    `torchrl.record.VideoRecorder`{.interpreted-text role=\"class\"}\n",
    "    transform to the environment after asking for rendering to get a\n",
    "    visual rendering of the inverted pendulum in action. Check\n",
    "    :py`torchrl.record`{.interpreted-text role=\"mod\"} to know more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hyperparameters\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# 1.1 Data collection parameters\n",
    "frames_per_batch = 1000\n",
    "total_frames = 50_000\n",
    "\n",
    "# 1.2 PPO parameters\n",
    "sub_batch_size = 64  # frame per batch will be subdivided\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# 2. Create environment using TorchRL wrappers and transforms.\n",
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)\n",
    "\n",
    "# 2.1Transforms\n",
    "# Wrap sequence of transforms in a torchrl.envs.transforms.TransformedEnv\n",
    "# Normalization - first transform. Also DoubltToFloat and StepCounter\n",
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "\n",
    "# 3. Policy network and value model > loss module\n",
    "# 3.1 Policy: for PPO, the neural network will have to output parameters of a distribution.\n",
    "# Tanh-normal distribution for continuous data\n",
    "# neural_network(observation) = loc(observation), scale(observation), both having dimension of D_action\n",
    "# tensordict.nn.distributions.NormalParamExtractor for extracting location and scale.\n",
    "# tensordict.nn.TensorDictModule can generate distribution and sample from it\n",
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")\n",
    "\n",
    "# the policy will talk to env with tensordict in_keys and out_keys\n",
    "policy_module = TensorDictModule(\n",
    "    actor_net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "# action distribution using ~torchrl.modules.tensordict_module.ProbabilisticActor class to build ~torchrl.modules.TanhNormal\n",
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    " )\n",
    "# 3.2 Value network - is not used for inference.\n",
    "# Returns estimation of discounted return of following trajectory for given observation.\n",
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "\n",
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))\n",
    "\n",
    "# 4. Data collector\n",
    "\n",
    "# 4.1data collector classes execute three operations: reset env,\n",
    "# compute action given latest observation, execute a stepin env\n",
    "# - repeat first 2 steps until stop.\n",
    "# how many frames to collect at each iteration: frames_per_batch\n",
    "# when to reset env: max_frames_per_traj, device to execute\n",
    "# simplest data collector: torchrl.collectors.collectors.SyncDataCollector\n",
    "# - iterator that gives batches of data of given length, stops if total_frames have been collected.\n",
    "# synchronous: torchrl.collectors.collectors.MultiSyncDataCollector\n",
    "# asynchronous: torchrl.collectors.collectors.MultiaSyncDataCollector\n",
    "# the data collector will return ~tensordict.TensorDict instance with\n",
    "# total number of elements equal to frames_per_batch\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 4.2 Replay buffer\n",
    "# common building piece of off-policy RL.\n",
    "# in on-policy, data is updated and consumed with data collection.\n",
    "# torchrl.data.ReplayBuffer takes components of buffer as argument:\n",
    "# a storage, a writer, a sampler, some transforms\n",
    "# sampleWithoutReplacement(), don't sample same data multiple times in an epoch\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n",
    "\n",
    "# 5. Training loop, result analysis\n",
    "# 5.1 loss function\n",
    "# PPO loss from torchRL: torchrl.objectives.ClipPPOLoss class\n",
    "# PPO requires \"advantage estimation\" to be computed - which reflects\n",
    "# expectancy over the return value while dealing with bias/variance tradeoff.\n",
    "\n",
    "# to compute advantage - 1. build advantage module that utilizes value operator\n",
    "# 2. pass each batch of data through it before each epoch\n",
    "# the GAE module will update the input tensordict with new \"advantage\"\n",
    "# and \"value_target\" entries.\n",
    "# The \"value_target\" is a gradient free tensor that represents the empirical \n",
    "# value that the value network should represent with the input observation\n",
    "# the \"advantage\" and \"value_target\" will be used by \n",
    "# torchrl.objectives.ClipPPOLoss to return the policy and value losses.\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n",
    "\n",
    "#5.2 Training loop - steps:\n",
    "# collect data\n",
    "    # compute advantage\n",
    "        # loop over the data to compute loss values\n",
    "        # back propagate\n",
    "        # optimize\n",
    "        # repeat\n",
    "    # repeat\n",
    "# repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# iterate over collector until it reaches total_frames\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    for _ in range(num_epochs):\n",
    "        # we need \"advantage\" signal for PPO\n",
    "        # it's based on value-network, and is recomputed\n",
    "        # at each epoch with update of value-network\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # optimization: backward, grad clipping, optimization step\n",
    "            loss_value.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    \n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]:4.4f}(init={logs['reward'][0]:4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mario Playing RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
    "Learning. At the end, you will implement an AI-powered Mario (using\n",
    "[Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)) that can\n",
    "play the game by itself.\n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you\n",
    "can familiarize yourself with these RL\n",
    "[concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html),\n",
    "and have this handy\n",
    "[cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)\n",
    "as your companion. The full code is available\n",
    "[here](https://github.com/yuansongFeng/MadMario/).\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/mario.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "``` {.sourceCode .bash}\n",
    "%%bash\n",
    "pip install gym-super-mario-bros==7.4.0\n",
    "pip install tensordict==0.3.0\n",
    "pip install torchrl==0.3.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Definitions\n",
    "\n",
    "**Environment** The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : How the Agent responds to the Environment. The set of\n",
    "all possible Actions is called *action-space*.\n",
    "\n",
    "**State** $s$ : The current characteristic of the Environment. The set\n",
    "of all possible States the Environment can be in is called\n",
    "*state-space*.\n",
    "\n",
    "**Reward** $r$ : Reward is the key feedback from Environment to Agent.\n",
    "It is what drives the Agent to learn and to change its future action. An\n",
    "aggregation of rewards over multiple time steps is called **Return**.\n",
    "\n",
    "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return\n",
    "if you start in state $s$, take an arbitrary action $a$, and then for\n",
    "each future time step take the action that maximizes returns. $Q$ can be\n",
    "said to stand for the \"quality\" of the action in a state. We try to\n",
    "approximate this function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "\n",
    "#### Initialize Environment\n",
    "\n",
    "In Mario, the environment consists of tubes, mushrooms and other\n",
    "components.\n",
    "\n",
    "When Mario makes an action, the environment responds with the changed\n",
    "(next) state, reward and other info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.11/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/sunzid/anaconda3/lib/python3.11/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Environment\n",
    "\n",
    "Environment data is returned to the agent in `next_state`. As you saw\n",
    "above, each state is represented by a `[3, 240, 256]` size array. Often\n",
    "that is more information than our agent needs; for instance, Mario's\n",
    "actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending it to\n",
    "the agent.\n",
    "\n",
    "`GrayScaleObservation` is a common wrapper to transform an RGB image to\n",
    "grayscale; doing so reduces the size of the state representation without\n",
    "losing useful information. Now the size of each state: `[1, 240, 256]`\n",
    "\n",
    "`ResizeObservation` downsamples each observation into a square image.\n",
    "New size: `[1, 84, 84]`\n",
    "\n",
    "`SkipFrame` is a custom wrapper that inherits from `gym.Wrapper` and\n",
    "implements the `step()` function. Because consecutive frames don't vary\n",
    "much, we can skip n-intermediate frames without losing much information.\n",
    "The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "`FrameStack` is a wrapper that allows us to squash consecutive frames of\n",
    "the environment into a single observation point to feed to our learning\n",
    "model. This way, we can identify if Mario was landing or jumping based\n",
    "on the direction of his movement in the previous several frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped\n",
    "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
    "shown above in the image on the left. Each time Mario makes an action,\n",
    "the environment responds with a state of this structure. The structure\n",
    "is represented by a 3-D array of size `[4, 84, 84]`.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/mario_env.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We create a class `Mario` to represent our agent in the game. Mario\n",
    "should be able to:\n",
    "\n",
    "-   **Act** according to the optimal action policy based on the current\n",
    "    state (of the environment).\n",
    "-   **Remember** experiences. Experience = (current state, current\n",
    "    action, reward, next state). Mario *caches* and later *recalls* his\n",
    "    experiences to update his action policy.\n",
    "-   **Learn** a better action policy over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Act\n",
    "\n",
    "For any given state, an agent can choose to do the most optimal action\n",
    "(**exploit**) or a random action (**explore**).\n",
    "\n",
    "Mario randomly explores with a chance of `self.exploration_rate`; when\n",
    "he chooses to exploit, he relies on `MarioNet` (implemented in `Learn`\n",
    "section) to provide the most optimal action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache and Recall\n",
    "\n",
    "These two functions serve as Mario's \"memory\" process.\n",
    "\n",
    "`cache()`: Each time Mario performs an action, he stores the\n",
    "`experience` to his memory. His experience includes the current *state*,\n",
    "*action* performed, *reward* from the action, the *next state*, and\n",
    "whether the game is *done*.\n",
    "\n",
    "`recall()`: Mario randomly samples a batch of experiences from his\n",
    "memory, and uses that to learn the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn\n",
    "\n",
    "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461) under\n",
    "the hood. DDQN uses two ConvNets - $Q_{online}$ and $Q_{target}$ - that\n",
    "independently approximate the optimal action-value function.\n",
    "\n",
    "In our implementation, we share feature generator `features` across\n",
    "$Q_{online}$ and $Q_{target}$, but maintain separate FC classifiers for\n",
    "each. $\\theta_{target}$ (the parameters of $Q_{target}$) is frozen to\n",
    "prevent updating by backprop. Instead, it is periodically synced with\n",
    "$\\theta_{online}$ (more on this later).\n",
    "\n",
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Estimate & TD Target\n",
    "\n",
    "Two values are involved in learning:\n",
    "\n",
    "**TD Estimate** - the predicted optimal $Q^*$ for a given state $s$\n",
    "\n",
    "$${TD}_e = Q_{online}^*(s,a)$$\n",
    "\n",
    "**TD Target** - aggregation of current reward and the estimated $Q^*$ in\n",
    "the next state $s'$\n",
    "\n",
    "$$a' = argmax_{a} Q_{online}(s', a)$$\n",
    "\n",
    "$${TD}_t = r + \\gamma Q_{target}^*(s',a')$$\n",
    "\n",
    "Because we don't know what next action $a'$ will be, we use the action\n",
    "$a'$ maximizes $Q_{online}$ in the next state $s'$.\n",
    "\n",
    "Notice we use the\n",
    "[\\@torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)\n",
    "decorator on `td_target()` to disable gradient calculations here\n",
    "(because we don't need to backpropagate on $\\theta_{target}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the model\n",
    "\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$ and\n",
    "$TD_e$ and backpropagate this loss down $Q_{online}$ to update its\n",
    "parameters $\\theta_{online}$ ($\\alpha$ is the learning rate `lr` passed\n",
    "to the `optimizer`)\n",
    "\n",
    "$$\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$$\n",
    "\n",
    "$\\theta_{target}$ does not update through backpropagation. Instead, we\n",
    "periodically copy $\\theta_{online}$ to $\\theta_{target}$\n",
    "\n",
    "$$\\theta_{target} \\leftarrow \\theta_{online}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "Learning to play requires 40k steps, below is 40 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('checkpoints')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Path(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 798 - Epsilon 0.9998005198738403 - Mean Reward 1731.0 - Mean Length 798.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 6.79 - Time 2024-07-14T18:28:05\n",
      "Episode 20 - Step 5403 - Epsilon 0.9986501616833698 - Mean Reward 787.095 - Mean Length 257.286 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 38.39 - Time 2024-07-14T18:28:43\n",
      "Episode 40 - Step 8481 - Epsilon 0.9978819958775218 - Mean Reward 671.951 - Mean Length 206.854 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 25.833 - Time 2024-07-14T18:29:09\n",
      "Episode 60 - Step 13069 - Epsilon 0.9967380812455541 - Mean Reward 684.295 - Mean Length 214.246 - Mean Loss 0.307 - Mean Q Value 0.817 - Time Delta 47.441 - Time 2024-07-14T18:29:57\n",
      "Episode 80 - Step 18220 - Epsilon 0.9954553577117881 - Mean Reward 714.481 - Mean Length 224.938 - Mean Loss 0.349 - Mean Q Value 1.42 - Time Delta 55.168 - Time 2024-07-14T18:30:52\n",
      "Episode 100 - Step 22294 - Epsilon 0.9944420024419173 - Mean Reward 697.14 - Mean Length 214.96 - Mean Loss 0.38 - Mean Q Value 2.177 - Time Delta 44.261 - Time 2024-07-14T18:31:36\n",
      "Episode 120 - Step 25865 - Epsilon 0.9935546104017481 - Mean Reward 666.51 - Mean Length 204.62 - Mean Loss 0.472 - Mean Q Value 3.384 - Time Delta 39.344 - Time 2024-07-14T18:32:15\n",
      "Episode 140 - Step 30827 - Epsilon 0.9923228698984082 - Mean Reward 686.32 - Mean Length 223.46 - Mean Loss 0.563 - Mean Q Value 4.626 - Time Delta 51.469 - Time 2024-07-14T18:33:07\n",
      "Episode 160 - Step 34343 - Epsilon 0.9914510012290773 - Mean Reward 674.53 - Mean Length 212.74 - Mean Loss 0.482 - Mean Q Value 5.839 - Time Delta 37.167 - Time 2024-07-14T18:33:44\n",
      "Episode 180 - Step 38360 - Epsilon 0.9904558362174215 - Mean Reward 648.9 - Mean Length 201.4 - Mean Loss 0.485 - Mean Q Value 6.899 - Time Delta 41.864 - Time 2024-07-14T18:34:26\n",
      "Episode 199 - Step 42075 - Epsilon 0.9895363772843077 - Mean Reward 640.83 - Mean Length 199.45 - Mean Loss 0.506 - Mean Q Value 7.787 - Time Delta 40.752 - Time 2024-07-14T18:35:07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 200\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum: writing environment and transforms with TorchRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an environment (a simulator or an interface to a physical\n",
    "control system) is an integrative part of reinforcement learning and\n",
    "control engineering.\n",
    "\n",
    "TorchRL provides a set of tools to do this in multiple contexts. This\n",
    "tutorial demonstrates how to use PyTorch and TorchRL code a pendulum\n",
    "simulator from the ground up. It is freely inspired by the Pendulum-v1\n",
    "implementation from [OpenAI-Gym/Farama-Gymnasium control\n",
    "library](https://github.com/Farama-Foundation/Gymnasium).\n",
    "\n",
    "![Simple\n",
    "Pendulum](https://pytorch.org/tutorials/_static/img/pendulum.gif){.align-center}\n",
    "\n",
    "Key learnings:\n",
    "\n",
    "-   How to design an environment in TorchRL:\n",
    "    -   Writing specs (input, observation and reward);\n",
    "    -   Implementing behavior: seeding, reset and step.\n",
    "-   Transforming your environment inputs and outputs, and writing your\n",
    "    own transforms;\n",
    "-   How to use `~tensordict.TensorDict`{.interpreted-text role=\"class\"}\n",
    "    to carry arbitrary data structures through the `codebase`.\n",
    "\n",
    "    In the process, we will touch three crucial components of TorchRL:\n",
    "\n",
    "-   [environments](https://pytorch.org/rl/reference/envs.html)\n",
    "-   [transforms](https://pytorch.org/rl/reference/envs.html#transforms)\n",
    "-   [models (policy and value\n",
    "    function)](https://pytorch.org/rl/reference/modules.html)\n",
    "\n",
    "To give a sense of what can be achieved with TorchRL\\'s environments, we\n",
    "will be designing a *stateless* environment. While stateful environments\n",
    "keep track of the latest physical state encountered and rely on this to\n",
    "simulate the state-to-state transition, stateless environments expect\n",
    "the current state to be provided to them at each step, along with the\n",
    "action undertaken. TorchRL supports both types of environments, but\n",
    "stateless environments are more generic and hence cover a broader range\n",
    "of features of the environment API in TorchRL.\n",
    "\n",
    "Modeling stateless environments gives users full control over the input\n",
    "and outputs of the simulator: one can reset an experiment at any stage\n",
    "or actively modify the dynamics from the outside. However, it assumes\n",
    "that we have some control over a task, which may not always be the case:\n",
    "solving a problem where we cannot control the current state is more\n",
    "challenging but has a much wider set of applications.\n",
    "\n",
    "Another advantage of stateless environments is that they can enable\n",
    "batched execution of transition simulations. If the backend and the\n",
    "implementation allow it, an algebraic operation can be executed\n",
    "seamlessly on scalars, vectors, or tensors. This tutorial gives such\n",
    "examples.\n",
    "\n",
    "This tutorial will be structured as follows:\n",
    "\n",
    "-   We will first get acquainted with the environment properties: its\n",
    "    shape (`batch_size`), its methods (mainly\n",
    "    `~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"},\n",
    "    `~torchrl.envs.EnvBase.reset`{.interpreted-text role=\"meth\"} and\n",
    "    `~torchrl.envs.EnvBase.set_seed`{.interpreted-text role=\"meth\"}) and\n",
    "    finally its specs.\n",
    "-   After having coded our simulator, we will demonstrate how it can be\n",
    "    used during training with transforms.\n",
    "-   We will explore new avenues that follow from the TorchRL\\'s API,\n",
    "    including: the possibility of transforming inputs, the vectorized\n",
    "    execution of the simulation and the possibility of backpropagation\n",
    "    through the simulation graph.\n",
    "-   Finally, we will train a simple policy to solve the system we\n",
    "    implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four things you must take care of when designing a new\n",
    "environment class:\n",
    "\n",
    "-   `EnvBase._reset`{.interpreted-text role=\"meth\"}, which codes for the\n",
    "    resetting of the simulator at a (potentially random) initial state;\n",
    "-   `EnvBase._step`{.interpreted-text role=\"meth\"} which codes for the\n",
    "    state transition dynamic;\n",
    "-   `EnvBase._set_seed`{.interpreted-text role=\"meth\"}\\` which\n",
    "    implements the seeding mechanism;\n",
    "-   the environment specs.\n",
    "\n",
    "Let us first describe the problem at hand: we would like to model a\n",
    "simple pendulum over which we can control the torque applied on its\n",
    "fixed point. Our goal is to place the pendulum in upward position\n",
    "(angular position at 0 by convention) and having it standing still in\n",
    "that position. To design our dynamic system, we need to define two\n",
    "equations: the motion equation following an action (the torque applied)\n",
    "and the reward equation that will constitute our objective function.\n",
    "\n",
    "For the motion equation, we will update the angular velocity following:\n",
    "\n",
    "$$\\dot{\\theta}_{t+1} = \\dot{\\theta}_t + (3 * g / (2 * L) * \\sin(\\theta_t) + 3 / (m * L^2) * u) * dt$$\n",
    "\n",
    "where $\\dot{\\theta}$ is the angular velocity in rad/sec, $g$ is the\n",
    "gravitational force, $L$ is the pendulum length, $m$ is its mass,\n",
    "$\\theta$ is its angular position and $u$ is the torque. The angular\n",
    "position is then updated according to\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\dot{\\theta}_{t+1} dt$$\n",
    "\n",
    "We define our reward as\n",
    "\n",
    "$$r = -(\\theta^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * u^2)$$\n",
    "\n",
    "which will be maximized when the angle is close to 0 (pendulum in upward\n",
    "position), the angular velocity is close to 0 (no motion) and the torque\n",
    "is 0 too.\n",
    "\n",
    "#### Coding the effect of an action: `~torchrl.envs.EnvBase._step`\n",
    "\n",
    "The step method is the first thing to consider, as it will encode the\n",
    "simulation that is of interest to us. In TorchRL, the\n",
    "`~torchrl.envs.EnvBase`{.interpreted-text role=\"class\"} class has a\n",
    "`EnvBase.step`{.interpreted-text role=\"meth\"} method that receives a\n",
    "`tensordict.TensorDict`{.interpreted-text role=\"class\"} instance with an\n",
    "`\"action\"` entry indicating what action is to be taken.\n",
    "\n",
    "To facilitate the reading and writing from that `tensordict` and to make\n",
    "sure that the keys are consistent with what\\'s expected from the\n",
    "library, the simulation part has been delegated to a private abstract\n",
    "method `_step`{.interpreted-text role=\"meth\"} which reads input data\n",
    "from a `tensordict`, and writes a *new* `tensordict` with the output\n",
    "data.\n",
    "\n",
    "The `_step`{.interpreted-text role=\"func\"} method should do the\n",
    "following:\n",
    "\n",
    "> 1.  Read the input keys (such as `\"action\"`) and execute the\n",
    ">     simulation based on these;\n",
    "> 2.  Retrieve observations, done state and reward;\n",
    "> 3.  Write the set of observation values along with the reward and done\n",
    ">     state at the corresponding entries in a new\n",
    ">     `TensorDict`{.interpreted-text role=\"class\"}.\n",
    "\n",
    "Next, the `~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"}\n",
    "method will merge the output of\n",
    "`~torchrl.envs.EnvBase.step`{.interpreted-text role=\"meth\"} in the input\n",
    "`tensordict` to enforce input/output consistency.\n",
    "\n",
    "Typically, for stateful environments, this will look like this:\n",
    "\n",
    "``` {.sourceCode .}\n",
    ">>> policy(env.reset())\n",
    ">>> print(tensordict)\n",
    "TensorDict(\n",
    "    fields={\n",
    "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "    batch_size=torch.Size([]),\n",
    "    device=cpu,\n",
    "    is_shared=False)\n",
    ">>> env.step(tensordict)\n",
    ">>> print(tensordict)\n",
    "TensorDict(\n",
    "    fields={\n",
    "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "        next: TensorDict(\n",
    "            fields={\n",
    "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
    "                observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "            batch_size=torch.Size([]),\n",
    "            device=cpu,\n",
    "            is_shared=False),\n",
    "        observation: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "    batch_size=torch.Size([]),\n",
    "    device=cpu,\n",
    "    is_shared=False)\n",
    "```\n",
    "\n",
    "Notice that the root `tensordict` has not changed, the only modification\n",
    "is the appearance of a new `\"next\"` entry that contains the new\n",
    "information.\n",
    "\n",
    "In the Pendulum example, our `_step`{.interpreted-text role=\"meth\"}\n",
    "method will read the relevant entries from the input `tensordict` and\n",
    "compute the position and velocity of the pendulum after the force\n",
    "encoded by the `\"action\"` key has been applied onto it. We compute the\n",
    "new angular position of the pendulum `\"new_th\"` as the result of the\n",
    "previous position `\"th\"` plus the new velocity `\"new_thdot\"` over a time\n",
    "interval `dt`.\n",
    "\n",
    "Since our goal is to turn the pendulum up and maintain it still in that\n",
    "position, our `cost` (negative reward) function is lower for positions\n",
    "close to the target and low speeds. Indeed, we want to discourage\n",
    "positions that are far from being \\\"upward\\\" and/or speeds that are far\n",
    "from 0.\n",
    "\n",
    "In our example, `EnvBase._step`{.interpreted-text role=\"meth\"} is\n",
    "encoded as a static method since our environment is stateless. In\n",
    "stateful settings, the `self` argument is needed as the state needs to\n",
    "be read from the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n",
    "\n",
    "    g_force = tensordict[\"params\", \"g\"]\n",
    "    mass = tensordict[\"params\", \"m\"]\n",
    "    length = tensordict[\"params\", \"l\"]\n",
    "    dt = tensordict[\"params\", \"dt\"]\n",
    "    u = tensordict[\"action\"].squeeze(-1)\n",
    "    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "    new_thdot = (\n",
    "        thdot\n",
    "        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n",
    "    )\n",
    "    new_thdot = new_thdot.clamp(\n",
    "        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n",
    "    )\n",
    "    new_th = th + new_thdot * dt\n",
    "    reward = -costs.view(*tensordict.shape, 1)\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": new_th,\n",
    "            \"thdot\": new_thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting the simulator: `~torchrl.envs.EnvBase._reset`\n",
    "\n",
    "The second method we need to care about is the\n",
    "`~torchrl.envs.EnvBase._reset`{.interpreted-text role=\"meth\"} method.\n",
    "Like `~torchrl.envs.EnvBase._step`{.interpreted-text role=\"meth\"}, it\n",
    "should write the observation entries and possibly a done state in the\n",
    "`tensordict` it outputs (if the done state is omitted, it will be filled\n",
    "as `False` by the parent method\n",
    "`~torchrl.envs.EnvBase.reset`{.interpreted-text role=\"meth\"}). In some\n",
    "contexts, it is required that the `_reset` method receives a command\n",
    "from the function that called it (for example, in multi-agent settings\n",
    "we may want to indicate which agents need to be reset). This is why the\n",
    "`~torchrl.envs.EnvBase._reset`{.interpreted-text role=\"meth\"} method\n",
    "also expects a `tensordict` as input, albeit it may perfectly be empty\n",
    "or `None`.\n",
    "\n",
    "The parent `EnvBase.reset`{.interpreted-text role=\"meth\"} does some\n",
    "simple checks like the `EnvBase.step`{.interpreted-text role=\"meth\"}\n",
    "does, such as making sure that a `\"done\"` state is returned in the\n",
    "output `tensordict` and that the shapes match what is expected from the\n",
    "specs.\n",
    "\n",
    "For us, the only important thing to consider is whether\n",
    "`EnvBase._reset`{.interpreted-text role=\"meth\"} contains all the\n",
    "expected observations. Once more, since we are working with a stateless\n",
    "environment, we pass the configuration of the pendulum in a nested\n",
    "`tensordict` named `\"params\"`.\n",
    "\n",
    "In this example, we do not pass a done state as this is not mandatory\n",
    "for `_reset`{.interpreted-text role=\"meth\"} and our environment is\n",
    "non-terminating, so we always expect it to be `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "    high_th = torch.tensor(DEFAULT_X, device=self.device)\n",
    "    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n",
    "    low_th = -high_th\n",
    "    low_thdot = -high_thdot\n",
    "\n",
    "    # for non batch-locked environments, the input ``tensordict`` shape dictates the number\n",
    "    # of simulators run simultaneously. In other contexts, the initial\n",
    "    # random state's shape will depend upon the environment batch-size instead.\n",
    "    th = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_th - low_th)\n",
    "        + low_th\n",
    "    )\n",
    "    thdot = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_thdot - low_thdot)\n",
    "        + low_thdot\n",
    "    )\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": th,\n",
    "            \"thdot\": thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment metadata: `env.*_spec`\n",
    "\n",
    "The specs define the input and output domain of the environment. It is\n",
    "important that the specs accurately define the tensors that will be\n",
    "received at runtime, as they are often used to carry information about\n",
    "environments in multiprocessing and distributed settings. They can also\n",
    "be used to instantiate lazily defined neural networks and test scripts\n",
    "without actually querying the environment (which can be costly with\n",
    "real-world physical systems for instance).\n",
    "\n",
    "There are four specs that we must code in our environment:\n",
    "\n",
    "-   `EnvBase.observation_spec`{.interpreted-text role=\"obj\"}: This will\n",
    "    be a `~torchrl.data.CompositeSpec`{.interpreted-text role=\"class\"}\n",
    "    instance where each key is an observation (a\n",
    "    `CompositeSpec`{.interpreted-text role=\"class\"} can be viewed as a\n",
    "    dictionary of specs).\n",
    "-   `EnvBase.action_spec`{.interpreted-text role=\"obj\"}: It can be any\n",
    "    type of spec, but it is required that it corresponds to the\n",
    "    `\"action\"` entry in the input `tensordict`;\n",
    "-   `EnvBase.reward_spec`{.interpreted-text role=\"obj\"}: provides\n",
    "    information about the reward space;\n",
    "-   `EnvBase.done_spec`{.interpreted-text role=\"obj\"}: provides\n",
    "    information about the space of the done flag.\n",
    "\n",
    "TorchRL specs are organized in two general containers: `input_spec`\n",
    "which contains the specs of the information that the step function reads\n",
    "(divided between `action_spec` containing the action and `state_spec`\n",
    "containing all the rest), and `output_spec` which encodes the specs that\n",
    "the step outputs (`observation_spec`, `reward_spec` and `done_spec`). In\n",
    "general, you should not interact directly with `output_spec` and\n",
    "`input_spec` but only with their content: `observation_spec`,\n",
    "`reward_spec`, `done_spec`, `action_spec` and `state_spec`. The reason\n",
    "if that the specs are organized in a non-trivial way within\n",
    "`output_spec` and `input_spec` and neither of these should be directly\n",
    "modified.\n",
    "\n",
    "In other words, the `observation_spec` and related properties are\n",
    "convenient shortcuts to the content of the output and input spec\n",
    "containers.\n",
    "\n",
    "TorchRL offers multiple `~torchrl.data.TensorSpec`{.interpreted-text\n",
    "role=\"class\"}\n",
    "[subclasses](https://pytorch.org/rl/reference/data.html#tensorspec) to\n",
    "encode the environment\\'s input and output characteristics.\n",
    "\n",
    "#### Specs shape\n",
    "\n",
    "The environment specs leading dimensions must match the environment\n",
    "batch-size. This is done to enforce that every component of an\n",
    "environment (including its transforms) have an accurate representation\n",
    "of the expected input and output shapes. This is something that should\n",
    "be accurately coded in stateful settings.\n",
    "\n",
    "For non batch-locked environments, such as the one in our example (see\n",
    "below), this is irrelevant as the environment batch size will most\n",
    "likely be empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    # Under the hood, this will populate self.output_spec[\"observation\"]\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        th=BoundedTensorSpec(\n",
    "            low=-torch.pi,\n",
    "            high=torch.pi,\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        thdot=BoundedTensorSpec(\n",
    "            low=-td_params[\"params\", \"max_speed\"],\n",
    "            high=td_params[\"params\", \"max_speed\"],\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        # we need to add the ``params`` to the observation specs, as we want\n",
    "        # to pass it at each step during a rollout\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "    # since the environment is stateless, we expect the previous output as input.\n",
    "    # For this, ``EnvBase`` expects some state_spec to be available\n",
    "    self.state_spec = self.observation_spec.clone()\n",
    "    # action-spec will be automatically wrapped in input_spec when\n",
    "    # `self.action_spec = spec` will be called supported\n",
    "    self.action_spec = BoundedTensorSpec(\n",
    "        low=-td_params[\"params\", \"max_torque\"],\n",
    "        high=td_params[\"params\", \"max_torque\"],\n",
    "        shape=(1,),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducible experiments: seeding\n",
    "\n",
    "Seeding an environment is a common operation when initializing an\n",
    "experiment. The only goal of `EnvBase._set_seed`{.interpreted-text\n",
    "role=\"func\"} is to set the seed of the contained simulator. If possible,\n",
    "this operation should not call `reset()` or interact with the\n",
    "environment execution. The parent `EnvBase.set_seed`{.interpreted-text\n",
    "role=\"func\"} method incorporates a mechanism that allows seeding\n",
    "multiple environments with a different pseudo-random and reproducible\n",
    "seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping things together: the `~torchrl.envs.EnvBase`{.interpreted-text role=\"class\"} class\n",
    "\n",
    "We can finally put together the pieces and design our environment class.\n",
    "The specs initialization needs to be performed during the environment\n",
    "construction, so we must take care of calling the\n",
    "`_make_spec`{.interpreted-text role=\"func\"} method within\n",
    "`PendulumEnv.__init__`{.interpreted-text role=\"func\"}.\n",
    "\n",
    "We add a static method `PendulumEnv.gen_params`{.interpreted-text\n",
    "role=\"meth\"} which deterministically generates a set of hyperparameters\n",
    "to be used during execution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"max_speed\": 8,\n",
    "                    \"max_torque\": 2.0,\n",
    "                    \"dt\": 0.05,\n",
    "                    \"g\": g,\n",
    "                    \"m\": 1.0,\n",
    "                    \"l\": 1.0,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the environment as non-`batch_locked` by turning the\n",
    "`homonymous` attribute to `False`. This means that we will **not**\n",
    "enforce the input `tensordict` to have a `batch-size` that matches the\n",
    "one of the environment.\n",
    "\n",
    "The following code will just put together the pieces we have coded\n",
    "above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing our environment\n",
    "\n",
    "TorchRL provides a simple function\n",
    "`~torchrl.envs.utils.check_env_specs`{.interpreted-text role=\"func\"} to\n",
    "check that a (transformed) environment has an input/output structure\n",
    "that matches the one dictated by its specs. Let us try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 21:07:44,523 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = PendulumEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at our specs to have a visual representation of the\n",
    "environment signature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    th: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    thdot: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_speed: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_torque: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        dt: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        g: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        m: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        l: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "state_spec: CompositeSpec(\n",
      "    th: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    thdot: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_speed: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_torque: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        dt: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        g: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        m: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        l: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute a couple of commands too to check that the output\n",
    "structure matches what is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the `env.rand_step`{.interpreted-text role=\"func\"} to\n",
    "generate an action randomly from the `action_spec` domain. A\n",
    "`tensordict` containing the hyperparameters and the current state\n",
    "**must** be passed since our environment is stateless. In stateful\n",
    "contexts, `env.rand_step()` works perfectly too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming an environment\n",
    "\n",
    "Writing environment transforms for stateless simulators is slightly more\n",
    "complicated than for stateful ones: transforming an output entry that\n",
    "needs to be read at the following iteration requires to apply the\n",
    "inverse transform before calling `meth.step`{.interpreted-text\n",
    "role=\"func\"} at the next step. This is an ideal scenario to showcase all\n",
    "the features of TorchRL\\'s transforms!\n",
    "\n",
    "For instance, in the following transformed environment we `unsqueeze`\n",
    "the entries `[\"th\", \"thdot\"]` to be able to stack them along the last\n",
    "dimension. We also pass them as `in_keys_inv` to squeeze them back to\n",
    "their original shape once they are passed as input in the next\n",
    "iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    # ``Unsqueeze`` the observations that we will concatenate\n",
    "    UnsqueezeTransform(\n",
    "        unsqueeze_dim=-1,\n",
    "        in_keys=[\"th\", \"thdot\"],\n",
    "        in_keys_inv=[\"th\", \"thdot\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing custom transforms\n",
    "\n",
    "TorchRL\\'s transforms may not cover all the operations one wants to\n",
    "execute after an environment has been executed. Writing a transform does\n",
    "not require much effort. As for the environment design, there are two\n",
    "steps in writing a transform:\n",
    "\n",
    "-   Getting the dynamics right (forward and inverse);\n",
    "-   Adapting the environment specs.\n",
    "\n",
    "A transform can be used in two settings: on its own, it can be used as a\n",
    "`~torch.nn.Module`{.interpreted-text role=\"class\"}. It can also be used\n",
    "appended to a\n",
    "`~torchrl.envs.transforms.TransformedEnv`{.interpreted-text\n",
    "role=\"class\"}. The structure of the class allows to customize the\n",
    "behavior in the different contexts.\n",
    "\n",
    "A `~torchrl.envs.transforms.Transform`{.interpreted-text role=\"class\"}\n",
    "skeleton can be summarized as follows:\n",
    "\n",
    "``` {.sourceCode .}\n",
    "class Transform(nn.Module):\n",
    "    def forward(self, tensordict):\n",
    "        ...\n",
    "    def _apply_transform(self, tensordict):\n",
    "        ...\n",
    "    def _step(self, tensordict):\n",
    "        ...\n",
    "    def _call(self, tensordict):\n",
    "        ...\n",
    "    def inv(self, tensordict):\n",
    "        ...\n",
    "    def _inv_apply_transform(self, tensordict):\n",
    "        ...\n",
    "```\n",
    "\n",
    "There are three entry points (`forward`{.interpreted-text role=\"func\"},\n",
    "`_step`{.interpreted-text role=\"func\"} and `inv`{.interpreted-text\n",
    "role=\"func\"}) which all receive\n",
    "`tensordict.TensorDict`{.interpreted-text role=\"class\"} instances. The\n",
    "first two will eventually go through the keys indicated by\n",
    "`~tochrl.envs.transforms.Transform.in_keys`{.interpreted-text\n",
    "role=\"obj\"} and call\n",
    "`~torchrl.envs.transforms.Transform._apply_transform`{.interpreted-text\n",
    "role=\"meth\"} to each of these. The results will be written in the\n",
    "entries pointed by `Transform.out_keys`{.interpreted-text role=\"obj\"} if\n",
    "provided (if not the `in_keys` will be updated with the transformed\n",
    "values). If inverse transforms need to be executed, a similar data flow\n",
    "will be executed but with the `Transform.inv`{.interpreted-text\n",
    "role=\"func\"} and `Transform._inv_apply_transform`{.interpreted-text\n",
    "role=\"func\"} methods and across the `in_keys_inv` and `out_keys_inv`\n",
    "list of keys. The following figure summarized this flow for environments\n",
    "and replay buffers.\n",
    "\n",
    "> Transform API\n",
    "\n",
    "In some cases, a transform will not work on a subset of keys in a\n",
    "unitary manner, but will execute some operation on the parent\n",
    "environment or work with the entire input `tensordict`. In those cases,\n",
    "the `_call`{.interpreted-text role=\"func\"} and\n",
    "`forward`{.interpreted-text role=\"func\"} methods should be re-written,\n",
    "and the `_apply_transform`{.interpreted-text role=\"func\"} method can be\n",
    "skipped.\n",
    "\n",
    "Let us code new transforms that will compute the `sine` and `cosine`\n",
    "values of the position angle, as these values are more useful to us to\n",
    "learn a policy than the raw angle value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.sin()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "class CosTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.cos()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "t_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\n",
    "t_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\n",
    "env.append_transform(t_sin)\n",
    "env.append_transform(t_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, let us check that our environment specs match what is\n",
    "received:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing a rollout\n",
    "\n",
    "Executing a rollout is a succession of simple steps:\n",
    "\n",
    "-   reset the environment\n",
    "-   while some condition is not met:\n",
    "    -   compute an action given a policy\n",
    "    -   execute a step given this action\n",
    "    -   collect the data\n",
    "    -   make a `MDP` step\n",
    "-   gather the data and return\n",
    "\n",
    "These operations have been conveniently wrapped in the\n",
    "`~torchrl.envs.EnvBase.rollout`{.interpreted-text role=\"meth\"} method,\n",
    "from which we provide a simplified version here below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rollout(steps=100):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching computations\n",
    "\n",
    "The last unexplored end of our tutorial is the ability that we have to\n",
    "batch computations in TorchRL. Because our environment does not make any\n",
    "assumptions regarding the input data shape, we can seamlessly execute it\n",
    "over batches of data. Even better: for non-batch-locked environments\n",
    "such as our Pendulum, we can change the batch size on the fly without\n",
    "recreating the environment. To do this, we just generate parameters with\n",
    "the desired shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(\"reset (batch size of 10)\", td)\n",
    "td = env.rand_step(td)\n",
    "print(\"rand step (batch size of 10)\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a rollout with a batch of data requires us to reset the\n",
    "environment out of the rollout function, since we need to define the\n",
    "batch\\_size dynamically and this is not supported by\n",
    "`~torchrl.envs.EnvBase.rollout`{.interpreted-text role=\"meth\"}:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(\n",
    "    3,\n",
    "    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n",
    "    tensordict=env.reset(env.gen_params(batch_size=[batch_size])),\n",
    ")\n",
    "print(\"rollout of len 3 (batch size of 10):\", rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple policy\n",
    "\n",
    "In this example, we will train a simple policy using the reward as a\n",
    "differentiable objective, such as a negative loss. We will take\n",
    "advantage of the fact that our dynamic system is fully differentiable to\n",
    "backpropagate through the trajectory return and adjust the weights of\n",
    "our policy to maximize this value directly. Of course, in many settings\n",
    "many of the assumptions we make do not hold, such as differentiable\n",
    "system and full access to the underlying mechanics.\n",
    "\n",
    "Still, this is a very simple example that showcases how a training loop\n",
    "can be coded with a custom environment in TorchRL.\n",
    "\n",
    "Let us first write the policy network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "We will successively:\n",
    "\n",
    "-   generate a trajectory\n",
    "-   sum the rewards\n",
    "-   backpropagate through the graph defined by these operations\n",
    "-   clip the gradient norm and make an optimization step\n",
    "-   repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to\n",
    "0 which demonstrates that the pendulum is upward and still as desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial, we have learned how to code a stateless environment\n",
    "from scratch. We touched the subjects of:\n",
    "\n",
    "-   The four essential components that need to be taken care of when\n",
    "    coding an environment (`step`, `reset`, seeding and building specs).\n",
    "    We saw how these methods and classes interact with the\n",
    "    `~tensordict.TensorDict`{.interpreted-text role=\"class\"} class;\n",
    "-   How to test that an environment is properly coded using\n",
    "    `~torchrl.envs.utils.check_env_specs`{.interpreted-text\n",
    "    role=\"func\"};\n",
    "-   How to append transforms in the context of stateless environments\n",
    "    and how to write custom transformations;\n",
    "-   How to train a policy on a fully differentiable simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    CatTensors,\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    th, thdot = tensordict[\"th\"], tensordict[\"thdot\"]  # th := theta\n",
    "\n",
    "    g_force = tensordict[\"params\", \"g\"]\n",
    "    mass = tensordict[\"params\", \"m\"]\n",
    "    length = tensordict[\"params\", \"l\"]\n",
    "    dt = tensordict[\"params\", \"dt\"]\n",
    "    u = tensordict[\"action\"].squeeze(-1)\n",
    "    u = u.clamp(-tensordict[\"params\", \"max_torque\"], tensordict[\"params\", \"max_torque\"])\n",
    "    costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "    new_thdot = (\n",
    "        thdot\n",
    "        + (3 * g_force / (2 * length) * th.sin() + 3.0 / (mass * length**2) * u) * dt\n",
    "    )\n",
    "    new_thdot = new_thdot.clamp(\n",
    "        -tensordict[\"params\", \"max_speed\"], tensordict[\"params\", \"max_speed\"]\n",
    "    )\n",
    "    new_th = th + new_thdot * dt\n",
    "    reward = -costs.view(*tensordict.shape, 1)\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": new_th,\n",
    "            \"thdot\": new_thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + torch.pi) % (2 * torch.pi)) - torch.pi\n",
    "\n",
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "    high_th = torch.tensor(DEFAULT_X, device=self.device)\n",
    "    high_thdot = torch.tensor(DEFAULT_Y, device=self.device)\n",
    "    low_th = -high_th\n",
    "    low_thdot = -high_thdot\n",
    "\n",
    "    # for non batch-locked environments, the input ``tensordict`` shape dictates the number\n",
    "    # of simulators run simultaneously. In other contexts, the initial\n",
    "    # random state's shape will depend upon the environment batch-size instead.\n",
    "    th = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_th - low_th)\n",
    "        + low_th\n",
    "    )\n",
    "    thdot = (\n",
    "        torch.rand(tensordict.shape, generator=self.rng, device=self.device)\n",
    "        * (high_thdot - low_thdot)\n",
    "        + low_thdot\n",
    "    )\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"th\": th,\n",
    "            \"thdot\": thdot,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def _make_spec(self, td_params):\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        th=BoundedTensorSpec(\n",
    "            low=-torch.pi,\n",
    "            high=torch.pi,\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        thdot=BoundedTensorSpec(\n",
    "            low=-td_params[\"params\", \"max_speed\"],\n",
    "            high=td_params[\"params\", \"max_speed\"],\n",
    "            shape=(),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "\n",
    "    self.state_spec = self.observation_spec.clone()\n",
    "    self.action_spec = BoundedTensorSpec(\n",
    "        low=-td_params[\"params\", \"max_torque\"],\n",
    "        high=td_params[\"params\", \"max_torque\"],\n",
    "        shape=(1,),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    self.reward_spc = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng\n",
    "\n",
    "def gen_params(g=10.0, batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"max_speed\": 8,\n",
    "                    \"max_torque\": 2.0,\n",
    "                    \"dt\": 0.05,\n",
    "                    \"g\": g,\n",
    "                    \"m\": 1.0,\n",
    "                    \"l\": 1.0,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td\n",
    "\n",
    "class PendulumEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 22:05:16,718 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = PendulumEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    th: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    thdot: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_speed: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_torque: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        dt: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        g: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        m: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        l: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "state_spec: CompositeSpec(\n",
      "    th: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    thdot: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        max_speed: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_torque: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        dt: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        g: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        m: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        l: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SinTransform.__init__() got an unexpected keyword argument 'in_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m\n",
      "\u001b[1;32m     62\u001b[0m     \u001b[38;5;129m@_apply_to_composite\u001b[39m\n",
      "\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_observation_spec\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation_spec):\n",
      "\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BoundedTensorSpec(\n",
      "\u001b[1;32m     65\u001b[0m             low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n",
      "\u001b[1;32m     66\u001b[0m             high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     69\u001b[0m             device\u001b[38;5;241m=\u001b[39mobservation_spec\u001b[38;5;241m.\u001b[39mdevice,\n",
      "\u001b[1;32m     70\u001b[0m         )\n",
      "\u001b[0;32m---> 73\u001b[0m t_sin \u001b[38;5;241m=\u001b[39m SinTransform(in_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m\"\u001b[39m], out_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m     74\u001b[0m t_cos \u001b[38;5;241m=\u001b[39m CosTransform(in_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m\"\u001b[39m], out_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m     75\u001b[0m env\u001b[38;5;241m.\u001b[39mappend_transform(t_sin)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:457\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n",
      "\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n",
      "\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    458\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n",
      "\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n",
      "\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    462\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: SinTransform.__init__() got an unexpected keyword argument 'in_keys'"
     ]
    }
   ],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    # ``Unsqueeze`` the observations that we will concatenate\n",
    "    UnsqueezeTransform(\n",
    "        unsqueeze_dim=-1,\n",
    "        in_keys=[\"th\", \"thdot\"],\n",
    "        in_keys_inv=[\"th\", \"thdot\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    def forward(self, tensordict):\n",
    "        ...\n",
    "    def _apply_transform(self, tensordict):\n",
    "        ...\n",
    "    def _step(self, tensordict):\n",
    "        ...\n",
    "    def _call(self, tensordict):\n",
    "        ...\n",
    "    def inv(self, tensordict):\n",
    "        ...\n",
    "    def _inv_apply_transform(self, tensordict):\n",
    "        ...\n",
    "\n",
    "class SinTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.sin()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "class CosTransform(Transform):\n",
    "    def _apply_transform(self, obs: torch.Tensor) -> None:\n",
    "        return obs.cos()\n",
    "\n",
    "    # The transform must also modify the data at reset time\n",
    "    def _reset(\n",
    "        self, tensordict: TensorDictBase, tensordict_reset: TensorDictBase\n",
    "    ) -> TensorDictBase:\n",
    "        return self._call(tensordict_reset)\n",
    "\n",
    "    # _apply_to_composite will execute the observation spec transform across all\n",
    "    # in_keys/out_keys pairs and write the result in the observation_spec which\n",
    "    # is of type ``Composite``\n",
    "    @_apply_to_composite\n",
    "    def transform_observation_spec(self, observation_spec):\n",
    "        return BoundedTensorSpec(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=observation_spec.shape,\n",
    "            dtype=observation_spec.dtype,\n",
    "            device=observation_spec.device,\n",
    "        )\n",
    "\n",
    "\n",
    "t_sin = SinTransform(in_keys=[\"th\"], out_keys=[\"sin\"])\n",
    "t_cos = CosTransform(in_keys=[\"th\"], out_keys=[\"cos\"])\n",
    "env.append_transform(t_sin)\n",
    "env.append_transform(t_cos)\n",
    "\n",
    "cat_transform = CatTensors(\n",
    "    in_keys=[\"sin\", \"cos\", \"thdot\"], dim=-1, out_key=\"observation\", del_keys=False\n",
    ")\n",
    "env.append_transform(cat_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from rollout: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                cos: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([100]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                th: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([100]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([100]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        sin: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([100]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def simple_rollout(steps=100):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset (batch size of 10) TensorDict(\n",
      "    fields={\n",
      "        cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "rand step (batch size of 10) TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                cos: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        sin: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10  # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(\"reset (batch size of 10)\", td)\n",
    "td = env.rand_step(td)\n",
    "print(\"rand step (batch size of 10)\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of len 3 (batch size of 10): TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                cos: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([10, 3]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                th: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                dt: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                g: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                l: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                m: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_speed: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_torque: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10, 3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        sin: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        th: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        thdot: Tensor(shape=torch.Size([10, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10, 3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(\n",
    "    3,\n",
    "    auto_reset=False,  # we're executing the reset out of the ``rollout`` call\n",
    "    tensordict=env.reset(env.gen_params(batch_size=[batch_size])),\n",
    ")\n",
    "print(\"rollout of len 3 (batch size of 10):\", rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Some tensors that are necessary for the module call may not have not been found in the input tensordict: the following inputs are None: {'observation'}.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorDictModule failed with operation\n",
      "    Sequential(\n",
      "      (0): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n",
      "      (5): Tanh()\n",
      "      (6): LazyLinear(in_features=0, out_features=1, bias=True)\n",
      "    )\n",
      "    in_keys=['observation']\n",
      "    out_keys=['action'].\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n",
      "\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n",
      "\u001b[1;32m     27\u001b[0m     init_td \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(env\u001b[38;5;241m.\u001b[39mgen_params(batch_size\u001b[38;5;241m=\u001b[39m[batch_size]))\n",
      "\u001b[0;32m---> 28\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrollout(\u001b[38;5;241m100\u001b[39m, policy, tensordict\u001b[38;5;241m=\u001b[39minit_td, auto_reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m     29\u001b[0m     traj_return \u001b[38;5;241m=\u001b[39m rollout[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;32m     30\u001b[0m     (\u001b[38;5;241m-\u001b[39mtraj_return)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchrl/envs/common.py:2393\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, out)\u001b[0m\n",
      "\u001b[1;32m   2383\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m   2384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensordict,\n",
      "\u001b[1;32m   2385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_cast_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: auto_cast_to_device,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback,\n",
      "\u001b[1;32m   2391\u001b[0m }\n",
      "\u001b[1;32m   2392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done:\n",
      "\u001b[0;32m-> 2393\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_stop_early(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   2394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   2395\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchrl/envs/common.py:2424\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n",
      "\u001b[1;32m   2422\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   2423\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39mclear_device_()\n",
      "\u001b[0;32m-> 2424\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m policy(tensordict)\n",
      "\u001b[1;32m   2425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_cast_to_device:\n",
      "\u001b[1;32m   2426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:589\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 589\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), fun_name)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;32m    591\u001b[0m         pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*takes \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ positional arguments but \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ were given|got multiple values for argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/nn/common.py:291\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    289\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(out[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest)\n",
      "\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n",
      "\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/nn/utils.py:261\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[1;32m    256\u001b[0m     skip_existing()\n",
      "\u001b[1;32m    257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out_keys)\n",
      "\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m out_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m in_keys)\n",
      "\u001b[1;32m    259\u001b[0m ):\n",
      "\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensordict\n",
      "\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/nn/common.py:1217\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1215\u001b[0m in_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m   1216\u001b[0m out_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[1;32m   1218\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictModule failed with operation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1219\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensordict/nn/common.py:1185\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[1;32m   1180\u001b[0m     none_set \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m   1181\u001b[0m         key\n",
      "\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys, tensors)\n",
      "\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1184\u001b[0m     }\n",
      "\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n",
      "\u001b[1;32m   1186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome tensors that are necessary for the module call may \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot have not been found in the input tensordict: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe following inputs are None: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnone_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1189\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Some tensors that are necessary for the module call may not have not been found in the input tensordict: the following inputs are None: {'observation'}.\""
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)\n",
    "\n",
    "batch_size = 32\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
