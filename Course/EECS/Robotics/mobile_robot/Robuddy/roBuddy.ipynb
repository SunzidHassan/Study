{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750fc456",
   "metadata": {},
   "source": [
    "# Log\n",
    "## 2025\n",
    "### November, 2025\n",
    "\n",
    "#### November 26, 2025\n",
    "- [ ] Design a control algorithm for the sim environment.\n",
    "  - [ ] Summary robot software setup.\n",
    "  - [ ] Fresh install libraries in a python environment.\n",
    "  - [ ] Debug the control algorithm for sim enviornment.\n",
    "- [ ] Hardware\n",
    "  - [ ] Design a larger chassis - aluminum frame, 3d printed levels.\n",
    "    - [ ] Solidworks intro course.\n",
    "    - [ ] Design chassis.\n",
    "\n",
    "#### November 25, 2025\n",
    "- [ ] Hardware\n",
    "  - [x] Order stronger motor (42mm) and bracket, 8-inch tires, caster wheels, aluminum t slot and connectors, power station.\n",
    "\n",
    "#### November 24, 2025\n",
    "- [ ] Design a shopping mall like test environment and run navigation.\n",
    "  - [x] Simulation world and map.\n",
    "    - [x] Designing a simple Gazebo box world.\n",
    "    - [x] Mapping the box world.\n",
    "    - [x] Loading the saved map with localization.\n",
    "    - [x] Run AMCL.\n",
    "    - [x] Run naivgation\n",
    "\n",
    "#### November 23, 2025\n",
    "- [x] Using controller to drive the robot.\n",
    "  - [x] The controller is now detected\n",
    "  - [x] Both the sim and real robot is now moving.\n",
    "\n",
    "#### November 22, 2025\n",
    "- [ ] Test the control algorithm in a gazebo shopping mall or warehouse simulation.\n",
    "  - [ ] Tried to run controller to drive the robot.\n",
    "  - [x] Use Gagzebo warehouse for testing in simulation\n",
    "  - [ ] Tried to generate and use map of the environment that can be used for navigation.\n",
    "\n",
    "### November 19, 2025\n",
    "- [x] Do custom model instead of agent studio.\n",
    "  - [x] Working loop: porcupineWake_sileroVAD_whisperASR_piperTTS_llama3.2-1B-instructLLM\n",
    "\n",
    "#### November 18, 2025\n",
    "1. [ ] Agent design\n",
    "  - [x] Agent Studio WhisperASR wasn't working\n",
    "  ```bash\n",
    "  jetson-containers run --env HUGGINGFACE_TOKEN=$HUGGINGFACE_TOKEN \\\n",
    "  $(autotag nano_llm) \\\n",
    "  /bin/bash\n",
    "\n",
    "  #then run\n",
    "  root@jetson:/opt/NanoLLM: pip install openai-whisper==20240927 --index-url https://pypi.org/simple\n",
    "\n",
    "  # after finishing\n",
    "  python3 -m nano_llm.studio\n",
    "  ```\n",
    "  - [x] The LLM that worked once: Hermes-2-Pro-LI - q3f16_0 quantization. And then it failed!\n",
    "  \n",
    "2. [ ] Hardware design: new battery, convenient and robust chassis\n",
    "\n",
    "\n",
    "#### November 17, 2025\n",
    "- [x] The robot dropped sideways, and now the left wheel is continuously rotating backwards -> the D5 got connected to D0 - swapping them solved the issue.\n",
    "\n",
    "#### November 15, 2025\n",
    "- [x] cmd_vel was working for TwistStamped, not for Twist. After the below update\n",
    "```yaml\n",
    "# twist_mux.yaml\n",
    "ros__parameters:\n",
    "  # adding cmd_vel_out\n",
    "  cmd_vel_out: \"/diff_cont/cmd_vel_unstamped\"\n",
    "\n",
    "# my_controllers_blue.yaml\n",
    "diff_cont:\n",
    "  ros__parameters:\n",
    "    # adding use_stamped_vel: false\n",
    "    use_stamped_vel: false\n",
    "```\n",
    "Now `ros2 run teleop_twist_keyboard teleop_twist_keyboard` worked.\n",
    "\n",
    "#### November 14, 2025\n",
    "- [x] Realized that Pi uses ROS Jazzy - whereas the drivers and code assume ROS humble -> try the Jetson again.\n",
    "- [x] While Jazzy fails, Humble runs launch_robot.launch.py.\n",
    "- [x] LiDAR works.\n",
    "- [ ] But the wheels still don't rotate for unstamped Teleop.\n",
    "  - [x] Pyserial still works.\n",
    "Why did I shifted from Jetson to Pi -> I think the camera wasn't working right, and I need a new battery/chassis to support it.\n",
    "\n",
    "#### November 12, 2025\n",
    "- [x] launch_sim.launch.py works with humble teleop.\n",
    "- [x] Matched the config, description and lanunch failes against articubot_one new_gazebo branch files - they match.\n",
    "- [x] Check the launch_robot.launch.py error with gemini - changed <plugin>diffdrive_arduino/DiffDriveArduinoHardware</plugin> to <plugin>diffdrive_arduino/DiffDriveArduino</plugin>, added use_stamped: false to twist_mux.yaml, checked URDF files - still <ros2_control name = \"RealRobot\" ...> is failing.\n",
    "\n",
    "#### November 11, 2025\n",
    "- [x] Diffdrive Arduino installation fails. `git clone -b humble https://github.com/Buzzology/diffdrive_arduino.git` worked.\n",
    "- [ ] Teleop operation not working.\n",
    "  - [x] Check if motor control over serial works -> it works for `/dev/ttyACM0`.\n",
    "  - [x] Check if serial_motor_demo GUI control works -> it works. So the hardware connections are OK.\n",
    "\n",
    "\n",
    "### September, 2025\n",
    "- Managed to run LiDAR, but not rplidar.launch.pi\n",
    "- Managed to run \n",
    "\n",
    "### August, 2025\n",
    "#### August 11, 2025\n",
    "- [x] Use original Arduino for Jetson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e966fb9",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "```bash\n",
    "ros2 launch bluebot_one launch_sim.launch.py world:=./src/bluebot_one/worlds/boxWorld.sdf\n",
    "\n",
    "ros2 run rviz2 rviz2 -d src/bluebot_one/rviz/baseConfig.rviz\n",
    "```\n",
    "\n",
    "Saving map\n",
    "```bash\n",
    "ros2 launch slam_toolbox online_async_launch.py params_file:=./src/bluebot_one/config/mapper_params_online_async.yaml use_sim_time:=true \n",
    "```\n",
    "\n",
    "Loading map\n",
    "```bash\n",
    "ros2 launch slam_toolbox localization_launch.py slam_params_file:=./src/bluebot_one/config/localization_params.yaml use_sim_time:=true\n",
    "```\n",
    "\n",
    "AMCL\n",
    "```bash\n",
    "ros2 run nav2_map_server map_server --ros-args \\\n",
    "-p yaml_filename:=src/bluebot_one/rviz/boxMap_save.yaml \\\n",
    "-p use_sim_time:=true\n",
    "\n",
    "# In a new tab:\n",
    "ros2 run nav2_util lifecycle_bringup map_server\n",
    "\n",
    "# In a new tab:\n",
    "ros2 run nav2_amcl amcl --ros-args -p use_sim_time:=true\n",
    "\n",
    "# In a new tab:\n",
    "ros2 run nav2_util lifecycle_bringup amcl\n",
    "```\n",
    "\n",
    "Nav2\n",
    "```bash\n",
    "ros2 run twist_mux twist_mux --ros-args --params-file ./src/bluebot_one/config/twist_mux.yaml -r cmd_vel_out:=diff_cont/cmd_vel_unstamped\n",
    "\n",
    "# launch simulation and rviz in a new tab\n",
    "ros2 launch nav2_bringup navigation_launch.py use_sim_time:=true\n",
    "```\n",
    "\n",
    "Alternative navigation running:\n",
    "```bash\n",
    "# for launching map\n",
    "ros2 launch bluebot_one localization_launch.py map:=./src/bluebot_one/rviz/boxMap_save.yaml use_sim_time:=true #then set the initial pose and set map durability to transient local.\n",
    "\n",
    "# for navigation\n",
    "ros2 launch bluebot_one navigation_launch.py use_time_time:=true map_subscribe_transient_local:=true # add costmap\n",
    "\n",
    "# Publish goal\n",
    "ros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose \"pose: {header: {frame_id: map}, pose: {position: {x: 1.0, y: -2.0, z: 0.0}, orientation: {x: 0.0, y: 0.0, z: 0.0, w: 1.0}}}\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcdc83",
   "metadata": {},
   "source": [
    "# Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36f078",
   "metadata": {},
   "source": [
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install python3-venv\n",
    "\n",
    "cd ~/humbleDev_ws\n",
    "python3 -m venv --system-site-packages robuddy\n",
    "\n",
    "source /opt/ros/humble/setup.bash\n",
    "source robuddy/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130  # Adjust CUDA version as needed\n",
    "pip install transformers piper-tts pyyaml sounddevice pvporcupine soundfile numpy accelerate librosa scipy\n",
    "```\n",
    "\n",
    "```bash\n",
    "source /opt/ros/humble/setup.bash\n",
    "source robuddy/bin/activate\n",
    "ros2 run bluebotone_controller blueBot_roBuddy_01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb76d25",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf052522",
   "metadata": {},
   "source": [
    "## Environment\n",
    "* Install Anaconda (install > nano ~/.bashrc > ~/anaconda3/bin/conda init > source ~/.bashrc)\n",
    "* Install CUDA toolkit and legacy driver (12.8 on Jul-25)\n",
    "* Create Conda environment:\n",
    "``` bash\n",
    "conda create --name robuddy python=3.11\n",
    "conda activate robuddy\n",
    "cd #Robuddy directory\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee2dd9",
   "metadata": {},
   "source": [
    "## Tips\n",
    "Removing previous nvidia installations\n",
    "``` bash\n",
    "sudo apt purge nvidia* -y\n",
    "sudo apt remove nvidia-* -y\n",
    "sudo rm /etc/apt/sources.list.d/cuda*\n",
    "sudo apt autoremove -y && sudo apt autoclean -y\n",
    "sudo rm -rf /usr/local/cuda*\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "```\n",
    "\n",
    "To free up space:\n",
    "```bash\n",
    "conda env remove --name <environment_name>\n",
    "conda clean --all\n",
    "rm -rf ~/.cache/huggingface/hub\n",
    "rm -rf ~/.cache/huggingface/transformers\n",
    "rm -rf ~/.cache/huggingface/datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568c8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 19:48:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P3             33W /  184W |     707MiB /  12288MiB |     42%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3215      G   /usr/lib/xorg/Xorg                      240MiB |\n",
      "|    0   N/A  N/A            3533      G   /usr/bin/gnome-shell                    146MiB |\n",
      "|    0   N/A  N/A            4571      G   ...rack-uuid=3190708988185955192        142MiB |\n",
      "|    0   N/A  N/A            5112      G   /proc/self/exe                          132MiB |\n",
      "|    0   N/A  N/A            9777      G   /usr/bin/gnome-clocks                    10MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA version\n",
    "!nvidia-smi\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c961416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "coqui-tts 0.26.2 requires transformers<4.52,>=4.47.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Old!!\n",
    "# Install/update required packages\n",
    "%pip install --quiet pvporcupine sounddevice soundfile librosa pyyaml matplotlib scipy transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22690519",
   "metadata": {},
   "source": [
    "## Wake word detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b0492",
   "metadata": {},
   "source": [
    "### Porcupine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"config.yaml\"\n",
    "\n",
    "def load_config(file_path: Path = CONFIG_PATH) -> str:\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {file_path}\")\n",
    "    cfg = yaml.safe_load(file_path.read_text())\n",
    "    if \"PORCUPINE_KEY\" not in cfg:\n",
    "        raise KeyError(\"Missing 'PORCUPINE_KEY' in config\")\n",
    "    return cfg[\"PORCUPINE_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05af8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcupine frame_length: 512\n"
     ]
    }
   ],
   "source": [
    "import pvporcupine\n",
    "porcupine = pvporcupine.create(access_key=load_config(), keywords=[\"jarvis\"])\n",
    "print(\"Porcupine frame_length:\", porcupine.frame_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e22447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 HDA NVidia: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
       "  1 HDA NVidia: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
       "  2 HDA NVidia: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
       "  3 HDA NVidia: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
       "  4 HD-Audio Generic: HDMI 0 (hw:1,3), ALSA (0 in, 8 out)\n",
       "  5 HD-Audio Generic: ALC256 Analog (hw:2,0), ALSA (2 in, 2 out)\n",
       "  6 hdmi, ALSA (0 in, 8 out)\n",
       "  7 pipewire, ALSA (64 in, 64 out)\n",
       "* 8 default, ALSA (64 in, 64 out)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "sd.query_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acef8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfae3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 'Jarvis' at sample 32256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, soundfile as sf, librosa\n",
    "\n",
    "data, sr = sf.read(\"output.wav\")\n",
    "if data.ndim > 1:\n",
    "    data = data[:,0]\n",
    "if sr != 16000:\n",
    "    data = librosa.resample(data, sr, 16000)\n",
    "    sr = 16000\n",
    "if data.dtype != np.int16:\n",
    "    data = (data * 32767).astype(np.int16)\n",
    "\n",
    "for i in range(0, len(data) - porcupine.frame_length + 1, porcupine.frame_length):\n",
    "    if porcupine.process(data[i:i+porcupine.frame_length]) >= 0:\n",
    "        print(\"Detected 'Jarvis' at sample\", i)\n",
    "        break\n",
    "porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2972c",
   "metadata": {},
   "source": [
    "## Image Object Detection\n",
    "### YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model8s = YOLO(\"models/YOLO/yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in model8s.track(source=0, classes=[0], conf=0.25, device=\"cuda:0\"):\n",
    "    human_near = (results[0].boxes.cls.cpu() == 0).any().item()\n",
    "    print(\"Human detected:\", human_near)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcfa04",
   "metadata": {},
   "source": [
    "## Speech-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba3517",
   "metadata": {},
   "source": [
    "### Whisper\n",
    "- By openai\n",
    "- High accuracy\n",
    "- Resoruce intensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai-whisper\n",
    "!sudo apt update && sudo apt install ffmpeg\n",
    "!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7187c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "stt_model = whisper.load_model(\"turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stt_model.transcribe(\"Camilla.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcef377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "whisperProcessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "whisperModel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = whisperProcessor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = whisperModel.generate(input_features)\n",
    "# decode token ids to text\n",
    "# transcription = whisperProcessor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "\n",
    "transcription = whisperProcessor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6e4d2",
   "metadata": {},
   "source": [
    "### Vosk\n",
    "- Less accurate than whisper\n",
    "- Less resource intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b80c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f22674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b928c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from stt/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from stt/vosk-model-small-en-us-0.15/graph/HCLr.fst stt/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo stt/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: what\n",
      "Partial: what\n",
      "Partial: what are\n",
      "Partial: what are\n",
      "Partial: what are your\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Transcript: what are your capabilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "SetLogLevel(0)  # Suppress verbose logging\n",
    "\n",
    "model = Model(\"stt/vosk-model-small-en-us-0.15\")\n",
    "wf = wave.open(\"output.wav\", \"rb\")\n",
    "assert wf.getnchannels() == 1 and wf.getsampwidth() == 2 and wf.getframerate() in (8000,16000,44100), \\\n",
    "       \"Use mono WAV with 16-bit samples\"\n",
    "\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)\n",
    "\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        results.append(result.get(\"text\", \"\"))\n",
    "    else:\n",
    "        # Optional: collect partial results if desired\n",
    "        partial = json.loads(rec.PartialResult()).get(\"partial\", \"\")\n",
    "        if partial:\n",
    "            print(\"Partial:\", partial)\n",
    "\n",
    "# Don't forget the final result!\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "results.append(final_result.get(\"text\", \"\"))\n",
    "\n",
    "# Join non-empty results\n",
    "transcript = \" \".join([r for r in results if r])\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe1e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are your capabilities', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cea793",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbda70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "ROOT_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"LLM\"\n",
    "\n",
    "with open(ROOT_PATH / \"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = cfg['huggingface']['api_token']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52495b",
   "metadata": {},
   "source": [
    "### Text-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304b17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_task: str,\n",
    "        formatted_kb: str,\n",
    "        output_instructions: str,\n",
    "        delimiter: str = \"###\",\n",
    "        model_name: str = None,\n",
    "        hf_pipeline: callable = None,\n",
    "        pipeline_kwargs: dict = None,\n",
    "        generation_kwargs: dict = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        llm_task: short description of the LLM’s task\n",
    "        formatted_kb: pre-formatted knowledge base text\n",
    "        output_instructions: any extra instructions for the assistant\n",
    "        delimiter: how to split raw generations to find the assistant’s reply\n",
    "\n",
    "        Either provide model_name (to build a HF pipeline) or an existing hf_pipeline.\n",
    "        pipeline_kwargs: kwargs passed to transformers.pipeline\n",
    "        generation_kwargs: kwargs passed to the pipeline when generating (e.g., max_new_tokens, temperature)\n",
    "        \"\"\"\n",
    "        self.llm_task = llm_task\n",
    "        self.formatted_kb = formatted_kb\n",
    "        self.output_instructions = output_instructions\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "        # Initialize pipeline\n",
    "        if hf_pipeline is not None:\n",
    "            self.pipe = hf_pipeline\n",
    "        elif model_name is not None:\n",
    "            pipeline_kwargs = pipeline_kwargs or {}\n",
    "            self.pipe = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model=model_name,\n",
    "                **pipeline_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"You must provide either hf_pipeline or model_name.\")\n",
    "\n",
    "        # Generation defaults\n",
    "        self.generation_kwargs = {\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"max_new_tokens\": 500,\n",
    "        }\n",
    "        if generation_kwargs:\n",
    "            self.generation_kwargs.update(generation_kwargs)\n",
    "\n",
    "        # Conversation state\n",
    "        self.history = []  # list of tuples (role, text)\n",
    "\n",
    "    def add_message(self, role: str, text: str):\n",
    "        self.history.append((role, text))\n",
    "\n",
    "    def build_prompt(self) -> str:\n",
    "        segments = [f\"Task: {self.llm_task}\", f\"Knowledge_base:\\n{self.formatted_kb}\"]\n",
    "        for role, text in self.history:\n",
    "            segments.append(f\"{role}: {text}\")\n",
    "        segments.append(f\"{self.output_instructions}\")\n",
    "        return \"\\n\".join(segments)\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Send a user question and get the assistant's reply.\"\"\"\n",
    "        # append user message\n",
    "        self.add_message(\"User\", question)\n",
    "\n",
    "        # generate\n",
    "        prompt = self.build_prompt()\n",
    "        out = self.pipe(prompt, **self.generation_kwargs)\n",
    "        raw = out[0].get('generated_text', out[0])\n",
    "\n",
    "        # extract assistant reply\n",
    "        parts = raw.split(self.delimiter)\n",
    "        reply = parts[-1].strip() if len(parts) > 1 else raw[len(prompt):].strip()\n",
    "\n",
    "        # append assistant message\n",
    "        self.add_message(\"Assistant\", reply)\n",
    "        return reply\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_PATH / \"knowledge_base.yaml\", \"r\") as f:\n",
    "    knowledge_base = yaml.safe_load(f)\n",
    "\n",
    "formatted_kb = yaml.dump(knowledge_base, default_flow_style=False)\n",
    "\n",
    "\n",
    "delimiter = \"#####\"\n",
    "\n",
    "llm_task = \"\"\"\n",
    "You are a friendly retail customer service assistant robot.\n",
    "You will be provided with a customer's question or request related to products, product locations, store policies.\n",
    "You will also be provided with a store's knowledge base containing information about products, promotions, and store policies.\n",
    "\n",
    "You must select one of the following two response types based on the customer's question or request:\n",
    "\n",
    "Response Type 1 - Product information:\n",
    "  - If the customer's question is about details of a specific product or promotion, briefly provide the relevant information from the store's knowledge base.\n",
    "\n",
    "Response Type 2 - Product location:\n",
    "  - If the customer asks for guidance to find or navigate directly to a specific product or product category, you must provide ONLY the precise coordinates (x, y) for navigation from the store's knowledge base.\n",
    "  - If exact coordinates aren't available for the product, provide aisle information instead.\n",
    "\n",
    "Response Type 3 - End interaction:\n",
    "  - If the customer indicates they need no further assistance (e.g., “That's all, thanks” or “No, I'm good”), respond with type 3 and a polite closing message such as “Happy to help—have a great day!”\n",
    "\n",
    "Response Type 4 - Seek assistance:\n",
    "  - If the customer asks for help with a query that is not covered by the knowledge base, respond with type 4 and a message like \"I will get someone who can help you with your query.\"\n",
    "\n",
    "Output format:\n",
    "Response type: {delimiter} <1, 2, or 3>\n",
    "Response: {delimiter} <your reply>\n",
    "\"\"\"\n",
    "\n",
    "outputInstructions = f\"\"\"\n",
    "\n",
    "- Do NOT output any internal reasoning, thought process, or steps.\n",
    "- Do NOT use any other words or punctuation beyond what’s above.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153bc8e",
   "metadata": {},
   "source": [
    "#### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate --quiet\n",
    "%pip install PyYAML --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b08486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acf7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: ## (12.4, 5.8)\n",
      "LLM Response: ## (12.4, 5.8)\n",
      "Conversation history cleared.\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a notebook:\n",
    "from transformers import pipeline\n",
    "\n",
    "sess = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    text_pipe=llama_pipe,\n",
    "    output_instructions=outputInstructions\n",
    ")\n",
    "\n",
    "sess.conversation(\"Recommend an healthy milk\")\n",
    "sess.conversation(\"Can you take me to it?\")\n",
    "sess.clear_conversation(force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480088a",
   "metadata": {},
   "source": [
    "#### R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f50a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes -U \"huggingface_hub[cli]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28d4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# bnb = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# dsr1_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# dsr1_tokenizer = AutoTokenizer.from_pretrained(dsr1_model_name)\n",
    "\n",
    "# dsr1_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     dsr1_model_name,\n",
    "#     quantization_config=bnb,\n",
    "#     device_map=\"auto\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dceb81eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "#\n",
    "# # Using a Hugging Face model by name:\n",
    "# session = ChatSession(\n",
    "#     llm_task=\"Answer questions based on the KB.\",\n",
    "#     formatted_kb=\"Here is some knowledge: ...\",\n",
    "#     output_instructions=\"Assistant:\",\n",
    "#     model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     pipeline_kwargs={\"device\": 0},\n",
    "#     generation_kwargs={\"temperature\": 0.5, \"max_new_tokens\": 200},\n",
    "# )\n",
    "# reply = session.ask(\"What is the capital of France?\")\n",
    "# print(reply)\n",
    "#\n",
    "# Using an existing pipeline for DeepSeek-R1:\n",
    "ds_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    device=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e34cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=ds_pipeline,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 500,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a486f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<your reply>\n",
      "\n",
      "- Do NOT output any internal reasoning, thought process, or steps.\n",
      "- Do NOT use any other words or punctuation beyond what’s above.\n",
      "\"\"\"\n",
      "\n",
      "Okay, so let me try to figure out how to handle this task step by step. The user asked if there's any milk in the store. My job is to determine whether to give them product info, suggest navigating via coordinates, ask for more details, or decline unless certain conditions apply like not needing further assistance.\n",
      "\n",
      "First, looking at the Knowledge Base. Under'milk', there are three items listed: Brand A, which has low-fat dairy milk; brand B, which sells full-cream organic milk; both located in their respective coordinate areas. Also, there's a promotion called \"Buy 2 Get 1 Free\" specifically linked to Brand B Milk.\n",
      "\n",
      "The user's question was straightforward—\"Do you have any milk here.\" So it seems they're asking for confirmation that milk exists in the store but don't know where exactly because they might want directions to check quickly.\n",
      "\n",
      "Since the question isn't detailed enough to pinpoint an exact location or product line except for knowing those particular stores exist, perhaps we should consider Response Type 2—Product Location—but wait, maybe even better than that since the store itself offers promotions too?\n",
      "\n",
      "Alternatively, thinking again—the prompt didn’t specify wanting immediate location hints. It just wanted to confirm availability without specifics. But given the presence of multiple locations and the promotion, perhaps providing the necessary info would satisfy being helpful.\n",
      "\n",
      "But hold on—if I mention specific locations where milk is sold, especially focusing on the promotion area for brand B, then customers could easily access those areas when checking inventory. However, the original instructions say to only include coordinates or aisles if the needed data isn't present elsewhere.\n",
      "\n",
      "Looking back at the Knowledge Base under 'Promotions' for Brand B Milk, the promotion states \"Buy 2 Get 1 Free,\" which applies to Brand B. So does that mean that the store allows purchasing these two units and getting the third free? Is there something else associated with this offer besides the price discount?\n",
      "\n",
      "In terms of end users accessing that offer, perhaps mentioning it in a way that provides direct links to purchase these deals could be useful. For instance, stating that buying 2 gives 1 free doesn't necessarily require entering coordinates or searching anywhere—it's already known through its own promotion.\n",
      "\n",
      "Therefore, considering all factors, maybe it's appropriate to inform the user that they can find milk near Brands A and B, particularly using the specified coordinates for each location. This includes offering the exact x,y coordinates where milk is readily available, allowing\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afebad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer2 = session.ask(\"Which one is less expensive.\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer3 = session.ask(\"Take me to the milk.\")\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd41a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bedba",
   "metadata": {},
   "source": [
    "#### FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4fe55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"#####\"\n",
    "\n",
    "llm_task = \"\"\"\n",
    "You are a friendly retail customer service assistant robot.\n",
    "You will be provided with a customer's question or request related to products, product locations, store policies.\n",
    "You will also be provided with a store's knowledge base containing information about products, promotions, and store policies.\n",
    "\n",
    "You must select one of the following two response types based on the customer's question or request:\n",
    "\n",
    "Response Type 1 - Product information:\n",
    "  - If the customer's question is about details of a specific product or promotion, briefly provide the relevant information from the store's knowledge base.\n",
    "\n",
    "Response Type 2 - Product location:\n",
    "  - If the customer asks for guidance to find or navigate directly to a specific product or product category, you must provide ONLY the precise coordinates (x, y) for navigation from the store's knowledge base.\n",
    "  - If exact coordinates aren't available for the product, provide aisle information instead.\n",
    "\n",
    "If neither the answer nor the coordinates or aisle are found in the store's knowledge base, politely respond with: \"I will get someone who can help you with your query.\"\n",
    "\"\"\"\n",
    "\n",
    "outputInstructions = f\"\"\"\n",
    "You must reply **exactly** (with no extra text or reasoning) in this format:\n",
    "\n",
    "Response type: {delimiter} <1, 2, or 3>  \n",
    "Response to user: {delimiter} <your reply>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "551c1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "flan_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    device=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cf79354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=flan_pipe,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39cfffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b83efe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193928fd",
   "metadata": {},
   "source": [
    "#### meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654f9295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [21:42<00:00, 651.23s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebd1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=llama_pipe,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5406e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<your reply>\n",
      "\n",
      "- Do NOT output any internal reasoning, thought process, or steps.\n",
      "- Do NOT use any other words or punctuation beyond what’s above.\n",
      "\n",
      "Please do not hesitate to ask if there's anything else you don't understand.\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e4928",
   "metadata": {},
   "source": [
    "#### unsloth/gemma-3-12b-it-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"unsloth/gemma-3-12b-it-GGUF\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "print(output[0][0][\"generated_text\"][-1][\"content\"])\n",
    "# Okay, let's take a look! \n",
    "# Based on the image, the animal on the candy is a **turtle**. \n",
    "# You can see the shell shape and the head and legs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b3131",
   "metadata": {},
   "source": [
    "### Audio/text to audio/text\n",
    "#### Qwen 2 Audio-7B Multimodal Audio Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import librosa\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation1 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n",
    "        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n",
    "    ]}\n",
    "]\n",
    "\n",
    "conversation2 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n",
    "        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "conversations = [conversation1, conversation2]\n",
    "\n",
    "text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n",
    "\n",
    "audios = []\n",
    "for conversation in conversations:\n",
    "    for message in conversation:\n",
    "        if isinstance(message[\"content\"], list):\n",
    "            for ele in message[\"content\"]:\n",
    "                if ele[\"type\"] == \"audio\":\n",
    "                    audios.append(\n",
    "                        librosa.load(\n",
    "                            BytesIO(urlopen(ele['audio_url']).read()), \n",
    "                            sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                    )\n",
    "\n",
    "inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "inputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\n",
    "inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_length=256)\n",
    "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15483e8f",
   "metadata": {},
   "source": [
    "## Text-to-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca79595",
   "metadata": {},
   "source": [
    "### eSpeak NG\n",
    "- Fully opensource\n",
    "- Mechanical sounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install espeak-ng\n",
    "# !sudo apt install mbrola mbrola-us1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def tts(text: str, voice: str = \"en-us+f3\", speed: int = 120, pitch: int = 50):\n",
    "    # voice example: \"mb-us1\" or \"en-us+f3\"\n",
    "    subprocess.call([\n",
    "        \"espeak-ng\",\n",
    "        \"-v\", voice,\n",
    "        \"-s\", str(speed),\n",
    "        \"-p\", str(pitch),\n",
    "        text\n",
    "    ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tts(\"Hello from Ubuntu Python, sounding nicer!\", voice=\"mb-us1\", speed=120, pitch=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9676",
   "metadata": {},
   "source": [
    "### Coqui-tts\n",
    "- Free for non-commercial use\n",
    "- Natural sounding\n",
    "- Many voice options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coqui-tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > You must confirm the following:\n",
      " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
      " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.87G/1.87G [02:45<00:00, 11.3MiB/s]\n",
      "100%|██████████| 4.37k/4.37k [00:00<00:00, 38.7kiB/s]\n",
      "100%|██████████| 361k/361k [00:00<00:00, 1.98MiB/s]\n",
      "100%|██████████| 32.0/32.0 [00:00<00:00, 142iB/s]\n",
      "100%|██████████| 7.75M/7.75M [00:14<00:00, 4.58MiB/s]"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tts = TTS(model_name=\"tts_models/en/vctk/vits\").to(device)\n",
    "# tts.tts_to_file(text=\"Natural voice is here!\", file_path=\"out.wav\")\n",
    "\n",
    "\n",
    "# Initialize TTS\n",
    "tts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claribel Dervla', 'Daisy Studious', 'Gracie Wise', 'Tammie Ema', 'Alison Dietlinde', 'Ana Florence', 'Annmarie Nele', 'Asya Anara', 'Brenda Stern', 'Gitta Nikolina', 'Henriette Usha', 'Sofia Hellen', 'Tammy Grit', 'Tanja Adelina', 'Vjollca Johnnie', 'Andrew Chipper', 'Badr Odhiambo', 'Dionisio Schuyler', 'Royston Min', 'Viktor Eka', 'Abrahan Mack', 'Adde Michal', 'Baldur Sanjin', 'Craig Gutsy', 'Damien Black', 'Gilberto Mathias', 'Ilkin Urbano', 'Kazuhiko Atallah', 'Ludvig Milivoj', 'Suad Qasim', 'Torcull Diarmuid', 'Viktor Menelaos', 'Zacharie Aimilios', 'Nova Hogarth', 'Maja Ruoho', 'Uta Obando', 'Lidiya Szekeres', 'Chandra MacFarland', 'Szofi Granger', 'Camilla Holmström', 'Lilya Stainthorpe', 'Zofija Kendrick', 'Narelle Moon', 'Barbora MacLean', 'Alexandra Hisakawa', 'Alma María', 'Rosemary Okafor', 'Ige Behringer', 'Filip Traverse', 'Damjan Chapman', 'Wulf Carlevaro', 'Aaron Dreschner', 'Kumar Dahl', 'Eugenio Mataracı', 'Ferran Simen', 'Xavier Hayasaka', 'Luis Moray', 'Marcos Rudaski']\n"
     ]
    }
   ],
   "source": [
    "# List speakers\n",
    "print(tts_model.speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb2a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annmarie.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TTS to a file, use a preset speaker\n",
    "tts_model.tts_to_file(\n",
    "  text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n",
    "\n",
    "# tts_model.tts_to_file(\n",
    "#   text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "#   speaker=\"Suad Qasim\",\n",
    "#   language=\"en\",\n",
    "#   file_path=\"Suad.wav\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb5577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camilla.wav'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using the model\n",
    "tts_model.tts_to_file(\n",
    "  text=generated_text,\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539426",
   "metadata": {},
   "source": [
    "## Full Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ef029",
   "metadata": {},
   "source": [
    "### PorcupineWake_voskASR_llamaLLM_coquiTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pvporcupine\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from TTS.api import TTS\n",
    "\n",
    "# ─── LOAD CONFIG ─────────────────────────────────────────────────────────────\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "PORCUPINE_KEYWORDS = cfg['porcupine']['keywords']\n",
    "PORCUPINE_ACCESS_KEY = cfg['porcupine']['access_key']\n",
    "AUDIO_SR         = cfg['audio']['sample_rate']\n",
    "QUESTION_DUR     = cfg['audio']['question_duration']\n",
    "VOSK_MODEL_PATH  = cfg['vosk']['model_path']\n",
    "LLAMA_CFG        = cfg['llama']\n",
    "TTS_CFG          = cfg['tts']\n",
    "HF_TOKEN = cfg['huggingface']['token']\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "# ─── INIT MODELS ─────────────────────────────────────────────────────────────\n",
    "porcupine = pvporcupine.create(access_key=PORCUPINE_ACCESS_KEY, keywords=PORCUPINE_KEYWORDS)\n",
    "\n",
    "SetLogLevel(0)\n",
    "vosk_model = Model(VOSK_MODEL_PATH)\n",
    "\n",
    "text_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLAMA_CFG['light_model_id'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=LLAMA_CFG['max_new_tokens'],\n",
    "    do_sample=LLAMA_CFG['do_sample']\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts_model = TTS(TTS_CFG['eng_model_name']).to(device)\n",
    "\n",
    "\n",
    "def listen_for_wakeword():\n",
    "    print(\"🔊 Listening for wake word...\")\n",
    "    with sd.InputStream(\n",
    "        samplerate=AUDIO_SR,\n",
    "        blocksize=porcupine.frame_length,\n",
    "        dtype=\"int16\",\n",
    "        channels=1\n",
    "    ) as stream:\n",
    "        while True:\n",
    "            pcm, _ = stream.read(porcupine.frame_length)\n",
    "            pcm = pcm.flatten().tolist()\n",
    "            if porcupine.process(pcm) >= 0:\n",
    "                print(\"✨ Wake word 'Jarvis' detected!\")\n",
    "                return\n",
    "\n",
    "\n",
    "def record_question(duration=QUESTION_DUR):\n",
    "    print(f\"🎤 Recording question for {duration}s…\")\n",
    "    audio = sd.rec(int(duration * AUDIO_SR), samplerate=AUDIO_SR, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    return audio.flatten()\n",
    "\n",
    "\n",
    "def transcribe_audio(audio: np.ndarray):\n",
    "    tmp = \"tmp_question.wav\"\n",
    "    write(tmp, AUDIO_SR, audio)\n",
    "    wf = wave.open(tmp, \"rb\")\n",
    "\n",
    "    rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    segments = []\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if not data:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            segments.append(json.loads(rec.Result()).get(\"text\", \"\"))\n",
    "    segments.append(json.loads(rec.FinalResult()).get(\"text\", \"\"))\n",
    "    os.remove(tmp)\n",
    "\n",
    "    transcript = \" \".join([s for s in segments if s])\n",
    "    print(\"📝 Transcribed question:\", transcript)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    print(\"🤖 Generating response…\")\n",
    "    out = text_pipe(prompt)\n",
    "    raw = out[0][\"generated_text\"]\n",
    "    answer = raw[len(prompt):].strip()\n",
    "    print(\"💬 Answer:\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def speak_and_play(text: str, fname=\"response.wav\"):\n",
    "    print(\"🔊 Synthesizing speech…\")\n",
    "    tts_model.tts_to_file(\n",
    "        text=text,\n",
    "        speaker=TTS_CFG['eng_speaker'],\n",
    "        # language=TTS_CFG['language'], #if english only model is used\n",
    "        file_path=fname\n",
    "    )\n",
    "    data, sr = sf.read(fname)\n",
    "    sd.play(data, sr)\n",
    "    sd.wait()\n",
    "    os.remove(fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Available speakers:\", tts_model.speakers)\n",
    "    # print(\"Supported langs:\",   tts_model.languages)\n",
    "    try:\n",
    "        while True:\n",
    "            listen_for_wakeword()\n",
    "            q_audio = record_question()\n",
    "            q_text  = transcribe_audio(q_audio)\n",
    "            if not q_text.strip():\n",
    "                print(\"⚠️ No speech detected—back to listening.\")\n",
    "                continue\n",
    "\n",
    "            ans = generate_response(q_text)\n",
    "            if not ans.strip():\n",
    "                print(\"⚠️ No answer generated—back to listening.\")\n",
    "                continue\n",
    "            # Speak the answer and play it\n",
    "            speak_and_play(ans)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Exiting.\")\n",
    "    finally:\n",
    "        porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39044ffd",
   "metadata": {},
   "source": [
    "### LLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76897bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading Knowledge Base from: /home/sunzid/Study/Course/EECS/Robotics/mobile_robot/Robuddy/LLM/storeAgent_knowledge_base.yaml\n",
      "Loading Llama model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Q: Where is Brand A milk?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: ##### 3 ##### [12.1, 4.5]\n",
      "--------------------------------------------------\n",
      "Q: How much is Brand B?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: ##### 2 ##### Brand B milk costs 3.49.\n",
      "--------------------------------------------------\n",
      "Q: Do you sell car tires?\n",
      "A: ##### 1\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import pipeline as hf_pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. Setup Paths\n",
    "ROOT_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"Robuddy\"\n",
    "CONFIG_PATH = ROOT_PATH / \"LLM/storeAgent_knowledge_base.yaml\"\n",
    "MAIN_CONFIG_PATH = ROOT_PATH / \"LLM/config.yaml\"\n",
    "\n",
    "# Load Main Config\n",
    "if not MAIN_CONFIG_PATH.exists():\n",
    "    print(f\"❌ Error: Config not found at {MAIN_CONFIG_PATH}\")\n",
    "    exit(1)\n",
    "\n",
    "with open(MAIN_CONFIG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Load Secrets\n",
    "os.environ[\"HF_TOKEN\"] = cfg['huggingface']['api_token']\n",
    "\n",
    "print(f\"📂 Loading Knowledge Base from: {CONFIG_PATH}\")\n",
    "\n",
    "# 2. Load the Knowledge Base\n",
    "if not CONFIG_PATH.exists():\n",
    "    print(f\"❌ Error: File not found at {CONFIG_PATH}\")\n",
    "    kb_data = {\"error\": \"Knowledge base file missing\"}\n",
    "else:\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        kb_data = yaml.safe_load(f)\n",
    "\n",
    "# Convert dict to a formatted JSON string\n",
    "kb_string = json.dumps(kb_data, indent=2, default=str)\n",
    "\n",
    "# 3. Define the System Prompt with FEW-SHOT EXAMPLES\n",
    "# Small models (1B) follow patterns better than instructions. \n",
    "# We provide examples of exactly what we want.\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "Role:\n",
    "You are a retail assistant robot. You have access to the \"Shop Knowledge Base\" below.\n",
    "\n",
    "Shop Knowledge Base:\n",
    "{kb_string}\n",
    "\n",
    "Instructions:\n",
    "1. Search the Knowledge Base for the user's query.\n",
    "2. Output ONLY the response code and content based on these rules:\n",
    "\n",
    "   - IF NOT FOUND:\n",
    "     Response: ##### 1\n",
    "\n",
    "   - IF FOUND (Location/Where/Navigation):\n",
    "     Response: ##### 3 ##### [x, y]\n",
    "     (Note: Output only the coordinate list. No text.)\n",
    "\n",
    "   - IF FOUND (Details/Price/Policy/Stock):\n",
    "     Response: ##### 2 ##### [Natural language answer]\n",
    "\n",
    "---\n",
    "EXAMPLES (Follow this pattern exactly):\n",
    "\n",
    "User: Do you sell jet engines?\n",
    "Assistant: ##### 1\n",
    "\n",
    "User: Where can I find the Milk?\n",
    "Assistant: ##### 3 ##### [12.3, 4.7]\n",
    "\n",
    "User: How much does Brand A cost?\n",
    "Assistant: ##### 2 ##### Brand A milk costs 2.99.\n",
    "\n",
    "User: What are your store hours?\n",
    "Assistant: ##### 1\n",
    "\n",
    "User: Where is Brand B located?\n",
    "Assistant: ##### 3 ##### [12.5, 4.9]\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MODEL SETUP\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Loading Llama model...\")\n",
    "text_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "def get_robot_response(user_query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "    \n",
    "    prompt = text_pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # KEY CHANGE: do_sample=False\n",
    "    # This forces \"Greedy Decoding\". The model picks the most likely next word.\n",
    "    # This kills creativity but drastically increases instruction following.\n",
    "    outputs = text_pipe(\n",
    "        prompt, \n",
    "        do_sample=False, \n",
    "        temperature=None, # Temperature is ignored if do_sample=False\n",
    "        top_p=None\n",
    "    )\n",
    "    \n",
    "    # Extract response\n",
    "    raw_response = outputs[0][\"generated_text\"]\n",
    "    answer = raw_response[len(prompt):].strip()\n",
    "    \n",
    "    # 4. Post-Processing Safety\n",
    "    # Even with good prompts, models might babble. \n",
    "    # We split by newline and take the first non-empty line.\n",
    "    lines = [line for line in answer.split('\\n') if line.strip()]\n",
    "    if lines:\n",
    "        return lines[0].strip()\n",
    "    return \"##### 1\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TEST CASES\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"-\" * 50)\n",
    "    q1 = \"Where is Brand A milk?\"\n",
    "    print(f\"Q: {q1}\")\n",
    "    print(f\"A: {get_robot_response(q1)}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    q2 = \"How much is Brand B?\"\n",
    "    print(f\"Q: {q2}\")\n",
    "    print(f\"A: {get_robot_response(q2)}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    q3 = \"Do you sell car tires?\"\n",
    "    print(f\"Q: {q3}\")\n",
    "    print(f\"A: {get_robot_response(q3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed39b8",
   "metadata": {},
   "source": [
    "### porcupineWake_sileroVAD_whisperASR_piperTTS_llama3.2-1B-instructLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running on device: cuda (CUDA available: True)\n",
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sunzid/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Loading Piper model from /home/sunzid/Study/Course/EECS/Robotics/mobile_robot/Robuddy/TTS/en_US-lessac-medium.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:123: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading Knowledge Base from: /home/sunzid/Study/Course/EECS/Robotics/mobile_robot/Robuddy/LLM/storeAgent_knowledge_base.yaml\n",
      "Loading Llama model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔊 Listening for wake word...\n",
      "✨ Wake word detected!\n",
      "🎤 Listening... (Speak now)\n",
      "   (Speech started...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Silence detected, stopping.)\n",
      "📝 Processing audio with Whisper...\n",
      "📝 Transcribed question:  Do you sell milk here?\n",
      "🤖 Generating response…\n",
      "💬 Raw Answer: ##### 1\n",
      "🔊 Synthesizing: I'm sorry, I couldn't find that information in my database.\n",
      "🔊 Listening for wake word...\n",
      "✨ Wake word detected!\n",
      "🎤 Listening... (Speak now)\n",
      "   (Speech started...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Silence detected, stopping.)\n",
      "📝 Processing audio with Whisper...\n",
      "📝 Transcribed question:  Do you see the car here?\n",
      "🤖 Generating response…\n",
      "💬 Raw Answer: I don't see a car.\n",
      "🔊 Synthesizing: I don't see a car.\n",
      "🔊 Listening for wake word...\n",
      "✨ Wake word detected!\n",
      "🎤 Listening... (Speak now)\n",
      "   (Speech started...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Silence detected, stopping.)\n",
      "📝 Processing audio with Whisper...\n",
      "📝 Transcribed question:  Do cell jet engines?\n",
      "🤖 Generating response…\n",
      "💬 Raw Answer: ##### 1\n",
      "🔊 Synthesizing: I'm sorry, I couldn't find that information in my database.\n",
      "🔊 Listening for wake word...\n",
      "✨ Wake word detected!\n",
      "🎤 Listening... (Speak now)\n",
      "   (Speech started...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Silence detected, stopping.)\n",
      "📝 Processing audio with Whisper...\n",
      "📝 Transcribed question:  Where can I find milk?\n",
      "🤖 Generating response…\n",
      "💬 Raw Answer: ##### 3 ##### [12.3, 4.7]\n",
      "🔊 Synthesizing: I have found the item. Navigating to its location now.\n",
      "📍 NAVIGATION TARGET: [12.3, 4.7]\n",
      "🔊 Listening for wake word...\n",
      "✨ Wake word detected!\n",
      "🎤 Listening... (Speak now)\n",
      "   (Speech started...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (Silence detected, stopping.)\n",
      "📝 Processing audio with Whisper...\n",
      "📝 Transcribed question:  Where is brand B located?\n",
      "🤖 Generating response…\n",
      "💬 Raw Answer: ##### 3 ##### [12.5, 4.9]\n",
      "🔊 Synthesizing: I have found the item. Navigating to its location now.\n",
      "📍 NAVIGATION TARGET: [12.5, 4.9]\n",
      "🔊 Listening for wake word...\n",
      "\n",
      "👋 Exiting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import wave\n",
    "import pvporcupine\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from piper import PiperVoice\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# ─── SETUP DEVICE ────────────────────────────────────────────────────────────\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"🚀 Running on device: {device} (CUDA available: {use_cuda})\")\n",
    "\n",
    "# ─── LOAD CONFIG ─────────────────────────────────────────────────────────────\n",
    "ROOT_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"Robuddy\"\n",
    "CONFIG_PATH = ROOT_PATH / \"LLM/config.yaml\"\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    print(f\"❌ Error: Config not found at {CONFIG_PATH}\")\n",
    "    exit(1)\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "PORCUPINE_KEYWORDS = cfg['porcupine']['keywords']\n",
    "PORCUPINE_ACCESS_KEY = cfg['porcupine']['PORCUPINE_KEY']\n",
    "AUDIO_SR         = cfg['audio']['sample_rate']\n",
    "QUESTION_DUR     = cfg['audio']['question_duration']\n",
    "LLAMA_CFG        = cfg['llama']\n",
    "HF_TOKEN         = cfg['huggingface']['api_token']\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# ─── INIT MODELS ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. Wake Word\n",
    "porcupine = pvporcupine.create(access_key=PORCUPINE_ACCESS_KEY, keywords=PORCUPINE_KEYWORDS)\n",
    "\n",
    "# 2. VAD Model\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                  model='silero_vad',\n",
    "                                  force_reload=False,\n",
    "                                  onnx=False)\n",
    "vad_model = vad_model.to(device)\n",
    "\n",
    "# 3. ASR Model\n",
    "print(\"Loading Whisper model...\")\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\").to(device)\n",
    "\n",
    "# 4. TTS Model\n",
    "PIPER_MODEL_PATH = ROOT_PATH / \"TTS/en_US-lessac-medium.onnx\" \n",
    "print(f\"Loading Piper model from {PIPER_MODEL_PATH}...\")\n",
    "\n",
    "if not PIPER_MODEL_PATH.exists():\n",
    "    print(f\"❌ Error: Piper model not found at {PIPER_MODEL_PATH}\")\n",
    "    exit(1)\n",
    "\n",
    "piper_voice = PiperVoice.load(str(PIPER_MODEL_PATH), use_cuda=use_cuda)\n",
    "\n",
    "# ─── LLM SETUP ─────────────────────────────────────────────────────\n",
    "\n",
    "KB_PATH = ROOT_PATH / \"LLM/storeAgent_knowledge_base.yaml\"\n",
    "print(f\"📂 Loading Knowledge Base from: {KB_PATH}\")\n",
    "\n",
    "# 1. Load the Knowledge Base\n",
    "if not KB_PATH.exists():\n",
    "    print(f\"❌ Error: KB file not found at {KB_PATH}\")\n",
    "    kb_data = {\"error\": \"Knowledge base file missing\"}\n",
    "else:\n",
    "    with open(KB_PATH, \"r\") as f:\n",
    "        kb_data = yaml.safe_load(f)\n",
    "\n",
    "# 2. Convert dict to a formatted JSON string\n",
    "# NOTE: Added 'default=str' to handle date objects safely\n",
    "kb_string = json.dumps(kb_data, indent=2, default=str)\n",
    "\n",
    "# 3. Define the System Prompt with FEW-SHOT EXAMPLES\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "Role:\n",
    "You are a retail assistant robot. You have access to the \"Shop Knowledge Base\" below.\n",
    "\n",
    "Shop Knowledge Base:\n",
    "{kb_string}\n",
    "\n",
    "Instructions:\n",
    "1. Search the Knowledge Base for the user's query.\n",
    "2. Output ONLY the response code and content based on these rules:\n",
    "\n",
    "   - IF NOT FOUND:\n",
    "     Response: ##### 1\n",
    "\n",
    "   - IF FOUND (Location/Where/Navigation):\n",
    "     Response: ##### 3 ##### [x, y]\n",
    "     (Note: Output only the coordinate list. No text.)\n",
    "\n",
    "   - IF FOUND (Details/Price/Policy/Stock):\n",
    "     Response: ##### 2 ##### [Natural language answer]\n",
    "\n",
    "---\n",
    "EXAMPLES (Follow this pattern exactly):\n",
    "\n",
    "User: Do you sell jet engines?\n",
    "Assistant: ##### 1\n",
    "\n",
    "User: Where can I find the Milk?\n",
    "Assistant: ##### 3 ##### [12.3, 4.7]\n",
    "\n",
    "User: How much does Brand A cost?\n",
    "Assistant: ##### 2 ##### Brand A milk costs 2.99.\n",
    "\n",
    "User: What are your store hours?\n",
    "Assistant: ##### 1\n",
    "\n",
    "User: Where is Brand B located?\n",
    "Assistant: ##### 3 ##### [12.5, 4.9]\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# 4. Load Llama Model\n",
    "print(\"Loading Llama model...\")\n",
    "text_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLAMA_CFG['light_model_id'], # Ensure this is \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=LLAMA_CFG['max_new_tokens'],\n",
    "    # Note: We override do_sample in the generate function\n",
    ")\n",
    "\n",
    "# ─── FUNCTIONS ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def listen_for_wakeword():\n",
    "    print(\"🔊 Listening for wake word...\")\n",
    "    with sd.InputStream(\n",
    "        samplerate=AUDIO_SR,\n",
    "        blocksize=porcupine.frame_length,\n",
    "        dtype=\"int16\",\n",
    "        channels=1\n",
    "    ) as stream:\n",
    "        while True:\n",
    "            pcm, _ = stream.read(porcupine.frame_length)\n",
    "            pcm = pcm.flatten().tolist()\n",
    "            if porcupine.process(pcm) >= 0:\n",
    "                print(\"✨ Wake word detected!\")\n",
    "                return\n",
    "\n",
    "\n",
    "def record_until_silence(sr=AUDIO_SR, silence_threshold=2.0, max_duration=15.0):\n",
    "    print(\"🎤 Listening... (Speak now)\")\n",
    "    \n",
    "    frame_len = 512\n",
    "    block_size = frame_len\n",
    "    \n",
    "    audio_buffer = []\n",
    "    speech_detected = False\n",
    "    silence_counter = 0\n",
    "    \n",
    "    max_chunks = int(max_duration * sr / block_size)\n",
    "    silence_chunks_limit = int(silence_threshold * sr / block_size)\n",
    "    \n",
    "    with sd.InputStream(samplerate=sr, blocksize=block_size, dtype=\"int16\", channels=1) as stream:\n",
    "        for _ in range(max_chunks):\n",
    "            pcm, _ = stream.read(block_size)\n",
    "            pcm_flat = pcm.flatten()\n",
    "            audio_buffer.append(pcm_flat)\n",
    "            \n",
    "            tensor_chunk = torch.from_numpy(pcm_flat.astype(np.float32) / 32768.0).unsqueeze(0).to(device)\n",
    "            speech_prob = vad_model(tensor_chunk, sr).item()\n",
    "            \n",
    "            if speech_prob > 0.5:\n",
    "                if not speech_detected:\n",
    "                    print(\"   (Speech started...)\")\n",
    "                    speech_detected = True\n",
    "                silence_counter = 0 \n",
    "            else:\n",
    "                if speech_detected:\n",
    "                    silence_counter += 1\n",
    "            \n",
    "            if speech_detected and silence_counter >= silence_chunks_limit:\n",
    "                print(\"   (Silence detected, stopping.)\")\n",
    "                break\n",
    "\n",
    "    full_audio = np.concatenate(audio_buffer)\n",
    "    if not speech_detected:\n",
    "        print(\"⚠️ No speech detected.\")\n",
    "        return np.array([], dtype=np.int16)\n",
    "    return full_audio\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_int16: np.ndarray):\n",
    "    print(\"📝 Processing audio with Whisper...\")\n",
    "    audio_float = audio_int16.astype(np.float32) / 32768.0\n",
    "\n",
    "    input_features = whisper_processor(\n",
    "        audio_float, \n",
    "        sampling_rate=AUDIO_SR, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features.to(device)\n",
    "\n",
    "    predicted_ids = whisper_model.generate(input_features)\n",
    "    transcript = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(\"📝 Transcribed question:\", transcript)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def generate_response(user_query: str):\n",
    "    print(\"🤖 Generating response…\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "    \n",
    "    # 1. Apply Chat Template\n",
    "    prompt = text_pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 2. Generate with Greedy Decoding (do_sample=False)\n",
    "    outputs = text_pipe(\n",
    "        prompt, \n",
    "        do_sample=False, \n",
    "        temperature=None, \n",
    "        top_p=None\n",
    "    )\n",
    "    \n",
    "    raw = outputs[0][\"generated_text\"]\n",
    "    answer = raw[len(prompt):].strip()\n",
    "    \n",
    "    # 3. Clean up lines\n",
    "    lines = [line for line in answer.split('\\n') if line.strip()]\n",
    "    if lines:\n",
    "        print(\"💬 Raw Answer:\", lines[0].strip())\n",
    "        return lines[0].strip()\n",
    "    \n",
    "    return \"##### 1\"\n",
    "\n",
    "\n",
    "def speak_and_play(text: str, fname=\"response.wav\"):\n",
    "    print(f\"🔊 Synthesizing: {text}\")\n",
    "    with wave.open(fname, \"wb\") as wav_file:\n",
    "        piper_voice.synthesize_wav(text, wav_file)\n",
    "    \n",
    "    data, sr = sf.read(fname)\n",
    "    sd.play(data, sr)\n",
    "    sd.wait()\n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "\n",
    "\n",
    "# ─── MAIN LOOP ───────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            listen_for_wakeword()\n",
    "            \n",
    "            # 1. Record\n",
    "            q_audio = record_until_silence(silence_threshold=1.5)\n",
    "            if len(q_audio) == 0: continue\n",
    "            \n",
    "            # 2. Transcribe\n",
    "            q_text = transcribe_audio(q_audio)\n",
    "            if not q_text.strip(): continue\n",
    "\n",
    "            # 3. Generate\n",
    "            ans = generate_response(q_text)\n",
    "            \n",
    "            # 4. Parse & Act\n",
    "            if \"##### 1\" in ans:\n",
    "                # Case 1: Not Found\n",
    "                speak_and_play(\"I'm sorry, I couldn't find that information in my database.\")\n",
    "            \n",
    "            elif \"##### 3\" in ans:\n",
    "                # Case 3: Location / Navigation\n",
    "                # Parse Coordinates [x, y] here if you need to send them to ROS\n",
    "                speak_and_play(\"I have found the item. Navigating to its location now.\")\n",
    "                print(f\"📍 NAVIGATION TARGET: {ans.split('#####')[-1].strip()}\")\n",
    "                \n",
    "            elif \"##### 2\" in ans:\n",
    "                # Case 2: Information\n",
    "                # Extract the text after the last delimiter\n",
    "                parts = ans.split(\"#####\")\n",
    "                if len(parts) >= 3:\n",
    "                    clean_text = parts[2].strip()\n",
    "                    speak_and_play(clean_text)\n",
    "                else:\n",
    "                    # Fallback if format is weird\n",
    "                    speak_and_play(ans.replace(\"#\", \"\"))\n",
    "            \n",
    "            else:\n",
    "                # Fallback for unexpected output\n",
    "                speak_and_play(ans)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Exiting.\")\n",
    "    finally:\n",
    "        porcupine.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b270f",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robuddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
