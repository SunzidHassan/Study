{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf052522",
   "metadata": {},
   "source": [
    "## Environment\n",
    "* Install Anaconda (install > nano ~/.bashrc > ~/anaconda3/bin/conda init > source ~/.bashrc)\n",
    "* Create Conda environment `conda create --name <your_environment_name> python=3.11`\n",
    "* Install CUDA toolkit\n",
    "* Install PyTorch (about 7GBs)\n",
    "* Install HuggingFace\n",
    "\n",
    "To free up space:\n",
    "```bash\n",
    "conda env remove --name <environment_name>\n",
    "conda clean --all\n",
    "rm -rf ~/.cache/huggingface/hub\n",
    "rm -rf ~/.cache/huggingface/transformers\n",
    "rm -rf ~/.cache/huggingface/datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568c8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 24 13:45:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 43%   48C    P3             30W /  184W |     601MiB /  12288MiB |     27%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2818      G   /usr/lib/xorg/Xorg                      284MiB |\n",
      "|    0   N/A  N/A            3139      G   /usr/bin/gnome-shell                     65MiB |\n",
      "|    0   N/A  N/A            3877      G   /opt/google/chrome/chrome                 1MiB |\n",
      "|    0   N/A  N/A            3931      G   ...ersion=20250623-050040.903000         89MiB |\n",
      "|    0   N/A  N/A            8688      G   ...ess --variations-seed-version        101MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA version\n",
    "!nvidia-smi\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c961416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "coqui-tts 0.26.2 requires transformers<4.52,>=4.47.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install/update required packages\n",
    "%pip install --quiet pvporcupine sounddevice soundfile librosa pyyaml matplotlib scipy transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22690519",
   "metadata": {},
   "source": [
    "## Wake word detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b0492",
   "metadata": {},
   "source": [
    "### Porcupine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"config.yaml\"\n",
    "\n",
    "def load_config(file_path: Path = CONFIG_PATH) -> str:\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {file_path}\")\n",
    "    cfg = yaml.safe_load(file_path.read_text())\n",
    "    if \"PORCUPINE_KEY\" not in cfg:\n",
    "        raise KeyError(\"Missing 'PORCUPINE_KEY' in config\")\n",
    "    return cfg[\"PORCUPINE_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05af8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcupine frame_length: 512\n"
     ]
    }
   ],
   "source": [
    "import pvporcupine\n",
    "porcupine = pvporcupine.create(access_key=load_config(), keywords=[\"jarvis\"])\n",
    "print(\"Porcupine frame_length:\", porcupine.frame_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e22447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 HDA NVidia: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
       "  1 HDA NVidia: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
       "  2 HDA NVidia: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
       "  3 HDA NVidia: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
       "  4 HD-Audio Generic: HDMI 0 (hw:1,3), ALSA (0 in, 8 out)\n",
       "  5 HD-Audio Generic: ALC256 Analog (hw:2,0), ALSA (2 in, 2 out)\n",
       "  6 hdmi, ALSA (0 in, 8 out)\n",
       "  7 pipewire, ALSA (64 in, 64 out)\n",
       "* 8 default, ALSA (64 in, 64 out)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "sd.query_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acef8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfae3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 'Jarvis' at sample 32256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, soundfile as sf, librosa\n",
    "\n",
    "data, sr = sf.read(\"output.wav\")\n",
    "if data.ndim > 1:\n",
    "    data = data[:,0]\n",
    "if sr != 16000:\n",
    "    data = librosa.resample(data, sr, 16000)\n",
    "    sr = 16000\n",
    "if data.dtype != np.int16:\n",
    "    data = (data * 32767).astype(np.int16)\n",
    "\n",
    "for i in range(0, len(data) - porcupine.frame_length + 1, porcupine.frame_length):\n",
    "    if porcupine.process(data[i:i+porcupine.frame_length]) >= 0:\n",
    "        print(\"Detected 'Jarvis' at sample\", i)\n",
    "        break\n",
    "porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2972c",
   "metadata": {},
   "source": [
    "## Image Object Detection\n",
    "### YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model8s = YOLO(\"models/YOLO/yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in model8s.track(source=0, classes=[0], conf=0.25, device=\"cuda:0\"):\n",
    "    human_near = (results[0].boxes.cls.cpu() == 0).any().item()\n",
    "    print(\"Human detected:\", human_near)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcfa04",
   "metadata": {},
   "source": [
    "## Speech-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba3517",
   "metadata": {},
   "source": [
    "### Whisper\n",
    "- By openai\n",
    "- High accuracy\n",
    "- Resoruce intensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai-whisper\n",
    "!sudo apt update && sudo apt install ffmpeg\n",
    "!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7187c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "stt_model = whisper.load_model(\"turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stt_model.transcribe(\"Camilla.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6e4d2",
   "metadata": {},
   "source": [
    "### Vosk\n",
    "- Less accurate than whisper\n",
    "- Less resource intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b80c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f22674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b928c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from stt/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from stt/vosk-model-small-en-us-0.15/graph/HCLr.fst stt/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo stt/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: what\n",
      "Partial: what\n",
      "Partial: what are\n",
      "Partial: what are\n",
      "Partial: what are your\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Transcript: what are your capabilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "SetLogLevel(0)  # Suppress verbose logging\n",
    "\n",
    "model = Model(\"stt/vosk-model-small-en-us-0.15\")\n",
    "wf = wave.open(\"output.wav\", \"rb\")\n",
    "assert wf.getnchannels() == 1 and wf.getsampwidth() == 2 and wf.getframerate() in (8000,16000,44100), \\\n",
    "       \"Use mono WAV with 16-bit samples\"\n",
    "\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)\n",
    "\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        results.append(result.get(\"text\", \"\"))\n",
    "    else:\n",
    "        # Optional: collect partial results if desired\n",
    "        partial = json.loads(rec.PartialResult()).get(\"partial\", \"\")\n",
    "        if partial:\n",
    "            print(\"Partial:\", partial)\n",
    "\n",
    "# Don't forget the final result!\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "results.append(final_result.get(\"text\", \"\"))\n",
    "\n",
    "# Join non-empty results\n",
    "transcript = \" \".join([r for r in results if r])\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe1e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are your capabilities', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cea793",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52495b",
   "metadata": {},
   "source": [
    "### Text-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22d7f2",
   "metadata": {},
   "source": [
    "#### deepseek-r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f199ceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface_hub[cli] in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (0.32.5)\n",
      "Collecting huggingface_hub[cli]\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from bitsandbytes) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: filelock in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from huggingface_hub[cli]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from huggingface_hub[cli]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from huggingface_hub[cli]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from huggingface_hub[cli]) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from huggingface_hub[cli]) (1.1.3)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.51)\n",
      "Requirement already satisfied: wcwidth in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2025.4.26)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: pfzy, InquirerPy, huggingface_hub, bitsandbytes\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.32.5\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.32.5:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.32.5\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [bitsandbytes][0m [bitsandbytes]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "coqui-tts 0.26.2 requires transformers<4.52,>=4.47.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed InquirerPy-0.3.4 bitsandbytes-0.46.0 huggingface_hub-0.33.1 pfzy-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes -U \"huggingface_hub[cli]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7eaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "dsr1_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "dsr1_tokenizer = AutoTokenizer.from_pretrained(dsr1_model_name)\n",
    "\n",
    "dsr1_model = AutoModelForCausalLM.from_pretrained(\n",
    "    dsr1_model_name,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc943b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dsr1_model.get_memory_footprint() / (1 << 30):.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e47e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    inputs = dsr1_tokenizer(prompt, return_tensors=\"pt\").to(dsr1_model.device)\n",
    "    outputs = dsr1_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,            # Enables sampling\n",
    "        temperature=0.6,           # Set desired creativity level\n",
    "        top_p=0.9,                 # Optional: nucleus sampling\n",
    "        repetition_penalty=1.1     # Optional: reduce repeats\n",
    "    )\n",
    "    return dsr1_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb2552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the relevant info:\n",
      "- Alice: senior engineer with 10 years at WidgetCorp.\n",
      "- Project Orion: deadline May 2026, focused on AI safety.\n",
      "\n",
      "Explain how we should conduct a risk assessment meeting next week. The meeting should include:\n",
      "\n",
      "a) How to determine whether there are any risks associated with the project.\n",
      "\n",
      "b) If so, what steps would be taken to address those risks.\n",
      "\n",
      "c) What if there are no risks?\n",
      "\n",
      "d) In addition, you\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Here is the relevant info:\n",
    "- Alice: senior engineer with 10 years at WidgetCorp.\n",
    "- Project Orion: deadline May 2026, focused on AI safety.\"\"\"\n",
    "\n",
    "user_prompt = \"Explain how we should conduct a risk assessment meeting next week.\"\n",
    "\n",
    "prompt = f\"{context}\\n\\n{user_prompt}\"\n",
    "print(generate(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df58a6",
   "metadata": {},
   "source": [
    "#### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3285e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate --quiet\n",
    "%pip install PyYAML --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b77813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "ROOT_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"LLM\"\n",
    "\n",
    "with open(ROOT_PATH / \"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "LLAMA_CFG = cfg['llama']\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = cfg['huggingface']['api_token']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88b1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLAMA_CFG['light_model_id'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=LLAMA_CFG['max_new_tokens'],\n",
    "    do_sample=LLAMA_CFG['do_sample']\n",
    ")\n",
    "\n",
    "\n",
    "with open(ROOT_PATH / \"knowledge_base.yaml\", \"r\") as f:\n",
    "    knowledge_base = yaml.safe_load(f)\n",
    "\n",
    "formatted_kb = yaml.dump(knowledge_base, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23650c45",
   "metadata": {},
   "source": [
    "#### Function Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3acf8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSession:\n",
    "    def __init__(self,\n",
    "                 llm_task: str,\n",
    "                 formatted_kb: str,\n",
    "                 text_pipe,               # your HF/text‐generation pipeline fn\n",
    "                 output_instructions: str,\n",
    "                 delimiter: str = \"###\"):\n",
    "        \"\"\"\n",
    "        llm_task:      short description of the LLM’s task\n",
    "        formatted_kb:  pre‐formatted knowledge base text\n",
    "        text_pipe:     a function like transformers.pipeline(…) that returns\n",
    "                       [{'generated_text': str}, …]\n",
    "        output_instructions: any extra “assistant should do X” text\n",
    "        delimiter:     how you split raw generations to find the assistant’s reply\n",
    "        \"\"\"\n",
    "        self.llm_task = llm_task\n",
    "        self.formatted_kb = formatted_kb\n",
    "        self.text_pipe = text_pipe\n",
    "        self.output_instructions = output_instructions\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "        # initialize conversation state\n",
    "        self.conversation_history = \"\"\n",
    "        self.previous_response = \"\"\n",
    "\n",
    "    def conversation(self, question: str):\n",
    "        \"\"\"Add the user’s question, generate an answer, and print it.\"\"\"\n",
    "        # 1) build or append history\n",
    "        if not self.conversation_history:\n",
    "            self.conversation_history = (\n",
    "                f\"Task: {self.llm_task}\\n\"\n",
    "                f\"Knowledge_base:\\n{self.formatted_kb}\\n\"\n",
    "                f\"User: {question}\"\n",
    "            )\n",
    "        else:\n",
    "            self.conversation_history += (\n",
    "                f\"\\nAssistant: {self.previous_response}\\n\"\n",
    "                f\"User: {question}\"\n",
    "            )\n",
    "\n",
    "        # 2) generate & log\n",
    "        self.previous_response = self.llm_generate_response()\n",
    "        print(\"LLM Response:\", self.previous_response)\n",
    "        return self.previous_response\n",
    "\n",
    "    def llm_generate_response(self) -> str:\n",
    "        \"\"\"Run the pipeline on the full prompt and return the last segment.\"\"\"\n",
    "        prompt = f\"{self.conversation_history}\\n{self.output_instructions}\"\n",
    "        out = self.text_pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        raw = out[0]['generated_text']\n",
    "        parts = raw.split(self.delimiter)\n",
    "        # return just the assistant’s final segment\n",
    "        return parts[-1].strip() if len(parts) > 1 else raw.strip()\n",
    "\n",
    "    def clear_conversation(self, force: bool = False):\n",
    "        \"\"\"\n",
    "        Reset history if force=True, otherwise do nothing.\n",
    "        (You can add whatever condition you like.)\n",
    "        \"\"\"\n",
    "        if force:\n",
    "            self.conversation_history = \"\"\n",
    "            self.previous_response = \"\"\n",
    "            print(\"Conversation history cleared.\")\n",
    "        else:\n",
    "            print(\"Continuing conversation. Pass force=True to clear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"#####\"\n",
    "\n",
    "llm_task = \"\"\"\n",
    "You are a friendly retail customer service assistant robot.\n",
    "You will be provided with a customer's question or request related to products, product locations, store policies.\n",
    "You will also be provided with a store's knowledge base containing information about products, promotions, and store policies.\n",
    "\n",
    "You must select one of the following two response types based on the customer's question or request:\n",
    "\n",
    "Response Type 1 - Product information:\n",
    "  - If the customer's question is about details of a specific product or promotion, briefly provide the relevant information from the store's knowledge base.\n",
    "\n",
    "Response Type 2 - Product location:\n",
    "  - If the customer asks for guidance to find or navigate directly to a specific product or product category, you must provide ONLY the precise coordinates (x, y) for navigation from the store's knowledge base.\n",
    "  - If exact coordinates aren't available for the product, provide aisle information instead.\n",
    "\n",
    "Response Type 3 - End interaction:\n",
    "  - If the customer indicates they need no further assistance (e.g., “That's all, thanks” or “No, I'm good”), respond with type 3 and a polite closing message such as “Happy to help—have a great day!”\n",
    "\n",
    "If neither the answer nor the coordinates or aisle are found in the store's knowledge base, politely respond with: \"I will get someone who can help you with your query.\"\n",
    "\"\"\"\n",
    "\n",
    "outputInstructions = f\"\"\"\n",
    "You must reply **exactly** (with no extra text or reasoning) in this format:\n",
    "\n",
    "Response type: {delimiter} <1, 2, or 3>  \n",
    "Response to user: {delimiter} <your reply>\n",
    "\n",
    "- Do NOT output any internal reasoning, thought process, or steps.\n",
    "- Do NOT use any other words or punctuation beyond what’s above.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153bc8e",
   "metadata": {},
   "source": [
    "##### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acf7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: ## (12.4, 5.8)\n",
      "LLM Response: ## (12.4, 5.8)\n",
      "Conversation history cleared.\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a notebook:\n",
    "from transformers import pipeline\n",
    "\n",
    "sess = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    text_pipe=llama_pipe,\n",
    "    output_instructions=outputInstructions\n",
    ")\n",
    "\n",
    "sess.conversation(\"Recommend an healthy milk\")\n",
    "sess.conversation(\"Can you take me to it?\")\n",
    "sess.clear_conversation(force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480088a",
   "metadata": {},
   "source": [
    "##### R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dceb81eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# 1) set up 4-bit quantization\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 2) load tokenizer & model\n",
    "DS_R1_CFG = cfg['ds_r1']['model_id']\n",
    "tokenizer = AutoTokenizer.from_pretrained(DS_R1_CFG)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "    DS_R1_CFG,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) build a text-generation pipeline around it\n",
    "\n",
    "dsr1_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=30,\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(delimiter),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: ## <your reply>\n",
      "\n",
      "- If you pick **1**, give a brief product/promotion info from the KB.  \n",
      "- If you pick **2**, give **ONLY** coordinates `(x, y)` or, if not in the KB, aisle info.  \n",
      "- If you pick **3**, give a polite closing (e.g. “Happy to help—have a great day!”).  \n",
      "- Do NOT include any markdown formatting, do NOT use any special characters except those already specified above.\n",
      "\n",
      "Alright, let me try to figure out how to approach this task step by step. The problem involves creating an appropriate response using the given instructions, so my goal here is to understand what exactly needs to be done and then apply it correctly.\n",
      "\n",
      "Firstly, the task outline gives clear steps that dictate how the AI should handle different scenarios. Let's break them down into manageable parts to avoid confusion.\n",
      "\n",
      "The first part is determining which response type to choose. There are three possible options, each serving a distinct purpose. So, depending on whether the user asked about product info, needed directions to locate something, or requested nothing helpful, we'll assign accordingly.\n",
      "\n",
      "Looking back at the example provided, when the user asked for recommendations regarding healthier milks, the system selected Response Type 1 because it was asking about specifics like brand and product features. Similarly, another scenario might fall under Response Type 2 where the user wanted direct navigational direction without more detailed info.\n",
      "\n",
      "So, moving forward, our plan is structured as follows:\n",
      "\n",
      "Step 1 – Identify the nature of the user’s inquiry.\n",
      "   - Is the user looking for product details?\n",
      "     → Use Response Type 1.\n",
      "   - Does the user want to know how to find a particular product?\n",
      "     → Use Response Type 2.\n",
      "   - Are there no questions needing clarification beyond basic presence?\n",
      "     → Use Response Type 3.\n",
      "\n",
      "Once determined, proceed to execute the corresponding action.\n",
      "\n",
      "In terms of handling Response Type 1, since the focus is on product details, especially perhaps including branding, nutritional information, etc., but according to the prompt, only a brief overview is needed. But considering the constraints of just providing product details without additional context may require selecting either the necessary info or narrowing it down.\n",
      "\n",
      "But wait, actually re-reading the initial instruction says that in the case of Response Type 1, we have to offer limited product-promotion info; however, sometimes it could involve offering multiple points unless there isn’t enough data. Hmm, maybe better phrased, in cases requiring product details, regardless of quantity, we’ll take whatever is present.\n",
      "\n",
      "Wait, upon closer reading, the second point states: For Response Type 2, even though some responses might contain extra information, users typically don't expect that level of detail. Therefore, during Query Handling, after ensuring the correct selection, once we've chosen Response Type 1 or Response Type 2, we must actuate appropriately.\n",
      "\n",
      "Given that, now, going through the examples again helps solidify understanding.\n",
      "\n",
      "Another thing to consider is the structure required in the output. It seems that the final response has to follow strictly these guidelines, mainly keeping it concise while adhering to the language requirements.\n",
      "\n",
      "Now, focusing specifically on the current user's request: \"Recommend a healthy milk.\" This doesn't specify anything else; thus, it falls under general recommendation, prompting us towards Response Type 1. However, recalling the Knowledge base includes various product details and promotions.\n",
      "\n",
      "Looking into the Knowledge Base for Healthier Milks:\n",
      "\n",
      "There's both Brand A and Brand B milks mentioned. Brand A offers low-fat, with smooth taste and high calcium content. Its coordinates are [12.1][4.5]. Brand B provides full cream, made from grass-fed cows, located at [12.5][4.9].\n",
      "\n",
      "Since the user didn't ask for coordinate-based info, he wants a general recommendation. Thus, under Response Type 1, the best course would be to inform him that both brands are suitable options.\n",
      "\n",
      "Therefore, in the Answer section, we'd mention that Brand A and Brand B are available, highlighting their benefits.\n",
      "\n",
      "Additionally, checking if the Coordinates are applicable. In this scenario, since there isn't a specific coordinate being sought, but rather a general recommendation, responding via coordinates wouldn't fit here—it would belong to Response Type 2, seeking navigation. Since we're dealing with product details here, non-coordinate-specific answers go to Response Type 1.\n",
      "\n",
      "Moreover, does the user indicate that they’re open to navigating? No, clearly—he’s requesting a recommendation, not necessarily searching around. Hence, Option 1 applies.\n",
      "\n",
      "Putting it together, the Output would state that both brands are recommended due to their respective nutritional properties and availability.\n",
      "\n",
      "Thus, putting it all together, the response should adhere to the rules outlined earlier, choosing the right type, extracting the necessary info or applyingaisles where available, otherwise indicating that someone knowledgeable can assist.\n",
      "\n",
      "Alternatively, think about edge cases. What if the user specifies a particular aspect beyond general recommendation?\n",
      "\n",
      "For instance, suppose the user said, 'Find the lowest cost option.' Then, we’d target Response Type 2 with coordinates leading toward Brand B Milk, whose price is lower than Brand A's.\n",
      "\n",
      "However, in the original question, the user simply requests'recommend' — meaning preference between brands based on\n",
      "LLM Response: ## <your reply>\n",
      "\n",
      "- If you pick **1**, give a brief product/promotion info from the KB.  \n",
      "- If you pick **2**, give **ONLY** coordinates `(x, y)` or, if not in the KB, aisle info.  \n",
      "- If you pick **3**, give a polite closing (e.g. “Happy to help—have a great day!”).  \n",
      "- If neither the answer nor the coordinates or aisles are found... respond with: \"I will get someone who can help you with your query.\"\n",
      "</think>\n",
      "\n",
      "Product information  \n",
      "Brand A Milk  \n",
      "\n",
      "```<product的信息>\n",
      "品牌A牛奶</``>\n",
      "Conversation history cleared.\n"
     ]
    }
   ],
   "source": [
    "# 4) plug it into ChatSession exactly as before\n",
    "sess = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    text_pipe=dsr1_pipe,\n",
    "    output_instructions=outputInstructions\n",
    ")\n",
    "\n",
    "sess.conversation(\"Recommend a healthy milk\")\n",
    "sess.conversation(\"Can you take me to it?\")\n",
    "sess.clear_conversation(force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c1b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82b3131",
   "metadata": {},
   "source": [
    "### Audio/text to audio/text\n",
    "#### Qwen 2 Audio-7B Multimodal Audio Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdc3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen2AudioForConditionalGeneration, AutoProcessor\n",
      "\u001b[32m      6\u001b[39m processor = AutoProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2-Audio-7B-Instruct\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mQwen2AudioForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2-Audio-7B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/RoBuddy/lib/python3.13/site-packages/transformers/modeling_utils.py:313\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    311\u001b[39m old_dtype = torch.get_default_dtype()\n",
      "\u001b[32m    312\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    314\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[32m    315\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/RoBuddy/lib/python3.13/site-packages/transformers/modeling_utils.py:4392\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n",
      "\u001b[32m   4390\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m   4391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m4392\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m   4393\u001b[39m             (\n",
      "\u001b[32m   4394\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   4395\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   4396\u001b[39m             )\n",
      "\u001b[32m   4397\u001b[39m         )\n",
      "\u001b[32m   4399\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n",
      "\u001b[32m   4400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import librosa\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation1 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n",
    "        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n",
    "    ]}\n",
    "]\n",
    "\n",
    "conversation2 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n",
    "        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "conversations = [conversation1, conversation2]\n",
    "\n",
    "text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n",
    "\n",
    "audios = []\n",
    "for conversation in conversations:\n",
    "    for message in conversation:\n",
    "        if isinstance(message[\"content\"], list):\n",
    "            for ele in message[\"content\"]:\n",
    "                if ele[\"type\"] == \"audio\":\n",
    "                    audios.append(\n",
    "                        librosa.load(\n",
    "                            BytesIO(urlopen(ele['audio_url']).read()), \n",
    "                            sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                    )\n",
    "\n",
    "inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "inputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\n",
    "inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_length=256)\n",
    "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15483e8f",
   "metadata": {},
   "source": [
    "## Text-to-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca79595",
   "metadata": {},
   "source": [
    "### eSpeak NG\n",
    "- Fully opensource\n",
    "- Mechanical sounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install espeak-ng\n",
    "# !sudo apt install mbrola mbrola-us1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def tts(text: str, voice: str = \"en-us+f3\", speed: int = 120, pitch: int = 50):\n",
    "    # voice example: \"mb-us1\" or \"en-us+f3\"\n",
    "    subprocess.call([\n",
    "        \"espeak-ng\",\n",
    "        \"-v\", voice,\n",
    "        \"-s\", str(speed),\n",
    "        \"-p\", str(pitch),\n",
    "        text\n",
    "    ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tts(\"Hello from Ubuntu Python, sounding nicer!\", voice=\"mb-us1\", speed=120, pitch=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9676",
   "metadata": {},
   "source": [
    "### Coqui-tts\n",
    "- Free for non-commercial use\n",
    "- Natural sounding\n",
    "- Many voice options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coqui-tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > You must confirm the following:\n",
      " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
      " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.87G/1.87G [02:45<00:00, 11.3MiB/s]\n",
      "100%|██████████| 4.37k/4.37k [00:00<00:00, 38.7kiB/s]\n",
      "100%|██████████| 361k/361k [00:00<00:00, 1.98MiB/s]\n",
      "100%|██████████| 32.0/32.0 [00:00<00:00, 142iB/s]\n",
      "100%|██████████| 7.75M/7.75M [00:14<00:00, 4.58MiB/s]"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tts = TTS(model_name=\"tts_models/en/vctk/vits\").to(device)\n",
    "# tts.tts_to_file(text=\"Natural voice is here!\", file_path=\"out.wav\")\n",
    "\n",
    "\n",
    "# Initialize TTS\n",
    "tts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claribel Dervla', 'Daisy Studious', 'Gracie Wise', 'Tammie Ema', 'Alison Dietlinde', 'Ana Florence', 'Annmarie Nele', 'Asya Anara', 'Brenda Stern', 'Gitta Nikolina', 'Henriette Usha', 'Sofia Hellen', 'Tammy Grit', 'Tanja Adelina', 'Vjollca Johnnie', 'Andrew Chipper', 'Badr Odhiambo', 'Dionisio Schuyler', 'Royston Min', 'Viktor Eka', 'Abrahan Mack', 'Adde Michal', 'Baldur Sanjin', 'Craig Gutsy', 'Damien Black', 'Gilberto Mathias', 'Ilkin Urbano', 'Kazuhiko Atallah', 'Ludvig Milivoj', 'Suad Qasim', 'Torcull Diarmuid', 'Viktor Menelaos', 'Zacharie Aimilios', 'Nova Hogarth', 'Maja Ruoho', 'Uta Obando', 'Lidiya Szekeres', 'Chandra MacFarland', 'Szofi Granger', 'Camilla Holmström', 'Lilya Stainthorpe', 'Zofija Kendrick', 'Narelle Moon', 'Barbora MacLean', 'Alexandra Hisakawa', 'Alma María', 'Rosemary Okafor', 'Ige Behringer', 'Filip Traverse', 'Damjan Chapman', 'Wulf Carlevaro', 'Aaron Dreschner', 'Kumar Dahl', 'Eugenio Mataracı', 'Ferran Simen', 'Xavier Hayasaka', 'Luis Moray', 'Marcos Rudaski']\n"
     ]
    }
   ],
   "source": [
    "# List speakers\n",
    "print(tts_model.speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb2a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annmarie.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TTS to a file, use a preset speaker\n",
    "tts_model.tts_to_file(\n",
    "  text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n",
    "\n",
    "# tts_model.tts_to_file(\n",
    "#   text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "#   speaker=\"Suad Qasim\",\n",
    "#   language=\"en\",\n",
    "#   file_path=\"Suad.wav\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb5577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camilla.wav'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using the model\n",
    "tts_model.tts_to_file(\n",
    "  text=generated_text,\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539426",
   "metadata": {},
   "source": [
    "## Commercial Assistant Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pvporcupine\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from TTS.api import TTS\n",
    "\n",
    "# ─── LOAD CONFIG ─────────────────────────────────────────────────────────────\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "PORCUPINE_KEYWORDS = cfg['porcupine']['keywords']\n",
    "PORCUPINE_ACCESS_KEY = cfg['porcupine']['access_key']\n",
    "AUDIO_SR         = cfg['audio']['sample_rate']\n",
    "QUESTION_DUR     = cfg['audio']['question_duration']\n",
    "VOSK_MODEL_PATH  = cfg['vosk']['model_path']\n",
    "LLAMA_CFG        = cfg['llama']\n",
    "TTS_CFG          = cfg['tts']\n",
    "HF_TOKEN = cfg['huggingface']['token']\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "# ─── INIT MODELS ─────────────────────────────────────────────────────────────\n",
    "porcupine = pvporcupine.create(access_key=PORCUPINE_ACCESS_KEY, keywords=PORCUPINE_KEYWORDS)\n",
    "\n",
    "SetLogLevel(0)\n",
    "vosk_model = Model(VOSK_MODEL_PATH)\n",
    "\n",
    "text_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLAMA_CFG['light_model_id'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=LLAMA_CFG['max_new_tokens'],\n",
    "    do_sample=LLAMA_CFG['do_sample']\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts_model = TTS(TTS_CFG['eng_model_name']).to(device)\n",
    "\n",
    "\n",
    "def listen_for_wakeword():\n",
    "    print(\"🔊 Listening for wake word...\")\n",
    "    with sd.InputStream(\n",
    "        samplerate=AUDIO_SR,\n",
    "        blocksize=porcupine.frame_length,\n",
    "        dtype=\"int16\",\n",
    "        channels=1\n",
    "    ) as stream:\n",
    "        while True:\n",
    "            pcm, _ = stream.read(porcupine.frame_length)\n",
    "            pcm = pcm.flatten().tolist()\n",
    "            if porcupine.process(pcm) >= 0:\n",
    "                print(\"✨ Wake word 'Jarvis' detected!\")\n",
    "                return\n",
    "\n",
    "\n",
    "def record_question(duration=QUESTION_DUR):\n",
    "    print(f\"🎤 Recording question for {duration}s…\")\n",
    "    audio = sd.rec(int(duration * AUDIO_SR), samplerate=AUDIO_SR, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    return audio.flatten()\n",
    "\n",
    "\n",
    "def transcribe_audio(audio: np.ndarray):\n",
    "    tmp = \"tmp_question.wav\"\n",
    "    write(tmp, AUDIO_SR, audio)\n",
    "    wf = wave.open(tmp, \"rb\")\n",
    "\n",
    "    rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    segments = []\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if not data:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            segments.append(json.loads(rec.Result()).get(\"text\", \"\"))\n",
    "    segments.append(json.loads(rec.FinalResult()).get(\"text\", \"\"))\n",
    "    os.remove(tmp)\n",
    "\n",
    "    transcript = \" \".join([s for s in segments if s])\n",
    "    print(\"📝 Transcribed question:\", transcript)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    print(\"🤖 Generating response…\")\n",
    "    out = text_pipe(prompt)\n",
    "    raw = out[0][\"generated_text\"]\n",
    "    answer = raw[len(prompt):].strip()\n",
    "    print(\"💬 Answer:\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def speak_and_play(text: str, fname=\"response.wav\"):\n",
    "    print(\"🔊 Synthesizing speech…\")\n",
    "    tts_model.tts_to_file(\n",
    "        text=text,\n",
    "        speaker=TTS_CFG['eng_speaker'],\n",
    "        # language=TTS_CFG['language'], #if english only model is used\n",
    "        file_path=fname\n",
    "    )\n",
    "    data, sr = sf.read(fname)\n",
    "    sd.play(data, sr)\n",
    "    sd.wait()\n",
    "    os.remove(fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Available speakers:\", tts_model.speakers)\n",
    "    # print(\"Supported langs:\",   tts_model.languages)\n",
    "    try:\n",
    "        while True:\n",
    "            listen_for_wakeword()\n",
    "            q_audio = record_question()\n",
    "            q_text  = transcribe_audio(q_audio)\n",
    "            if not q_text.strip():\n",
    "                print(\"⚠️ No speech detected—back to listening.\")\n",
    "                continue\n",
    "\n",
    "            ans = generate_response(q_text)\n",
    "            if not ans.strip():\n",
    "                print(\"⚠️ No answer generated—back to listening.\")\n",
    "                continue\n",
    "            # Speak the answer and play it\n",
    "            speak_and_play(ans)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Exiting.\")\n",
    "    finally:\n",
    "        porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b270f",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robuddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
