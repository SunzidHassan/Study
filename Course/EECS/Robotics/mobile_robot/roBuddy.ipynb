{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed304b2f",
   "metadata": {},
   "source": [
    "## Wake word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daac7e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pvporcupine in /home/sunzid/Study/.conda/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: pyyaml in /home/sunzid/Study/.conda/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: sounddevice in /home/sunzid/Study/.conda/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Requirement already satisfied: scipy in /home/sunzid/Study/.conda/lib/python3.11/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from scipy) (2.2.5)\n",
      "Requirement already satisfied: matplotlib in /home/sunzid/Study/.conda/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: soundfile in /home/sunzid/Study/.conda/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from soundfile) (2.2.5)\n",
      "Requirement already satisfied: pycparser in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from librosa) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from librosa) (1.15.2)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from librosa) (0.13.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from librosa) (4.13.2)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: packaging in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Collecting requests>=2.19.0 (from pooch>=1.1->librosa)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/sunzid/Study/.conda/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: urllib3, threadpoolctl, soxr, msgpack, llvmlite, lazy_loader, joblib, idna, charset-normalizer, certifi, audioread, scikit-learn, requests, numba, pooch, librosa\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [librosa]3/16\u001b[0m [numba]-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed audioread-3.0.1 certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 joblib-1.5.0 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 msgpack-1.1.0 numba-0.61.2 pooch-1.8.2 requests-2.32.3 scikit-learn-1.6.1 soxr-0.5.0.post1 threadpoolctl-3.6.0 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pvporcupine\n",
    "!pip install pyyaml\n",
    "!pip install sounddevice\n",
    "!pip install scipy\n",
    "!pip install matplotlib\n",
    "!pip install soundfile\n",
    "!pip install librosa\n",
    "# !apt install libportaudio2 libportaudiocpp0 portaudio19-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5beb113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path('/home/sunzid/Study/Course/EECS/Robotics/mobile_robot/config.yaml')\n",
    "\n",
    "\n",
    "def load_config(file_path: Path = CONFIG_PATH):\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {file_path}\")\n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    # return a tuple (porcupine_key, openai_key, chat_model)\n",
    "    return (\n",
    "        cfg['PORCUPINE_KEY']\n",
    "        # cfg['OPENAI_KEY'],\n",
    "        # cfg['OPENAI_CHAT_MODEL']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c1f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvporcupine\n",
    "\n",
    "# porcupine_key, openai_key, chat_model = load_config()\n",
    "porcupine_key = load_config()\n",
    "\n",
    "porcupine = pvporcupine.create(\n",
    "    access_key=porcupine_key,\n",
    "    keywords=['jarvis']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4265cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 5  # Duration of recording\n",
    "\n",
    "print(\"Recording...\")\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording complete.\")\n",
    "write('output.wav', fs, myrecording)  # Save as WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4591744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 HDA NVidia: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
      "  1 HDA NVidia: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
      "  2 HDA NVidia: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
      "  3 HDA NVidia: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
      "  4 HD-Audio Generic: EI322QUR (hw:1,3), ALSA (0 in, 2 out)\n",
      "  5 HD-Audio Generic: ALC256 Analog (hw:2,0), ALSA (2 in, 2 out)\n",
      "  6 hdmi, ALSA (0 in, 8 out)\n",
      "  7 pipewire, ALSA (64 in, 64 out)\n",
      "* 8 default, ALSA (64 in, 64 out)\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca73a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keyword detected!\n"
     ]
    }
   ],
   "source": [
    "import pvporcupine\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# 1. Load your recorded audio\n",
    "data, samplerate = sf.read('output.wav')\n",
    "\n",
    "# Porcupine expects:\n",
    "# - mono audio\n",
    "# - 16-bit int PCM\n",
    "# - sample rate: typically 16000 Hz\n",
    "# - frame length: usually 512 samples\n",
    "\n",
    "# Ensure mono\n",
    "if data.ndim == 2:\n",
    "    data = data[:, 0]  # take left channel\n",
    "\n",
    "# Resample if needed (Porcupine default is 16000 Hz)\n",
    "if samplerate != 16000:\n",
    "    import librosa\n",
    "    data = librosa.resample(data, orig_sr=samplerate, target_sr=16000)\n",
    "    samplerate = 16000\n",
    "\n",
    "# Convert to int16 if needed\n",
    "if data.dtype != np.int16:\n",
    "    data = (data * 32767).astype(np.int16)\n",
    "\n",
    "\n",
    "frame_length = porcupine.frame_length\n",
    "\n",
    "# 3. Iterator to get audio frames\n",
    "def audio_frame_generator(data, frame_length):\n",
    "    for i in range(0, len(data) - frame_length + 1, frame_length):\n",
    "        yield data[i:i + frame_length]\n",
    "\n",
    "# 4. Detection loop\n",
    "for audio_frame in audio_frame_generator(data, frame_length):\n",
    "    keyword_index = porcupine.process(audio_frame)\n",
    "    if keyword_index >= 0:\n",
    "        print(\"✅ Keyword detected!\")\n",
    "        break\n",
    "\n",
    "porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce04928",
   "metadata": {},
   "source": [
    "## Full Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc964e",
   "metadata": {},
   "source": [
    "Create a python virtual environment and install all inside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee497a43",
   "metadata": {},
   "source": [
    "┌────────────────────────────┐\n",
    "│  Audio Capture Node        │──► raw audio ──┐\n",
    "└────────────────────────────┘               │\n",
    "                                             ▼\n",
    "┌────────────────────────────┐         ┌──────────────────┐\n",
    "│  Wake‑Word Detector        │────────►│  Dialog Manager  │\n",
    "│  (e.g. Porcupine, Snowboy) │         └──────────────────┘\n",
    "└────────────────────────────┘                  │\n",
    "       ▲             │                          │\n",
    "       │             │ yes “Hey Robot!”          ▼\n",
    "       │ no trigger  │                  ┌─────────────────┐\n",
    "       └─────────────┘                  │  Qwen2.5 Omni   │\n",
    "                                        │  (ASR+LLM+MM)   │\n",
    "                                        └─────────────────┘\n",
    "                                                 │\n",
    "                                            text/audio\n",
    "                                                 ▼\n",
    "                                      ┌─────────────────────┐\n",
    "                                      │  Intent Parser /    │\n",
    "                                      │  End‑of‑Dialog DET  │\n",
    "                                      └─────────────────────┘\n",
    "                                       ┌─────────┴─────────┐\n",
    "                                       │                   │\n",
    "                                       ▼                   ▼\n",
    "                             “continue conv.”       “move + conv.” \n",
    "                              loop back to           dispatch to\n",
    "                              Dialog Manager         Motion Planner\n",
    "                                                         │\n",
    "                                                         ▼\n",
    "                                               ┌────────────────────┐\n",
    "                                               │  Motion Controller │\n",
    "                                               │  (ROS2 action lib) │\n",
    "                                               └────────────────────┘\n",
    "                                                         │\n",
    "                                                         ▼\n",
    "                                                    Bluetooth\n",
    "                                                    Speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for top‐level state machine\n",
    "state = \"IDLE\"\n",
    "\n",
    "while rclpy.ok():\n",
    "    if state == \"IDLE\":\n",
    "        if wake_word_detector.has_triggered():\n",
    "            state = \"CONVERSATION\"\n",
    "            dialog_manager.start_dialog()\n",
    "    elif state == \"CONVERSATION\":\n",
    "        response = dialog_manager.poll_response()\n",
    "        intent = intent_parser.parse(response)\n",
    "        if intent == \"move\":\n",
    "            state = \"MOVE\"\n",
    "        elif intent == \"end_conversation\":\n",
    "            dialog_manager.end_dialog()\n",
    "            state = \"IDLE\"\n",
    "        else:\n",
    "            # still chatting\n",
    "            continue\n",
    "    elif state == \"MOVE\":\n",
    "        # extract target from intent (e.g. “go to kitchen” → pose)\n",
    "        target_pose = intent_parser.extract_pose(response)\n",
    "        motion_planner.go_to(target_pose)\n",
    "        # Optionally chat en route or upon arrival\n",
    "        state = \"CONVERSATION\"\n",
    "    rclpy.spin_once()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from rclpy.qos import qos_profile_sensor_data\n",
    "\n",
    "# Placeholders for external dependencies\n",
    "# from wake_word import WakeWordDetector\n",
    "# from qwen_client import QwenClient\n",
    "# from tts import TTSClient\n",
    "# from intent_parser import IntentParser\n",
    "# from nav2_msgs.action import NavigateToPose\n",
    "# from rclpy.action import ActionClient\n",
    "\n",
    "class AssistantNode(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('assistant_node')\n",
    "\n",
    "        # State: IDLE, CONVERSATION, MOVE\n",
    "        self.state = 'IDLE'\n",
    "\n",
    "        # Audio subscription\n",
    "        self.audio_sub = self.create_subscription(\n",
    "            # Assume audio raw message type\n",
    "            # AudioData, '/audio_raw', self.audio_callback,\n",
    "            msg_type=None,\n",
    "            topic='/audio_raw',\n",
    "            callback=self.audio_callback,\n",
    "            qos_profile=qos_profile_sensor_data)\n",
    "\n",
    "        # Publisher for TTS audio\n",
    "        self.tts_pub = self.create_publisher(\n",
    "            # AudioData,\n",
    "            msg_type=None,\n",
    "            topic='/tts_audio',\n",
    "            qos_profile=10)\n",
    "\n",
    "        # Wake-word detector\n",
    "        # self.wake_detector = WakeWordDetector()\n",
    "\n",
    "        # Qwen client as ROS service\n",
    "        # self.qwen_client = QwenClient(self)\n",
    "\n",
    "        # TTS client\n",
    "        # self.tts_client = TTSClient(self)\n",
    "\n",
    "        # Intent parser\n",
    "        # self.intent_parser = IntentParser()\n",
    "\n",
    "        # Navigation action client\n",
    "        # self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n",
    "\n",
    "        # Buffer for audio frames\n",
    "        self.audio_buffer = []\n",
    "\n",
    "        # Timer to process state machine\n",
    "        self.create_timer(0.1, self.tick)\n",
    "\n",
    "    def audio_callback(self, msg):\n",
    "        # Append audio to buffer\n",
    "        self.audio_buffer.append(msg.data)\n",
    "\n",
    "        # If in IDLE, feed to wake-word detector\n",
    "        if self.state == 'IDLE':\n",
    "            # if self.wake_detector.is_triggered(self.audio_buffer):\n",
    "            if self.mock_wake_trigger():\n",
    "                self.get_logger().info('Wake word detected!')\n",
    "                self.state = 'CONVERSATION'\n",
    "                # initialize dialog context\n",
    "                # self.qwen_client.start_session()\n",
    "                self.audio_buffer.clear()\n",
    "\n",
    "    def tick(self):\n",
    "        if self.state == 'CONVERSATION':\n",
    "            # Collect user speech until silence or end token\n",
    "            user_text = self.mock_asr(self.audio_buffer)\n",
    "            if not user_text:\n",
    "                return\n",
    "            self.audio_buffer.clear()\n",
    "\n",
    "            # Send to Qwen\n",
    "            # response = self.qwen_client.query(user_text)\n",
    "            response = self.mock_qwen(user_text)\n",
    "            self.get_logger().info(f'Qwen response: {response}')\n",
    "\n",
    "            # Publish TTS\n",
    "            # audio_resp = self.tts_client.text_to_speech(response)\n",
    "            audio_resp = self.mock_tts(response)\n",
    "            self.tts_pub.publish(audio_resp)\n",
    "\n",
    "            # Parse intent\n",
    "            # intent, params = self.intent_parser.parse(response)\n",
    "            intent, params = self.mock_intent(response)\n",
    "\n",
    "            if intent == 'move':\n",
    "                target = params.get('location')\n",
    "                self.target_pose = self.resolve_pose(target)\n",
    "                self.state = 'MOVE'\n",
    "\n",
    "            elif intent == 'end':\n",
    "                self.get_logger().info('Conversation ended')\n",
    "                self.state = 'IDLE'\n",
    "\n",
    "            # else: continue conversation\n",
    "\n",
    "        elif self.state == 'MOVE':\n",
    "            # Dispatch navigation\n",
    "            # goal = NavigateToPose.Goal()\n",
    "            # goal.pose = self.target_pose\n",
    "            # self.nav_client.wait_for_server()\n",
    "            # self.nav_client.send_goal_async(goal)\n",
    "            self.mock_navigate(self.target_pose)\n",
    "            self.get_logger().info(f'Moving to {self.target_pose}')\n",
    "            # After move, return to conversation\n",
    "            self.state = 'CONVERSATION'\n",
    "\n",
    "    # ----- Mock implementations -----\n",
    "    def mock_wake_trigger(self):\n",
    "        # Replace with real detection\n",
    "        return True\n",
    "\n",
    "    def mock_asr(self, buffer):\n",
    "        # Replace with real ASR\n",
    "        return 'Move to kitchen'\n",
    "\n",
    "    def mock_qwen(self, text):\n",
    "        # Replace with real Qwen query\n",
    "        if 'move' in text:\n",
    "            return 'Sure, navigating to the kitchen now.'\n",
    "        return 'Hello! How can I assist you?'\n",
    "\n",
    "    def mock_tts(self, text):\n",
    "        # Create a dummy audio message\n",
    "        class AudioMsg:\n",
    "            def __init__(self, data): self.data = data\n",
    "        return AudioMsg(data=text.encode())\n",
    "\n",
    "    def mock_intent(self, response_text):\n",
    "        if 'navigating' in response_text.lower():\n",
    "            return 'move', {'location': 'kitchen'}\n",
    "        if 'bye' in response_text.lower():\n",
    "            return 'end', {}\n",
    "        return 'chat', {}\n",
    "\n",
    "    def resolve_pose(self, location_str):\n",
    "        # Map location string to coordinates\n",
    "        poses = {'kitchen': (1.0, 2.0, 0.0)}\n",
    "        return poses.get(location_str, (0.0, 0.0, 0.0))\n",
    "\n",
    "    def mock_navigate(self, pose):\n",
    "        # Simulate navigation\n",
    "        pass\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = AssistantNode()\n",
    "    try:\n",
    "        rclpy.spin(node)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
