{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf052522",
   "metadata": {},
   "source": [
    "## Environment\n",
    "* Install Anaconda (install > nano ~/.bashrc > ~/anaconda3/bin/conda init > source ~/.bashrc)\n",
    "* Install CUDA toolkit and legacy driver (12.8 on Jul-25)\n",
    "* Install PyTorch (about 7GBs)\n",
    "* Create Conda environment `conda create --name robuddy python=3.11`\n",
    "* Install HuggingFace\n",
    "\n",
    "Removing previous nvidia installations\n",
    "``` bash\n",
    "sudo apt purge nvidia* -y\n",
    "sudo apt remove nvidia-* -y\n",
    "sudo rm /etc/apt/sources.list.d/cuda*\n",
    "sudo apt autoremove -y && sudo apt autoclean -y\n",
    "sudo rm -rf /usr/local/cuda*\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "```\n",
    "\n",
    "To free up space:\n",
    "```bash\n",
    "conda env remove --name <environment_name>\n",
    "conda clean --all\n",
    "rm -rf ~/.cache/huggingface/hub\n",
    "rm -rf ~/.cache/huggingface/transformers\n",
    "rm -rf ~/.cache/huggingface/datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568c8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  4 17:23:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P0             21W /   90W |       1MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA version\n",
    "!nvidia-smi\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c961416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "coqui-tts 0.26.2 requires transformers<4.52,>=4.47.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install/update required packages\n",
    "%pip install --quiet pvporcupine sounddevice soundfile librosa pyyaml matplotlib scipy transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22690519",
   "metadata": {},
   "source": [
    "## Wake word detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b0492",
   "metadata": {},
   "source": [
    "### Porcupine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"config.yaml\"\n",
    "\n",
    "def load_config(file_path: Path = CONFIG_PATH) -> str:\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {file_path}\")\n",
    "    cfg = yaml.safe_load(file_path.read_text())\n",
    "    if \"PORCUPINE_KEY\" not in cfg:\n",
    "        raise KeyError(\"Missing 'PORCUPINE_KEY' in config\")\n",
    "    return cfg[\"PORCUPINE_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05af8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcupine frame_length: 512\n"
     ]
    }
   ],
   "source": [
    "import pvporcupine\n",
    "porcupine = pvporcupine.create(access_key=load_config(), keywords=[\"jarvis\"])\n",
    "print(\"Porcupine frame_length:\", porcupine.frame_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e22447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 HDA NVidia: HDMI 0 (hw:0,3), ALSA (0 in, 8 out)\n",
       "  1 HDA NVidia: HDMI 1 (hw:0,7), ALSA (0 in, 8 out)\n",
       "  2 HDA NVidia: HDMI 2 (hw:0,8), ALSA (0 in, 8 out)\n",
       "  3 HDA NVidia: HDMI 3 (hw:0,9), ALSA (0 in, 8 out)\n",
       "  4 HD-Audio Generic: HDMI 0 (hw:1,3), ALSA (0 in, 8 out)\n",
       "  5 HD-Audio Generic: ALC256 Analog (hw:2,0), ALSA (2 in, 2 out)\n",
       "  6 hdmi, ALSA (0 in, 8 out)\n",
       "  7 pipewire, ALSA (64 in, 64 out)\n",
       "* 8 default, ALSA (64 in, 64 out)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "sd.query_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acef8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfae3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 'Jarvis' at sample 32256\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, soundfile as sf, librosa\n",
    "\n",
    "data, sr = sf.read(\"output.wav\")\n",
    "if data.ndim > 1:\n",
    "    data = data[:,0]\n",
    "if sr != 16000:\n",
    "    data = librosa.resample(data, sr, 16000)\n",
    "    sr = 16000\n",
    "if data.dtype != np.int16:\n",
    "    data = (data * 32767).astype(np.int16)\n",
    "\n",
    "for i in range(0, len(data) - porcupine.frame_length + 1, porcupine.frame_length):\n",
    "    if porcupine.process(data[i:i+porcupine.frame_length]) >= 0:\n",
    "        print(\"Detected 'Jarvis' at sample\", i)\n",
    "        break\n",
    "porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2972c",
   "metadata": {},
   "source": [
    "## Image Object Detection\n",
    "### YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model8s = YOLO(\"models/YOLO/yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in model8s.track(source=0, classes=[0], conf=0.25, device=\"cuda:0\"):\n",
    "    human_near = (results[0].boxes.cls.cpu() == 0).any().item()\n",
    "    print(\"Human detected:\", human_near)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcfa04",
   "metadata": {},
   "source": [
    "## Speech-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba3517",
   "metadata": {},
   "source": [
    "### Whisper\n",
    "- By openai\n",
    "- High accuracy\n",
    "- Resoruce intensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai-whisper\n",
    "!sudo apt update && sudo apt install ffmpeg\n",
    "!pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7187c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "stt_model = whisper.load_model(\"turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stt_model.transcribe(\"Camilla.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6e4d2",
   "metadata": {},
   "source": [
    "### Vosk\n",
    "- Less accurate than whisper\n",
    "- Less resource intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b80c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f22674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Saved output.wav\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs, duration = 16000, 5\n",
    "print(\"Recording...\")\n",
    "rec = sd.rec(int(fs*duration), samplerate=fs, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(\"output.wav\", fs, rec)\n",
    "print(\"Saved output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b928c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from stt/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from stt/vosk-model-small-en-us-0.15/graph/HCLr.fst stt/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo stt/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial: what\n",
      "Partial: what\n",
      "Partial: what are\n",
      "Partial: what are\n",
      "Partial: what are your\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Partial: what are your capabilities\n",
      "Transcript: what are your capabilities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "SetLogLevel(0)  # Suppress verbose logging\n",
    "\n",
    "model = Model(\"stt/vosk-model-small-en-us-0.15\")\n",
    "wf = wave.open(\"output.wav\", \"rb\")\n",
    "assert wf.getnchannels() == 1 and wf.getsampwidth() == 2 and wf.getframerate() in (8000,16000,44100), \\\n",
    "       \"Use mono WAV with 16-bit samples\"\n",
    "\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)\n",
    "\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        results.append(result.get(\"text\", \"\"))\n",
    "    else:\n",
    "        # Optional: collect partial results if desired\n",
    "        partial = json.loads(rec.PartialResult()).get(\"partial\", \"\")\n",
    "        if partial:\n",
    "            print(\"Partial:\", partial)\n",
    "\n",
    "# Don't forget the final result!\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "results.append(final_result.get(\"text\", \"\"))\n",
    "\n",
    "# Join non-empty results\n",
    "transcript = \" \".join([r for r in results if r])\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe1e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are your capabilities', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cea793",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbda70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "ROOT_PATH = Path.home() / \"Study\" / \"Course\" / \"EECS\" / \"Robotics\" / \"mobile_robot\" / \"LLM\"\n",
    "\n",
    "with open(ROOT_PATH / \"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = cfg['huggingface']['api_token']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52495b",
   "metadata": {},
   "source": [
    "### Text-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304b17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_task: str,\n",
    "        formatted_kb: str,\n",
    "        output_instructions: str,\n",
    "        delimiter: str = \"###\",\n",
    "        model_name: str = None,\n",
    "        hf_pipeline: callable = None,\n",
    "        pipeline_kwargs: dict = None,\n",
    "        generation_kwargs: dict = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        llm_task: short description of the LLM’s task\n",
    "        formatted_kb: pre-formatted knowledge base text\n",
    "        output_instructions: any extra instructions for the assistant\n",
    "        delimiter: how to split raw generations to find the assistant’s reply\n",
    "\n",
    "        Either provide model_name (to build a HF pipeline) or an existing hf_pipeline.\n",
    "        pipeline_kwargs: kwargs passed to transformers.pipeline\n",
    "        generation_kwargs: kwargs passed to the pipeline when generating (e.g., max_new_tokens, temperature)\n",
    "        \"\"\"\n",
    "        self.llm_task = llm_task\n",
    "        self.formatted_kb = formatted_kb\n",
    "        self.output_instructions = output_instructions\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "        # Initialize pipeline\n",
    "        if hf_pipeline is not None:\n",
    "            self.pipe = hf_pipeline\n",
    "        elif model_name is not None:\n",
    "            pipeline_kwargs = pipeline_kwargs or {}\n",
    "            self.pipe = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model=model_name,\n",
    "                **pipeline_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"You must provide either hf_pipeline or model_name.\")\n",
    "\n",
    "        # Generation defaults\n",
    "        self.generation_kwargs = {\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"max_new_tokens\": 500,\n",
    "        }\n",
    "        if generation_kwargs:\n",
    "            self.generation_kwargs.update(generation_kwargs)\n",
    "\n",
    "        # Conversation state\n",
    "        self.history = []  # list of tuples (role, text)\n",
    "\n",
    "    def add_message(self, role: str, text: str):\n",
    "        self.history.append((role, text))\n",
    "\n",
    "    def build_prompt(self) -> str:\n",
    "        segments = [f\"Task: {self.llm_task}\", f\"Knowledge_base:\\n{self.formatted_kb}\"]\n",
    "        for role, text in self.history:\n",
    "            segments.append(f\"{role}: {text}\")\n",
    "        segments.append(f\"{self.output_instructions}\")\n",
    "        return \"\\n\".join(segments)\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Send a user question and get the assistant's reply.\"\"\"\n",
    "        # append user message\n",
    "        self.add_message(\"User\", question)\n",
    "\n",
    "        # generate\n",
    "        prompt = self.build_prompt()\n",
    "        out = self.pipe(prompt, **self.generation_kwargs)\n",
    "        raw = out[0].get('generated_text', out[0])\n",
    "\n",
    "        # extract assistant reply\n",
    "        parts = raw.split(self.delimiter)\n",
    "        reply = parts[-1].strip() if len(parts) > 1 else raw[len(prompt):].strip()\n",
    "\n",
    "        # append assistant message\n",
    "        self.add_message(\"Assistant\", reply)\n",
    "        return reply\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bd0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_PATH / \"knowledge_base.yaml\", \"r\") as f:\n",
    "    knowledge_base = yaml.safe_load(f)\n",
    "\n",
    "formatted_kb = yaml.dump(knowledge_base, default_flow_style=False)\n",
    "\n",
    "\n",
    "delimiter = \"#####\"\n",
    "\n",
    "llm_task = \"\"\"\n",
    "You are a friendly retail customer service assistant robot.\n",
    "You will be provided with a customer's question or request related to products, product locations, store policies.\n",
    "You will also be provided with a store's knowledge base containing information about products, promotions, and store policies.\n",
    "\n",
    "You must select one of the following two response types based on the customer's question or request:\n",
    "\n",
    "Response Type 1 - Product information:\n",
    "  - If the customer's question is about details of a specific product or promotion, briefly provide the relevant information from the store's knowledge base.\n",
    "\n",
    "Response Type 2 - Product location:\n",
    "  - If the customer asks for guidance to find or navigate directly to a specific product or product category, you must provide ONLY the precise coordinates (x, y) for navigation from the store's knowledge base.\n",
    "  - If exact coordinates aren't available for the product, provide aisle information instead.\n",
    "\n",
    "Response Type 3 - End interaction:\n",
    "  - If the customer indicates they need no further assistance (e.g., “That's all, thanks” or “No, I'm good”), respond with type 3 and a polite closing message such as “Happy to help—have a great day!”\n",
    "\n",
    "If neither the answer nor the coordinates or aisle are found in the store's knowledge base, politely respond with: \"I will get someone who can help you with your query.\"\n",
    "\"\"\"\n",
    "\n",
    "outputInstructions = f\"\"\"\n",
    "You must reply **exactly** (with no extra text or reasoning) in this format:\n",
    "\n",
    "Response type: {delimiter} <1, 2, or 3>  \n",
    "Response to user: {delimiter} <your reply>\n",
    "\n",
    "- Do NOT output any internal reasoning, thought process, or steps.\n",
    "- Do NOT use any other words or punctuation beyond what’s above.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153bc8e",
   "metadata": {},
   "source": [
    "#### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate --quiet\n",
    "%pip install PyYAML --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b08486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acf7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: ## (12.4, 5.8)\n",
      "LLM Response: ## (12.4, 5.8)\n",
      "Conversation history cleared.\n"
     ]
    }
   ],
   "source": [
    "# Example usage in a notebook:\n",
    "from transformers import pipeline\n",
    "\n",
    "sess = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    text_pipe=llama_pipe,\n",
    "    output_instructions=outputInstructions\n",
    ")\n",
    "\n",
    "sess.conversation(\"Recommend an healthy milk\")\n",
    "sess.conversation(\"Can you take me to it?\")\n",
    "sess.clear_conversation(force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480088a",
   "metadata": {},
   "source": [
    "#### R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f50a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes -U \"huggingface_hub[cli]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28d4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunzid/anaconda3/envs/robuddy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# bnb = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# dsr1_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# dsr1_tokenizer = AutoTokenizer.from_pretrained(dsr1_model_name)\n",
    "\n",
    "# dsr1_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     dsr1_model_name,\n",
    "#     quantization_config=bnb,\n",
    "#     device_map=\"auto\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dceb81eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "#\n",
    "# # Using a Hugging Face model by name:\n",
    "# session = ChatSession(\n",
    "#     llm_task=\"Answer questions based on the KB.\",\n",
    "#     formatted_kb=\"Here is some knowledge: ...\",\n",
    "#     output_instructions=\"Assistant:\",\n",
    "#     model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#     pipeline_kwargs={\"device\": 0},\n",
    "#     generation_kwargs={\"temperature\": 0.5, \"max_new_tokens\": 200},\n",
    "# )\n",
    "# reply = session.ask(\"What is the capital of France?\")\n",
    "# print(reply)\n",
    "#\n",
    "# Using an existing pipeline for DeepSeek-R1:\n",
    "ds_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    device=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e34cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=ds_pipeline,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 500,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a486f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<your reply>\n",
      "\n",
      "- Do NOT output any internal reasoning, thought process, or steps.\n",
      "- Do NOT use any other words or punctuation beyond what’s above.\n",
      "\"\"\"\n",
      "\n",
      "Okay, so let me try to figure out how to handle this task step by step. The user asked if there's any milk in the store. My job is to determine whether to give them product info, suggest navigating via coordinates, ask for more details, or decline unless certain conditions apply like not needing further assistance.\n",
      "\n",
      "First, looking at the Knowledge Base. Under'milk', there are three items listed: Brand A, which has low-fat dairy milk; brand B, which sells full-cream organic milk; both located in their respective coordinate areas. Also, there's a promotion called \"Buy 2 Get 1 Free\" specifically linked to Brand B Milk.\n",
      "\n",
      "The user's question was straightforward—\"Do you have any milk here.\" So it seems they're asking for confirmation that milk exists in the store but don't know where exactly because they might want directions to check quickly.\n",
      "\n",
      "Since the question isn't detailed enough to pinpoint an exact location or product line except for knowing those particular stores exist, perhaps we should consider Response Type 2—Product Location—but wait, maybe even better than that since the store itself offers promotions too?\n",
      "\n",
      "Alternatively, thinking again—the prompt didn’t specify wanting immediate location hints. It just wanted to confirm availability without specifics. But given the presence of multiple locations and the promotion, perhaps providing the necessary info would satisfy being helpful.\n",
      "\n",
      "But hold on—if I mention specific locations where milk is sold, especially focusing on the promotion area for brand B, then customers could easily access those areas when checking inventory. However, the original instructions say to only include coordinates or aisles if the needed data isn't present elsewhere.\n",
      "\n",
      "Looking back at the Knowledge Base under 'Promotions' for Brand B Milk, the promotion states \"Buy 2 Get 1 Free,\" which applies to Brand B. So does that mean that the store allows purchasing these two units and getting the third free? Is there something else associated with this offer besides the price discount?\n",
      "\n",
      "In terms of end users accessing that offer, perhaps mentioning it in a way that provides direct links to purchase these deals could be useful. For instance, stating that buying 2 gives 1 free doesn't necessarily require entering coordinates or searching anywhere—it's already known through its own promotion.\n",
      "\n",
      "Therefore, considering all factors, maybe it's appropriate to inform the user that they can find milk near Brands A and B, particularly using the specified coordinates for each location. This includes offering the exact x,y coordinates where milk is readily available, allowing\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afebad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer2 = session.ask(\"Which one is less expensive.\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer3 = session.ask(\"Take me to the milk.\")\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd41a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bedba",
   "metadata": {},
   "source": [
    "#### FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4fe55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"#####\"\n",
    "\n",
    "llm_task = \"\"\"\n",
    "You are a friendly retail customer service assistant robot.\n",
    "You will be provided with a customer's question or request related to products, product locations, store policies.\n",
    "You will also be provided with a store's knowledge base containing information about products, promotions, and store policies.\n",
    "\n",
    "You must select one of the following two response types based on the customer's question or request:\n",
    "\n",
    "Response Type 1 - Product information:\n",
    "  - If the customer's question is about details of a specific product or promotion, briefly provide the relevant information from the store's knowledge base.\n",
    "\n",
    "Response Type 2 - Product location:\n",
    "  - If the customer asks for guidance to find or navigate directly to a specific product or product category, you must provide ONLY the precise coordinates (x, y) for navigation from the store's knowledge base.\n",
    "  - If exact coordinates aren't available for the product, provide aisle information instead.\n",
    "\n",
    "If neither the answer nor the coordinates or aisle are found in the store's knowledge base, politely respond with: \"I will get someone who can help you with your query.\"\n",
    "\"\"\"\n",
    "\n",
    "outputInstructions = f\"\"\"\n",
    "You must reply **exactly** (with no extra text or reasoning) in this format:\n",
    "\n",
    "Response type: {delimiter} <1, 2, or 3>  \n",
    "Response to user: {delimiter} <your reply>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "551c1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "flan_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    device=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cf79354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=flan_pipe,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39cfffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b83efe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193928fd",
   "metadata": {},
   "source": [
    "#### meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654f9295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [21:42<00:00, 651.23s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebd1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) start a session\n",
    "session = ChatSession(\n",
    "    llm_task=llm_task,\n",
    "    formatted_kb=formatted_kb,\n",
    "    output_instructions=outputInstructions,\n",
    "    model_name=None,           # we’re passing an existing pipeline…\n",
    "    hf_pipeline=llama_pipe,\n",
    "    delimiter=delimiter,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5406e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<your reply>\n",
      "\n",
      "- Do NOT output any internal reasoning, thought process, or steps.\n",
      "- Do NOT use any other words or punctuation beyond what’s above.\n",
      "\n",
      "Please do not hesitate to ask if there's anything else you don't understand.\n"
     ]
    }
   ],
   "source": [
    "answer1 = session.ask(\"Do you have any milk here.\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e4928",
   "metadata": {},
   "source": [
    "#### unsloth/gemma-3-12b-it-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"unsloth/gemma-3-12b-it-GGUF\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "print(output[0][0][\"generated_text\"][-1][\"content\"])\n",
    "# Okay, let's take a look! \n",
    "# Based on the image, the animal on the candy is a **turtle**. \n",
    "# You can see the shell shape and the head and legs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b3131",
   "metadata": {},
   "source": [
    "### Audio/text to audio/text\n",
    "#### Qwen 2 Audio-7B Multimodal Audio Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import librosa\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation1 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n",
    "        {\"type\": \"text\", \"text\": \"What's that sound?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n",
    "        {\"type\": \"text\", \"text\": \"What can you hear?\"},\n",
    "    ]}\n",
    "]\n",
    "\n",
    "conversation2 = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n",
    "        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "conversations = [conversation1, conversation2]\n",
    "\n",
    "text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n",
    "\n",
    "audios = []\n",
    "for conversation in conversations:\n",
    "    for message in conversation:\n",
    "        if isinstance(message[\"content\"], list):\n",
    "            for ele in message[\"content\"]:\n",
    "                if ele[\"type\"] == \"audio\":\n",
    "                    audios.append(\n",
    "                        librosa.load(\n",
    "                            BytesIO(urlopen(ele['audio_url']).read()), \n",
    "                            sr=processor.feature_extractor.sampling_rate)[0]\n",
    "                    )\n",
    "\n",
    "inputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n",
    "inputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\n",
    "inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_length=256)\n",
    "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
    "\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15483e8f",
   "metadata": {},
   "source": [
    "## Text-to-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca79595",
   "metadata": {},
   "source": [
    "### eSpeak NG\n",
    "- Fully opensource\n",
    "- Mechanical sounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install espeak-ng\n",
    "# !sudo apt install mbrola mbrola-us1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def tts(text: str, voice: str = \"en-us+f3\", speed: int = 120, pitch: int = 50):\n",
    "    # voice example: \"mb-us1\" or \"en-us+f3\"\n",
    "    subprocess.call([\n",
    "        \"espeak-ng\",\n",
    "        \"-v\", voice,\n",
    "        \"-s\", str(speed),\n",
    "        \"-p\", str(pitch),\n",
    "        text\n",
    "    ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tts(\"Hello from Ubuntu Python, sounding nicer!\", voice=\"mb-us1\", speed=120, pitch=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9676",
   "metadata": {},
   "source": [
    "### Coqui-tts\n",
    "- Free for non-commercial use\n",
    "- Natural sounding\n",
    "- Many voice options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coqui-tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > You must confirm the following:\n",
      " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
      " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.87G/1.87G [02:45<00:00, 11.3MiB/s]\n",
      "100%|██████████| 4.37k/4.37k [00:00<00:00, 38.7kiB/s]\n",
      "100%|██████████| 361k/361k [00:00<00:00, 1.98MiB/s]\n",
      "100%|██████████| 32.0/32.0 [00:00<00:00, 142iB/s]\n",
      "100%|██████████| 7.75M/7.75M [00:14<00:00, 4.58MiB/s]"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tts = TTS(model_name=\"tts_models/en/vctk/vits\").to(device)\n",
    "# tts.tts_to_file(text=\"Natural voice is here!\", file_path=\"out.wav\")\n",
    "\n",
    "\n",
    "# Initialize TTS\n",
    "tts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Claribel Dervla', 'Daisy Studious', 'Gracie Wise', 'Tammie Ema', 'Alison Dietlinde', 'Ana Florence', 'Annmarie Nele', 'Asya Anara', 'Brenda Stern', 'Gitta Nikolina', 'Henriette Usha', 'Sofia Hellen', 'Tammy Grit', 'Tanja Adelina', 'Vjollca Johnnie', 'Andrew Chipper', 'Badr Odhiambo', 'Dionisio Schuyler', 'Royston Min', 'Viktor Eka', 'Abrahan Mack', 'Adde Michal', 'Baldur Sanjin', 'Craig Gutsy', 'Damien Black', 'Gilberto Mathias', 'Ilkin Urbano', 'Kazuhiko Atallah', 'Ludvig Milivoj', 'Suad Qasim', 'Torcull Diarmuid', 'Viktor Menelaos', 'Zacharie Aimilios', 'Nova Hogarth', 'Maja Ruoho', 'Uta Obando', 'Lidiya Szekeres', 'Chandra MacFarland', 'Szofi Granger', 'Camilla Holmström', 'Lilya Stainthorpe', 'Zofija Kendrick', 'Narelle Moon', 'Barbora MacLean', 'Alexandra Hisakawa', 'Alma María', 'Rosemary Okafor', 'Ige Behringer', 'Filip Traverse', 'Damjan Chapman', 'Wulf Carlevaro', 'Aaron Dreschner', 'Kumar Dahl', 'Eugenio Mataracı', 'Ferran Simen', 'Xavier Hayasaka', 'Luis Moray', 'Marcos Rudaski']\n"
     ]
    }
   ],
   "source": [
    "# List speakers\n",
    "print(tts_model.speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb2a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annmarie.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TTS to a file, use a preset speaker\n",
    "tts_model.tts_to_file(\n",
    "  text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n",
    "\n",
    "# tts_model.tts_to_file(\n",
    "#   text=\"The quick brown fox jumps over the lazy dog!\",\n",
    "#   speaker=\"Suad Qasim\",\n",
    "#   language=\"en\",\n",
    "#   file_path=\"Suad.wav\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb5577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camilla.wav'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using the model\n",
    "tts_model.tts_to_file(\n",
    "  text=generated_text,\n",
    "  speaker=\"Camilla Holmström\",\n",
    "  language=\"en\",\n",
    "  file_path=\"Camilla.wav\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539426",
   "metadata": {},
   "source": [
    "## Commercial Assistant Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pvporcupine\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from TTS.api import TTS\n",
    "\n",
    "# ─── LOAD CONFIG ─────────────────────────────────────────────────────────────\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "PORCUPINE_KEYWORDS = cfg['porcupine']['keywords']\n",
    "PORCUPINE_ACCESS_KEY = cfg['porcupine']['access_key']\n",
    "AUDIO_SR         = cfg['audio']['sample_rate']\n",
    "QUESTION_DUR     = cfg['audio']['question_duration']\n",
    "VOSK_MODEL_PATH  = cfg['vosk']['model_path']\n",
    "LLAMA_CFG        = cfg['llama']\n",
    "TTS_CFG          = cfg['tts']\n",
    "HF_TOKEN = cfg['huggingface']['token']\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "# ─── INIT MODELS ─────────────────────────────────────────────────────────────\n",
    "porcupine = pvporcupine.create(access_key=PORCUPINE_ACCESS_KEY, keywords=PORCUPINE_KEYWORDS)\n",
    "\n",
    "SetLogLevel(0)\n",
    "vosk_model = Model(VOSK_MODEL_PATH)\n",
    "\n",
    "text_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLAMA_CFG['light_model_id'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=LLAMA_CFG['max_new_tokens'],\n",
    "    do_sample=LLAMA_CFG['do_sample']\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts_model = TTS(TTS_CFG['eng_model_name']).to(device)\n",
    "\n",
    "\n",
    "def listen_for_wakeword():\n",
    "    print(\"🔊 Listening for wake word...\")\n",
    "    with sd.InputStream(\n",
    "        samplerate=AUDIO_SR,\n",
    "        blocksize=porcupine.frame_length,\n",
    "        dtype=\"int16\",\n",
    "        channels=1\n",
    "    ) as stream:\n",
    "        while True:\n",
    "            pcm, _ = stream.read(porcupine.frame_length)\n",
    "            pcm = pcm.flatten().tolist()\n",
    "            if porcupine.process(pcm) >= 0:\n",
    "                print(\"✨ Wake word 'Jarvis' detected!\")\n",
    "                return\n",
    "\n",
    "\n",
    "def record_question(duration=QUESTION_DUR):\n",
    "    print(f\"🎤 Recording question for {duration}s…\")\n",
    "    audio = sd.rec(int(duration * AUDIO_SR), samplerate=AUDIO_SR, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    return audio.flatten()\n",
    "\n",
    "\n",
    "def transcribe_audio(audio: np.ndarray):\n",
    "    tmp = \"tmp_question.wav\"\n",
    "    write(tmp, AUDIO_SR, audio)\n",
    "    wf = wave.open(tmp, \"rb\")\n",
    "\n",
    "    rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    segments = []\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if not data:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            segments.append(json.loads(rec.Result()).get(\"text\", \"\"))\n",
    "    segments.append(json.loads(rec.FinalResult()).get(\"text\", \"\"))\n",
    "    os.remove(tmp)\n",
    "\n",
    "    transcript = \" \".join([s for s in segments if s])\n",
    "    print(\"📝 Transcribed question:\", transcript)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    print(\"🤖 Generating response…\")\n",
    "    out = text_pipe(prompt)\n",
    "    raw = out[0][\"generated_text\"]\n",
    "    answer = raw[len(prompt):].strip()\n",
    "    print(\"💬 Answer:\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def speak_and_play(text: str, fname=\"response.wav\"):\n",
    "    print(\"🔊 Synthesizing speech…\")\n",
    "    tts_model.tts_to_file(\n",
    "        text=text,\n",
    "        speaker=TTS_CFG['eng_speaker'],\n",
    "        # language=TTS_CFG['language'], #if english only model is used\n",
    "        file_path=fname\n",
    "    )\n",
    "    data, sr = sf.read(fname)\n",
    "    sd.play(data, sr)\n",
    "    sd.wait()\n",
    "    os.remove(fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Available speakers:\", tts_model.speakers)\n",
    "    # print(\"Supported langs:\",   tts_model.languages)\n",
    "    try:\n",
    "        while True:\n",
    "            listen_for_wakeword()\n",
    "            q_audio = record_question()\n",
    "            q_text  = transcribe_audio(q_audio)\n",
    "            if not q_text.strip():\n",
    "                print(\"⚠️ No speech detected—back to listening.\")\n",
    "                continue\n",
    "\n",
    "            ans = generate_response(q_text)\n",
    "            if not ans.strip():\n",
    "                print(\"⚠️ No answer generated—back to listening.\")\n",
    "                continue\n",
    "            # Speak the answer and play it\n",
    "            speak_and_play(ans)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Exiting.\")\n",
    "    finally:\n",
    "        porcupine.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b270f",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robuddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
